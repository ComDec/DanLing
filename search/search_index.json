{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-]"},"docs":[{"title":"DanLing","text":"","location":""},{"title":"Introduction","text":"<p>DanLing (\u200b\u4e39\u7075\u200b) is a high-level library to help with running neural networks flexibly and transparently.</p> <p>DanLing is meant to be a scaffold for experienced researchers and engineers who know how to define a training loop, but are bored of writing the same boilerplate code, such as DDP, logging, checkpointing, etc., over and over again.</p> <p>Therefore, DanLing does not feature complex Runner designs with many pre-defined methods and complicated hooks. Instead, the Runner of DanLing just initialise the essential parts for you, and you can do whatever you want, however you want.</p> <p>Although many attributes and properties are pre-defined and are expected to be used in DanLing, you have full control over your code.</p> <p>DanLing also provides some utilities, such as Registry, NestedTensor, catch, etc.</p>","location":"#introduction"},{"title":"Installation","text":"<p>Install the most recent stable version on pypi:</p> Bash<pre><code>pip install danling\n</code></pre> <p>Install the latest version from source:</p> Bash<pre><code>pip install git+https://github.com/ZhiyuanChen/DanLing\n</code></pre> <p>It works the way it should have worked.</p>","location":"#installation"},{"title":"License","text":"<p>DanLing is multi-licensed under the following licenses:</p> <ul> <li>Unlicense</li> <li>GNU GPL 2.0 (or any later version)</li> <li>MIT</li> <li>Apache 2.0</li> <li>BSD 2-Clause</li> <li>BSD 3-Clause</li> </ul> <p>You can choose any (one or more) of them if you use this work.</p> <p><code>SPDX-License-Identifier: Unlicense OR GPL-2.0-or-later OR MIT OR Apache-2.0 OR BSD-2-Clause OR BSD-3-Clause</code></p>","location":"#license"},{"title":"DanLing","text":"","location":"package/"},{"title":"<code>AverageMeter</code>","text":"<p>Computes and stores the average and current value.</p> <p>Attributes:</p>    Name Type Description     <code>val</code>  <code>int</code>  <p>Current value.</p>   <code>avg</code>  <code>float</code>  <p>Average value.</p>   <code>sum</code>  <code>float</code>  <p>Sum of values.</p>   <code>count</code>  <code>int</code>  <p>Number of values.</p>    <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.update(0.9)\n&gt;&gt;&gt; meter.val\n0.9\n&gt;&gt;&gt; meter.avg\n0.8\n&gt;&gt;&gt; meter.sum\n1.6\n&gt;&gt;&gt; meter.count\n2\n&gt;&gt;&gt; meter.reset()\n&gt;&gt;&gt; meter.val\n0\n&gt;&gt;&gt; meter.avg\n0\n</code></pre>  Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>class AverageMeter:\n    r\"\"\"\n    Computes and stores the average and current value.\n\n    Attributes\n    ----------\n    val: int\n        Current value.\n    avg: float\n        Average value.\n    sum: float\n        Sum of values.\n    count: int\n        Number of values.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; meter = AverageMeter()\n    &gt;&gt;&gt; meter.update(0.7)\n    &gt;&gt;&gt; meter.val\n    0.7\n    &gt;&gt;&gt; meter.avg\n    0.7\n    &gt;&gt;&gt; meter.update(0.9)\n    &gt;&gt;&gt; meter.val\n    0.9\n    &gt;&gt;&gt; meter.avg\n    0.8\n    &gt;&gt;&gt; meter.sum\n    1.6\n    &gt;&gt;&gt; meter.count\n    2\n    &gt;&gt;&gt; meter.reset()\n    &gt;&gt;&gt; meter.val\n    0\n    &gt;&gt;&gt; meter.avg\n    0\n\n    ```\n    \"\"\"\n\n    val: int = 0\n    avg: float = 0\n    sum: int = 0\n    count: int = 0\n\n    def __init__(self) -&gt; None:\n        self.reset()\n\n    def reset(self) -&gt; None:\n        r\"\"\"\n        Resets the meter.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; meter = AverageMeter()\n        &gt;&gt;&gt; meter.update(0.7)\n        &gt;&gt;&gt; meter.val\n        0.7\n        &gt;&gt;&gt; meter.avg\n        0.7\n        &gt;&gt;&gt; meter.reset()\n        &gt;&gt;&gt; meter.val\n        0\n        &gt;&gt;&gt; meter.avg\n        0\n\n        ```\n        \"\"\"\n\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n: int = 1) -&gt; None:\n        r\"\"\"\n        Updates the average and current value in the meter.\n\n        Parameters\n        ----------\n        val: int\n            Value to be added to the average.\n        n: int = 1\n            Number of values to be added.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; meter = AverageMeter()\n        &gt;&gt;&gt; meter.update(0.7)\n        &gt;&gt;&gt; meter.val\n        0.7\n        &gt;&gt;&gt; meter.avg\n        0.7\n        &gt;&gt;&gt; meter.update(0.9)\n        &gt;&gt;&gt; meter.val\n        0.9\n        &gt;&gt;&gt; meter.avg\n        0.8\n        &gt;&gt;&gt; meter.sum\n        1.6\n        &gt;&gt;&gt; meter.count\n        2\n\n        ```\n        \"\"\"\n\n        # pylint: disable=C0103\n\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n</code></pre>","location":"package/#danling.AverageMeter"},{"title":"<code>reset()</code>","text":"<p>Resets the meter.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.reset()\n&gt;&gt;&gt; meter.val\n0\n&gt;&gt;&gt; meter.avg\n0\n</code></pre>  Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>def reset(self) -&gt; None:\n    r\"\"\"\n    Resets the meter.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; meter = AverageMeter()\n    &gt;&gt;&gt; meter.update(0.7)\n    &gt;&gt;&gt; meter.val\n    0.7\n    &gt;&gt;&gt; meter.avg\n    0.7\n    &gt;&gt;&gt; meter.reset()\n    &gt;&gt;&gt; meter.val\n    0\n    &gt;&gt;&gt; meter.avg\n    0\n\n    ```\n    \"\"\"\n\n    self.val = 0\n    self.avg = 0\n    self.sum = 0\n    self.count = 0\n</code></pre>","location":"package/#danling.metrics.average_meter.AverageMeter.reset"},{"title":"<code>update(val, n=1)</code>","text":"<p>Updates the average and current value in the meter.</p> <p>Parameters:</p>    Name Type Description Default     <code>val</code>   <p>Value to be added to the average.</p>  required    <code>n</code>  <code>int</code>  <p>Number of values to be added.</p>  <code>1</code>     <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.update(0.9)\n&gt;&gt;&gt; meter.val\n0.9\n&gt;&gt;&gt; meter.avg\n0.8\n&gt;&gt;&gt; meter.sum\n1.6\n&gt;&gt;&gt; meter.count\n2\n</code></pre>  Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>def update(self, val, n: int = 1) -&gt; None:\n    r\"\"\"\n    Updates the average and current value in the meter.\n\n    Parameters\n    ----------\n    val: int\n        Value to be added to the average.\n    n: int = 1\n        Number of values to be added.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; meter = AverageMeter()\n    &gt;&gt;&gt; meter.update(0.7)\n    &gt;&gt;&gt; meter.val\n    0.7\n    &gt;&gt;&gt; meter.avg\n    0.7\n    &gt;&gt;&gt; meter.update(0.9)\n    &gt;&gt;&gt; meter.val\n    0.9\n    &gt;&gt;&gt; meter.avg\n    0.8\n    &gt;&gt;&gt; meter.sum\n    1.6\n    &gt;&gt;&gt; meter.count\n    2\n\n    ```\n    \"\"\"\n\n    # pylint: disable=C0103\n\n    self.val = val\n    self.sum += val * n\n    self.count += n\n    self.avg = self.sum / self.count\n</code></pre>","location":"package/#danling.metrics.average_meter.AverageMeter.update"},{"title":"<code>MultiHeadAttention</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Allows the model to jointly attend to information from different representation subspaces. See <code>Attention Is All You Need &lt;https://arxiv.org/abs/1706.03762&gt;</code>_ .. math::     \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O where :math:<code>head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)</code>. Args:     embed_dim: total dimension of the model.     num_heads: parallel attention heads.     dropout: a Dropout layer on attn_output_weights. Default: 0.0.     bias: add bias as module parameter. Default: True.     add_bias_kv: add bias to the key and value sequences at dim=0.     add_zero_attn: add a new batch of zeros to the key and                    value sequences at dim=1.     k_dim: total number of features in key. Default: None.     v_dim: total number of features in value. Default: None.     batch_first: If <code>True</code>, then the input and output tensors are provided         as (batch, seq, feature). Default: <code>False</code> (seq, batch, feature). Note that if :attr:<code>k_dim</code> and :attr:<code>v_dim</code> are None, they will be set to :attr:<code>embed_dim</code> such that query, key, and value have the same number of features. Examples::     &gt;&gt;&gt; multihead_attn = dl.model.MultiHeadAttention(embed_dim, num_heads)     &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)</p>  Source code in <code>danling/models/transformer/attention/multihead_attention.py</code> Python<pre><code>class MultiHeadAttention(nn.Module):\n    r\"\"\"Allows the model to jointly attend to information\n    from different representation subspaces.\n    See `Attention Is All You Need &lt;https://arxiv.org/abs/1706.03762&gt;`_\n    .. math::\n        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n    where :math:`head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n    Args:\n        embed_dim: total dimension of the model.\n        num_heads: parallel attention heads.\n        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n        bias: add bias as module parameter. Default: True.\n        add_bias_kv: add bias to the key and value sequences at dim=0.\n        add_zero_attn: add a new batch of zeros to the key and\n                       value sequences at dim=1.\n        k_dim: total number of features in key. Default: None.\n        v_dim: total number of features in value. Default: None.\n        batch_first: If ``True``, then the input and output tensors are provided\n            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n    Note that if :attr:`k_dim` and :attr:`v_dim` are None, they will be set\n    to :attr:`embed_dim` such that query, key, and value have the same\n    number of features.\n    Examples::\n        &gt;&gt;&gt; multihead_attn = dl.model.MultiHeadAttention(embed_dim, num_heads)\n        &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)\n    \"\"\"\n    __constants__ = [\"batch_first\"]\n    bias_k: Optional[Tensor]\n    bias_v: Optional[Tensor]\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        attn_dropout: float = 0.0,\n        scale_factor: float = 1.0,\n        bias: bool = True,\n        add_bias_kv: bool = False,\n        add_zero_attn: bool = False,\n        k_dim: Optional[int] = None,\n        v_dim: Optional[int] = None,\n        batch_first: bool = True,\n        **kwargs: Optional[Dict[str, Any]],\n    ) -&gt; None:\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.k_dim = k_dim if k_dim is not None else self.embed_dim\n        self.v_dim = v_dim if v_dim is not None else self.embed_dim\n        self._qkv_same_embed_dim = self.embed_dim == self.k_dim == self.v_dim\n\n        self.num_heads = num_heads\n        self.scale_factor = scale_factor\n        self.batch_first = batch_first\n        self.head_dim = self.embed_dim // self.num_heads\n        self.scaling = float(self.head_dim * self.scale_factor) ** -0.5\n        if not self.head_dim * self.num_heads == self.embed_dim:\n            raise ValueError(f\"embed_dim {self.embed_dim} not divisible by num_heads {self.num_heads}\")\n\n        self.in_proj = nn.Linear(self.embed_dim, self.embed_dim + self.k_dim + self.v_dim, bias=bias)\n        self.dropout = nn.Dropout(attn_dropout)\n        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=bias)\n\n        if add_bias_kv:\n            self.bias_k = nn.Parameter(torch.empty((1, 1, self.embed_dim)))\n            self.bias_v = nn.Parameter(torch.empty((1, 1, self.embed_dim)))\n        else:\n            self.bias_k = self.bias_v = None\n\n        self.add_zero_attn = add_zero_attn\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        nn.init.xavier_uniform_(self.in_proj.weight)\n        if self.in_proj.bias is not None:\n            nn.init.constant_(self.in_proj.bias, 0.0)\n            nn.init.constant_(self.out_proj.bias, 0.0)\n        if self.bias_k is not None:\n            nn.init.xavier_normal_(self.bias_k)\n        if self.bias_v is not None:\n            nn.init.xavier_normal_(self.bias_v)\n\n    def __setstate__(self, state):\n        # Support loading old MultiHeadAttention checkpoints generated by v1.1.0\n        if \"_qkv_same_embed_dim\" not in state:\n            state[\"_qkv_same_embed_dim\"] = True\n        super(MultiHeadAttention, self).__setstate__(state)\n\n    def forward(\n        self,\n        query: Tensor,\n        key: Tensor,\n        value: Tensor,\n        attn_bias: Optional[Tensor] = None,\n        attn_mask: Optional[Tensor] = None,\n        key_padding_mask: Optional[Tensor] = None,\n        need_weights: bool = False,\n        static_k: Optional[Tensor] = None,\n        static_v: Optional[Tensor] = None,\n    ) -&gt; Tuple[Tensor, Optional[Tensor]]:\n        r\"\"\"\n        Args:\n            query, key, value: map a query and a set of key-value pairs to an output.\n                See \"Attention Is All You Need\" for more details.\n            attn_bias: 2D or 3D mask that add bias to attention output weights. Used for relative positional embedding.\n                A 2D bias will be broadcasted for all the batches while a 3D mask allows to specify a different mask for\n                the entries of each batch.\n            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n                the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n            key_padding_mask: if provided, specified padding elements in the key will\n                be ignored by the attention. When given a binary mask and a value is True,\n                the corresponding value on the attention layer will be ignored. When given\n                a byte mask and a value is non-zero, the corresponding value on the attention\n                layer will be ignored\n            need_weights: output attn_output_weights.\n            static_k, static_v: static key and value used for attention operators.\n        Shapes for inputs:\n            - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n                the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n            - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n                the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n            - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n                the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n            - attn_bias: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n                source sequence length.\n                If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n                length, S is the source sequence length. ``attn_bias`` allows to pass pos embed directly into attention\n                If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will\n                be unchanged. If a BoolTensor is provided, positions with ``True`` is not allowed to attend while ``False``\n                values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight.\n            - attn_mask: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n                source sequence length.\n                If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n                length, S is the source sequence length. ``attn_mask`` ensure that position i is allowed to attend\n                the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n                while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n                is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n                is provided, it will be added to the attention weight.\n            - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n                If a ByteTensor is provided, the non-zero positions will be ignored while the position\n                with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the\n                value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n            - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n                N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n            - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n                N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n        Outputs:\n            - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n                E is the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n            - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n                L is the target sequence length, S is the source sequence length.\n        \"\"\"\n        if self.batch_first:\n            query, key, value = [x.transpose(0, 1) for x in (query, key, value)]\n\n        # set up shape vars\n        target_len, batch_size, embed_dim = query.shape\n        source_len, _, _ = key.shape\n        if not key.shape[:2] == value.shape[:2]:\n            raise ValueError(f\"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}\")\n\n        q, k, v = self.in_projection(query, key, value)\n\n        # prep attention mask\n        if attn_mask is not None:\n            if attn_mask.dtype == torch.uint8:\n                warnings.warn(\n                    \"attn_mask is of type uint8. This type is deprecated. Please use bool or float tensors instead.\"\n                )\n                attn_mask = attn_mask.to(torch.bool)\n            elif not (attn_mask.is_floating_point() or attn_mask.dtype == torch.bool):\n                raise ValueError(f\"attn_mask should have type float or bool, but got {attn_mask.dtype}.\")\n            # ensure attn_mask's dim is 3\n            if attn_mask.dim() == 2:\n                correct_shape = (target_len, source_len)\n                if attn_mask.shape != correct_shape:\n                    raise ValueError(f\"attn_mask should have shape {correct_shape}, but got {attn_mask.shape}.\")\n                attn_mask = attn_mask.unsqueeze(0)\n            elif attn_mask.dim() == 3:\n                correct_shape = (batch_size * self.num_heads, target_len, source_len)  # type: ignore\n                if attn_mask.shape != correct_shape:\n                    raise ValueError(f\"attn_mask should have shape {correct_shape}, but got {attn_mask.shape}.\")\n            else:\n                raise RuntimeError(f\"attn_mask should have dimension 2 or 3, bug got {attn_mask.dim()}.\")\n\n        # prep key padding mask\n        if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n            warnings.warn(\n                \"key_padding_mask is of type uint8. This type is deprecated. Please use bool or float tensors instead.\"\n            )\n            key_padding_mask = key_padding_mask.to(torch.bool)\n\n        # add bias along batch dimension (currently second)\n        if self.bias_k is not None and self.bias_v is not None:\n            assert static_k is None, \"bias cannot be added to static key.\"\n            assert static_v is None, \"bias cannot be added to static value.\"\n            k = torch.cat([k, self.bias_k.repeat(1, batch_size, 1)])\n            v = torch.cat([v, self.bias_v.repeat(1, batch_size, 1)])\n            if attn_mask is not None:\n                attn_mask = F.pad(attn_mask, (0, 1))\n            if key_padding_mask is not None:\n                key_padding_mask = F.pad(key_padding_mask, (0, 1))\n        else:\n            assert self.bias_k is None\n            assert self.bias_v is None\n\n        # reshape q, k, v for multihead attention and make em batch first\n        q = q.reshape(target_len, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n        k = k.reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1) if static_k is None else static_k\n        v = v.reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1) if static_v is None else static_v\n\n        if static_k is not None:\n            correct_shape = (  # type: ignore\n                batch_size * self.num_heads,\n                static_k.shape[1],\n                self.head_dim,\n            )\n            if static_k.shape != correct_shape:\n                raise ValueError(f\"static_k should have shape {correct_shape}, but got {static_k.shape}.\")\n        if static_v is not None:\n            correct_shape = (  # type: ignore\n                batch_size * self.num_heads,\n                static_v.shape[1],\n                self.head_dim,\n            )\n            if static_v.shape != correct_shape:\n                raise ValueError(f\"static_v should have shape {correct_shape}, but got {static_v.shape}.\")\n\n        # add zero attention along batch dimension (now first)\n        if self.add_zero_attn:\n            zero_attn_shape = (batch_size * self.num_heads, 1, self.head_dim)\n            k = torch.cat([k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1)\n            v = torch.cat([v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1)\n            if attn_mask is not None:\n                attn_mask = F.pad(attn_mask, (0, 1))\n            if key_padding_mask is not None:\n                key_padding_mask = F.pad(key_padding_mask, (0, 1))\n\n        # update source sequence length after adjustments\n        source_len = k.shape[1]\n\n        # merge key padding and attention masks\n        if key_padding_mask is not None:\n            if key_padding_mask.shape != (batch_size, source_len):\n                raise ValueError(\n                    f\"key_padding_mask should have shape {(batch_size, source_len)}, but got {key_padding_mask.shape}\"\n                )\n            key_padding_mask = (\n                key_padding_mask.view(batch_size, 1, 1, source_len)\n                .expand(-1, self.num_heads, -1, -1)\n                .reshape(batch_size * self.num_heads, 1, source_len)\n            )\n            if attn_mask is None:\n                attn_mask = key_padding_mask\n            elif attn_mask.dtype == torch.bool:\n                attn_mask = attn_mask.logical_or(key_padding_mask)\n            else:\n                attn_mask = attn_mask.masked_fill(key_padding_mask, float(\"-inf\"))\n\n        # convert mask to float\n        if attn_mask is not None and attn_mask.dtype == torch.bool:\n            new_attn_mask = torch.zeros_like(attn_mask, dtype=torch.float)\n            new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n            attn_mask = new_attn_mask\n\n        # (deep breath) calculate attention and out projection\n        attn_output, attn_output_weights = self.attention(q, k, v, attn_bias, attn_mask)\n        attn_output = attn_output.transpose(0, 1).reshape(target_len, batch_size, embed_dim)\n        attn_output = self.out_projection(attn_output)\n\n        # attn_output_weights is set to torch.empty(0, requires_grad=False) to avoid errors in DDP\n        attn_output_weights = (\n            attn_output_weights.view(batch_size, self.num_heads, target_len, source_len)\n            if need_weights\n            else torch.empty(0, requires_grad=False)\n        )\n\n        if self.batch_first:\n            return attn_output.transpose(0, 1), attn_output_weights\n        else:\n            return attn_output, attn_output_weights\n\n    def in_projection(self, q: Tensor, k: Tensor, v: Tensor) -&gt; Tuple[Tensor, Tensor, Tensor]:\n        r\"\"\"\n        Performs the in-projection step of the attention operation, using packed weights.\n        Output is a triple containing projection tensors for query, key and value.\n        Args:\n            q, k, v: query, key and value tensors to be projected. For self-attention,\n                these are typically the same tensor; for encoder-decoder attention,\n                k and v are typically the same tensor. (We take advantage of these\n                identities for performance if they are present.) Regardless, q, k and v\n                must share a common embedding dimension; otherwise their shapes may vary.\n        Shape:\n            Inputs:\n            - q: :math:`(..., E)` where E is the embedding dimension\n            - k: :math:`(..., E)` where E is the embedding dimension\n            - v: :math:`(..., E)` where E is the embedding dimension\n            Output:\n            - in output list :math:`[q', k', v']`, each output tensor will have the\n                same shape as the corresponding input tensor.\n        \"\"\"\n        if k is v:\n            # self-attention\n            if q is k:\n                return self.in_proj(q).split((self.embed_dim, self.k_dim, self.v_dim), dim=-1)\n            # encoder-decoder attention\n            else:\n                w_q, w_kv = self.in_proj.weight.split([self.embed_dim, self.k_dim + self.v_dim])\n                b_q, b_kv = (\n                    None\n                    if self.in_proj.bias is None\n                    else self.in_proj.bias.split([self.embed_dim, self.k_dim + self.v_dim])\n                )\n                return (F.linear(q, w_q, b_q),) + F.linear(k, w_kv, b_kv).split((self.k_dim, self.v_dim), dim=-1)\n        else:\n            w_q, w_k, w_v = self.in_proj.weight.split([self.embed_dim, self.k_dim, self.v_dim])\n            b_q, b_k, b_v = (\n                None if self.in_proj.bias is None else self.in_proj.bias.split([self.embed_dim, self.k_dim, self.v_dim])\n            )\n            return F.linear(q, w_q, b_q), F.linear(k, w_k, b_k), F.linear(v, w_v, b_v)\n\n    def attention(\n        self,\n        q: Tensor,\n        k: Tensor,\n        v: Tensor,\n        attn_bias: Optional[Tensor] = None,\n        attn_mask: Optional[Tensor] = None,\n    ) -&gt; Tuple[Tensor, Tensor]:\n        r\"\"\"\n        Computes scaled dot product attention on query, key and value tensors, using\n        an optional attention mask if passed, and applying dropout if a probability\n        greater than 0.0 is specified.\n        Returns a tensor pair containing attended values and attention weights.\n        Args:\n            q, k, v: query, key and value tensors. See Shape section for shape details.\n            attn_mask: optional tensor containing mask values to be added to calculated\n                attention. May be 2D or 3D; see Shape section for details.\n            attn_bias: optional tensor containing bias values to be added to calculated\n                attention. Used for relative positional embedding. May be 2D or 3D; see\n                Shape section for details.\n        Shape:\n            - q: :math:`(B, Nt, E)` where B is batch size, Nt is the target sequence length,\n                and E is embedding dimension.\n            - key: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n                and E is embedding dimension.\n            - value: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n                and E is embedding dimension.\n            - attn_bias: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n                shape :math:`(Nt, Ns)`.\n            - attn_mask: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n                shape :math:`(Nt, Ns)`.\n            - Output: attention values have shape :math:`(B, Nt, E)`; attention weights\n                have shape :math:`(B, Nt, Ns)`\n        \"\"\"\n        q *= self.scaling\n        # (B, Nt, E) x (B, E, Ns) -&gt; (B, Nt, Ns)\n        attn = torch.bmm(q, k.transpose(-2, -1))\n        if attn_bias is not None:\n            attn += attn_bias\n        if attn_mask is not None:\n            attn += attn_mask\n        attn = F.softmax(attn, dim=-1)\n        attn = self.dropout(attn)\n        # (B, Nt, Ns) x (B, Ns, E) -&gt; (B, Nt, E)\n        output = torch.bmm(attn, v)\n        return output, attn\n\n    def out_projection(self, attn_output: Tensor) -&gt; Tensor:\n        return self.out_proj(attn_output)\n</code></pre>","location":"package/#danling.MultiHeadAttention"},{"title":"<code>attention(q, k, v, attn_bias=None, attn_mask=None)</code>","text":"<p>Computes scaled dot product attention on query, key and value tensors, using an optional attention mask if passed, and applying dropout if a probability greater than 0.0 is specified. Returns a tensor pair containing attended values and attention weights. Args:     q, k, v: query, key and value tensors. See Shape section for shape details.     attn_mask: optional tensor containing mask values to be added to calculated         attention. May be 2D or 3D; see Shape section for details.     attn_bias: optional tensor containing bias values to be added to calculated         attention. Used for relative positional embedding. May be 2D or 3D; see         Shape section for details. Shape:     - q: :math:<code>(B, Nt, E)</code> where B is batch size, Nt is the target sequence length,         and E is embedding dimension.     - key: :math:<code>(B, Ns, E)</code> where B is batch size, Ns is the source sequence length,         and E is embedding dimension.     - value: :math:<code>(B, Ns, E)</code> where B is batch size, Ns is the source sequence length,         and E is embedding dimension.     - attn_bias: either a 3D tensor of shape :math:<code>(B, Nt, Ns)</code> or a 2D tensor of         shape :math:<code>(Nt, Ns)</code>.     - attn_mask: either a 3D tensor of shape :math:<code>(B, Nt, Ns)</code> or a 2D tensor of         shape :math:<code>(Nt, Ns)</code>.     - Output: attention values have shape :math:<code>(B, Nt, E)</code>; attention weights         have shape :math:<code>(B, Nt, Ns)</code></p>  Source code in <code>danling/models/transformer/attention/multihead_attention.py</code> Python<pre><code>def attention(\n    self,\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    attn_bias: Optional[Tensor] = None,\n    attn_mask: Optional[Tensor] = None,\n) -&gt; Tuple[Tensor, Tensor]:\n    r\"\"\"\n    Computes scaled dot product attention on query, key and value tensors, using\n    an optional attention mask if passed, and applying dropout if a probability\n    greater than 0.0 is specified.\n    Returns a tensor pair containing attended values and attention weights.\n    Args:\n        q, k, v: query, key and value tensors. See Shape section for shape details.\n        attn_mask: optional tensor containing mask values to be added to calculated\n            attention. May be 2D or 3D; see Shape section for details.\n        attn_bias: optional tensor containing bias values to be added to calculated\n            attention. Used for relative positional embedding. May be 2D or 3D; see\n            Shape section for details.\n    Shape:\n        - q: :math:`(B, Nt, E)` where B is batch size, Nt is the target sequence length,\n            and E is embedding dimension.\n        - key: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n            and E is embedding dimension.\n        - value: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n            and E is embedding dimension.\n        - attn_bias: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n            shape :math:`(Nt, Ns)`.\n        - attn_mask: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n            shape :math:`(Nt, Ns)`.\n        - Output: attention values have shape :math:`(B, Nt, E)`; attention weights\n            have shape :math:`(B, Nt, Ns)`\n    \"\"\"\n    q *= self.scaling\n    # (B, Nt, E) x (B, E, Ns) -&gt; (B, Nt, Ns)\n    attn = torch.bmm(q, k.transpose(-2, -1))\n    if attn_bias is not None:\n        attn += attn_bias\n    if attn_mask is not None:\n        attn += attn_mask\n    attn = F.softmax(attn, dim=-1)\n    attn = self.dropout(attn)\n    # (B, Nt, Ns) x (B, Ns, E) -&gt; (B, Nt, E)\n    output = torch.bmm(attn, v)\n    return output, attn\n</code></pre>","location":"package/#danling.models.transformer.attention.multihead_attention.MultiHeadAttention.attention"},{"title":"<code>forward(query, key, value, attn_bias=None, attn_mask=None, key_padding_mask=None, need_weights=False, static_k=None, static_v=None)</code>","text":"<p>Shapes for inputs:     - query: :math:<code>(L, N, E)</code> where L is the target sequence length, N is the batch size, E is         the embedding dimension. :math:<code>(N, L, E)</code> if <code>batch_first</code> is <code>True</code>.     - key: :math:<code>(S, N, E)</code>, where S is the source sequence length, N is the batch size, E is         the embedding dimension. :math:<code>(N, S, E)</code> if <code>batch_first</code> is <code>True</code>.     - value: :math:<code>(S, N, E)</code> where S is the source sequence length, N is the batch size, E is         the embedding dimension. :math:<code>(N, S, E)</code> if <code>batch_first</code> is <code>True</code>.     - attn_bias: if a 2D mask: :math:<code>(L, S)</code> where L is the target sequence length, S is the         source sequence length.         If a 3D mask: :math:<code>(N\\cdot\\text{num\\_heads}, L, S)</code> where N is the batch size, L is the target sequence         length, S is the source sequence length. <code>attn_bias</code> allows to pass pos embed directly into attention         If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will         be unchanged. If a BoolTensor is provided, positions with <code>True</code> is not allowed to attend while <code>False</code>         values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight.     - attn_mask: if a 2D mask: :math:<code>(L, S)</code> where L is the target sequence length, S is the         source sequence length.         If a 3D mask: :math:<code>(N\\cdot\\text{num\\_heads}, L, S)</code> where N is the batch size, L is the target sequence         length, S is the source sequence length. <code>attn_mask</code> ensure that position i is allowed to attend         the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend         while the zero positions will be unchanged. If a BoolTensor is provided, positions with <code>True</code>         is not allowed to attend while <code>False</code> values will be unchanged. If a FloatTensor         is provided, it will be added to the attention weight.     - key_padding_mask: :math:<code>(N, S)</code> where N is the batch size, S is the source sequence length.         If a ByteTensor is provided, the non-zero positions will be ignored while the position         with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the         value of <code>True</code> will be ignored while the position with the value of <code>False</code> will be unchanged.     - static_k: :math:<code>(N*num_heads, S, E/num_heads)</code>, where S is the source sequence length,         N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.     - static_v: :math:<code>(N*num_heads, S, E/num_heads)</code>, where S is the source sequence length,         N is the batch size, E is the embedding dimension. E/num_heads is the head dimension. Outputs:     - attn_output: :math:<code>(L, N, E)</code> where L is the target sequence length, N is the batch size,         E is the embedding dimension. :math:<code>(N, L, E)</code> if <code>batch_first</code> is <code>True</code>.     - attn_output_weights: :math:<code>(N, L, S)</code> where N is the batch size,         L is the target sequence length, S is the source sequence length.</p>  Source code in <code>danling/models/transformer/attention/multihead_attention.py</code> Python<pre><code>def forward(\n    self,\n    query: Tensor,\n    key: Tensor,\n    value: Tensor,\n    attn_bias: Optional[Tensor] = None,\n    attn_mask: Optional[Tensor] = None,\n    key_padding_mask: Optional[Tensor] = None,\n    need_weights: bool = False,\n    static_k: Optional[Tensor] = None,\n    static_v: Optional[Tensor] = None,\n) -&gt; Tuple[Tensor, Optional[Tensor]]:\n    r\"\"\"\n    Args:\n        query, key, value: map a query and a set of key-value pairs to an output.\n            See \"Attention Is All You Need\" for more details.\n        attn_bias: 2D or 3D mask that add bias to attention output weights. Used for relative positional embedding.\n            A 2D bias will be broadcasted for all the batches while a 3D mask allows to specify a different mask for\n            the entries of each batch.\n        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n        key_padding_mask: if provided, specified padding elements in the key will\n            be ignored by the attention. When given a binary mask and a value is True,\n            the corresponding value on the attention layer will be ignored. When given\n            a byte mask and a value is non-zero, the corresponding value on the attention\n            layer will be ignored\n        need_weights: output attn_output_weights.\n        static_k, static_v: static key and value used for attention operators.\n    Shapes for inputs:\n        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n            the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n            the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n            the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n        - attn_bias: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n            source sequence length.\n            If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n            length, S is the source sequence length. ``attn_bias`` allows to pass pos embed directly into attention\n            If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will\n            be unchanged. If a BoolTensor is provided, positions with ``True`` is not allowed to attend while ``False``\n            values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight.\n        - attn_mask: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n            source sequence length.\n            If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n            length, S is the source sequence length. ``attn_mask`` ensure that position i is allowed to attend\n            the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n            while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n            is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n            is provided, it will be added to the attention weight.\n        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n            If a ByteTensor is provided, the non-zero positions will be ignored while the position\n            with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the\n            value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n            N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n            N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n    Outputs:\n        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n            E is the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n            L is the target sequence length, S is the source sequence length.\n    \"\"\"\n    if self.batch_first:\n        query, key, value = [x.transpose(0, 1) for x in (query, key, value)]\n\n    # set up shape vars\n    target_len, batch_size, embed_dim = query.shape\n    source_len, _, _ = key.shape\n    if not key.shape[:2] == value.shape[:2]:\n        raise ValueError(f\"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}\")\n\n    q, k, v = self.in_projection(query, key, value)\n\n    # prep attention mask\n    if attn_mask is not None:\n        if attn_mask.dtype == torch.uint8:\n            warnings.warn(\n                \"attn_mask is of type uint8. This type is deprecated. Please use bool or float tensors instead.\"\n            )\n            attn_mask = attn_mask.to(torch.bool)\n        elif not (attn_mask.is_floating_point() or attn_mask.dtype == torch.bool):\n            raise ValueError(f\"attn_mask should have type float or bool, but got {attn_mask.dtype}.\")\n        # ensure attn_mask's dim is 3\n        if attn_mask.dim() == 2:\n            correct_shape = (target_len, source_len)\n            if attn_mask.shape != correct_shape:\n                raise ValueError(f\"attn_mask should have shape {correct_shape}, but got {attn_mask.shape}.\")\n            attn_mask = attn_mask.unsqueeze(0)\n        elif attn_mask.dim() == 3:\n            correct_shape = (batch_size * self.num_heads, target_len, source_len)  # type: ignore\n            if attn_mask.shape != correct_shape:\n                raise ValueError(f\"attn_mask should have shape {correct_shape}, but got {attn_mask.shape}.\")\n        else:\n            raise RuntimeError(f\"attn_mask should have dimension 2 or 3, bug got {attn_mask.dim()}.\")\n\n    # prep key padding mask\n    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n        warnings.warn(\n            \"key_padding_mask is of type uint8. This type is deprecated. Please use bool or float tensors instead.\"\n        )\n        key_padding_mask = key_padding_mask.to(torch.bool)\n\n    # add bias along batch dimension (currently second)\n    if self.bias_k is not None and self.bias_v is not None:\n        assert static_k is None, \"bias cannot be added to static key.\"\n        assert static_v is None, \"bias cannot be added to static value.\"\n        k = torch.cat([k, self.bias_k.repeat(1, batch_size, 1)])\n        v = torch.cat([v, self.bias_v.repeat(1, batch_size, 1)])\n        if attn_mask is not None:\n            attn_mask = F.pad(attn_mask, (0, 1))\n        if key_padding_mask is not None:\n            key_padding_mask = F.pad(key_padding_mask, (0, 1))\n    else:\n        assert self.bias_k is None\n        assert self.bias_v is None\n\n    # reshape q, k, v for multihead attention and make em batch first\n    q = q.reshape(target_len, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n    k = k.reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1) if static_k is None else static_k\n    v = v.reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1) if static_v is None else static_v\n\n    if static_k is not None:\n        correct_shape = (  # type: ignore\n            batch_size * self.num_heads,\n            static_k.shape[1],\n            self.head_dim,\n        )\n        if static_k.shape != correct_shape:\n            raise ValueError(f\"static_k should have shape {correct_shape}, but got {static_k.shape}.\")\n    if static_v is not None:\n        correct_shape = (  # type: ignore\n            batch_size * self.num_heads,\n            static_v.shape[1],\n            self.head_dim,\n        )\n        if static_v.shape != correct_shape:\n            raise ValueError(f\"static_v should have shape {correct_shape}, but got {static_v.shape}.\")\n\n    # add zero attention along batch dimension (now first)\n    if self.add_zero_attn:\n        zero_attn_shape = (batch_size * self.num_heads, 1, self.head_dim)\n        k = torch.cat([k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1)\n        v = torch.cat([v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1)\n        if attn_mask is not None:\n            attn_mask = F.pad(attn_mask, (0, 1))\n        if key_padding_mask is not None:\n            key_padding_mask = F.pad(key_padding_mask, (0, 1))\n\n    # update source sequence length after adjustments\n    source_len = k.shape[1]\n\n    # merge key padding and attention masks\n    if key_padding_mask is not None:\n        if key_padding_mask.shape != (batch_size, source_len):\n            raise ValueError(\n                f\"key_padding_mask should have shape {(batch_size, source_len)}, but got {key_padding_mask.shape}\"\n            )\n        key_padding_mask = (\n            key_padding_mask.view(batch_size, 1, 1, source_len)\n            .expand(-1, self.num_heads, -1, -1)\n            .reshape(batch_size * self.num_heads, 1, source_len)\n        )\n        if attn_mask is None:\n            attn_mask = key_padding_mask\n        elif attn_mask.dtype == torch.bool:\n            attn_mask = attn_mask.logical_or(key_padding_mask)\n        else:\n            attn_mask = attn_mask.masked_fill(key_padding_mask, float(\"-inf\"))\n\n    # convert mask to float\n    if attn_mask is not None and attn_mask.dtype == torch.bool:\n        new_attn_mask = torch.zeros_like(attn_mask, dtype=torch.float)\n        new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n        attn_mask = new_attn_mask\n\n    # (deep breath) calculate attention and out projection\n    attn_output, attn_output_weights = self.attention(q, k, v, attn_bias, attn_mask)\n    attn_output = attn_output.transpose(0, 1).reshape(target_len, batch_size, embed_dim)\n    attn_output = self.out_projection(attn_output)\n\n    # attn_output_weights is set to torch.empty(0, requires_grad=False) to avoid errors in DDP\n    attn_output_weights = (\n        attn_output_weights.view(batch_size, self.num_heads, target_len, source_len)\n        if need_weights\n        else torch.empty(0, requires_grad=False)\n    )\n\n    if self.batch_first:\n        return attn_output.transpose(0, 1), attn_output_weights\n    else:\n        return attn_output, attn_output_weights\n</code></pre>","location":"package/#danling.models.transformer.attention.multihead_attention.MultiHeadAttention.forward"},{"title":"<code>in_projection(q, k, v)</code>","text":"<p>Performs the in-projection step of the attention operation, using packed weights. Output is a triple containing projection tensors for query, key and value. Args:     q, k, v: query, key and value tensors to be projected. For self-attention,         these are typically the same tensor; for encoder-decoder attention,         k and v are typically the same tensor. (We take advantage of these         identities for performance if they are present.) Regardless, q, k and v         must share a common embedding dimension; otherwise their shapes may vary. Shape:     Inputs:     - q: :math:<code>(..., E)</code> where E is the embedding dimension     - k: :math:<code>(..., E)</code> where E is the embedding dimension     - v: :math:<code>(..., E)</code> where E is the embedding dimension     Output:     - in output list :math:<code>[q', k', v']</code>, each output tensor will have the         same shape as the corresponding input tensor.</p>  Source code in <code>danling/models/transformer/attention/multihead_attention.py</code> Python<pre><code>def in_projection(self, q: Tensor, k: Tensor, v: Tensor) -&gt; Tuple[Tensor, Tensor, Tensor]:\n    r\"\"\"\n    Performs the in-projection step of the attention operation, using packed weights.\n    Output is a triple containing projection tensors for query, key and value.\n    Args:\n        q, k, v: query, key and value tensors to be projected. For self-attention,\n            these are typically the same tensor; for encoder-decoder attention,\n            k and v are typically the same tensor. (We take advantage of these\n            identities for performance if they are present.) Regardless, q, k and v\n            must share a common embedding dimension; otherwise their shapes may vary.\n    Shape:\n        Inputs:\n        - q: :math:`(..., E)` where E is the embedding dimension\n        - k: :math:`(..., E)` where E is the embedding dimension\n        - v: :math:`(..., E)` where E is the embedding dimension\n        Output:\n        - in output list :math:`[q', k', v']`, each output tensor will have the\n            same shape as the corresponding input tensor.\n    \"\"\"\n    if k is v:\n        # self-attention\n        if q is k:\n            return self.in_proj(q).split((self.embed_dim, self.k_dim, self.v_dim), dim=-1)\n        # encoder-decoder attention\n        else:\n            w_q, w_kv = self.in_proj.weight.split([self.embed_dim, self.k_dim + self.v_dim])\n            b_q, b_kv = (\n                None\n                if self.in_proj.bias is None\n                else self.in_proj.bias.split([self.embed_dim, self.k_dim + self.v_dim])\n            )\n            return (F.linear(q, w_q, b_q),) + F.linear(k, w_kv, b_kv).split((self.k_dim, self.v_dim), dim=-1)\n    else:\n        w_q, w_k, w_v = self.in_proj.weight.split([self.embed_dim, self.k_dim, self.v_dim])\n        b_q, b_k, b_v = (\n            None if self.in_proj.bias is None else self.in_proj.bias.split([self.embed_dim, self.k_dim, self.v_dim])\n        )\n        return F.linear(q, w_q, b_q), F.linear(k, w_k, b_k), F.linear(v, w_v, b_v)\n</code></pre>","location":"package/#danling.models.transformer.attention.multihead_attention.MultiHeadAttention.in_projection"},{"title":"<code>NestedTensor</code>","text":"<p>Wrap a sequence of tensors into a single tensor with a mask.</p> <p>In sequence to sequence tasks, elements of a batch are usually not of the same length. This made it tricky to use a single tensor to represent a batch of sequences.</p> <p>NestedTensor allows to store a sequence of tensors of different lengths in a single object. It also provides a mask that can be used to retrieve the original sequence of tensors.</p> <p>Attributes:</p>    Name Type Description     <code>storage</code>  <code>Sequence[torch.Tensor]</code>  <p>The sequence of tensors.</p>   <code>batch_first</code>  <code>bool = True</code>  <p>Whether the first dimension of the tensors is the batch dimension.</p> <p>If <code>True</code>, the first dimension is the batch dimension, i.e., <code>B, N, *</code>.</p> <p>If <code>False</code>, the first dimension is the sequence dimension, i.e., <code>N, B, *</code></p>    <p>Parameters:</p>    Name Type Description Default     <code>tensors</code>  <code>Sequence[Tensor]</code>    required    <code>batch_first</code>  <code>bool</code>    <code>True</code>     <p>Raises:</p>    Type Description      <code>ValueError</code>  <p>If <code>tensors</code> is not a sequence.</p> <p>If <code>tensors</code> is empty.</p>","location":"package/#danling.NestedTensor"},{"title":"Notes","text":"<p>We have rewritten the <code>__getattr__</code> function to support as much native tensor operations as possible. However, not all operations are tested.</p> <p>Please file an issue if you find any bugs.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.device\ndevice(type='cpu')\n&gt;&gt;&gt; nested_tensor.dtype\ntorch.int64\n&gt;&gt;&gt; nested_tensor.tensor\ntensor([[1, 2, 3],\n        [4, 5, 0]])\n&gt;&gt;&gt; nested_tensor.mask\ntensor([[ True,  True,  True],\n        [ True,  True, False]])\n&gt;&gt;&gt; nested_tensor.to(torch.float).tensor\ntensor([[1., 2., 3.],\n        [4., 5., 0.]])\n&gt;&gt;&gt; nested_tensor.half().tensor\ntensor([[1., 2., 3.],\n        [4., 5., 0.]], dtype=torch.float16)\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>class NestedTensor:\n    r\"\"\"\n    Wrap a sequence of tensors into a single tensor with a mask.\n\n    In sequence to sequence tasks, elements of a batch are usually not of the same length.\n    This made it tricky to use a single tensor to represent a batch of sequences.\n\n    NestedTensor allows to store a sequence of tensors of different lengths in a single object.\n    It also provides a mask that can be used to retrieve the original sequence of tensors.\n\n    Attributes\n    ----------\n    storage: Sequence[torch.Tensor]\n        The sequence of tensors.\n    batch_first: bool = True\n        Whether the first dimension of the tensors is the batch dimension.\n\n        If `True`, the first dimension is the batch dimension, i.e., `B, N, *`.\n\n        If `False`, the first dimension is the sequence dimension, i.e., `N, B, *`\n\n    Parameters\n    ----------\n    tensors: Sequence[torch.Tensor]\n    batch_first: bool = True\n\n    Raises\n    ------\n    ValueError\n        If `tensors` is not a sequence.\n\n        If `tensors` is empty.\n\n    Notes\n    -----\n    We have rewritten the `__getattr__` function to support as much native tensor operations as possible.\n    However, not all operations are tested.\n\n    Please file an issue if you find any bugs.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.shape\n    torch.Size([2, 3])\n    &gt;&gt;&gt; nested_tensor.device\n    device(type='cpu')\n    &gt;&gt;&gt; nested_tensor.dtype\n    torch.int64\n    &gt;&gt;&gt; nested_tensor.tensor\n    tensor([[1, 2, 3],\n            [4, 5, 0]])\n    &gt;&gt;&gt; nested_tensor.mask\n    tensor([[ True,  True,  True],\n            [ True,  True, False]])\n    &gt;&gt;&gt; nested_tensor.to(torch.float).tensor\n    tensor([[1., 2., 3.],\n            [4., 5., 0.]])\n    &gt;&gt;&gt; nested_tensor.half().tensor\n    tensor([[1., 2., 3.],\n            [4., 5., 0.]], dtype=torch.float16)\n\n    ```\n    \"\"\"\n\n    storage: Sequence[Tensor] = []\n    batch_first: bool = True\n\n    def __init__(self, tensors: Sequence[Tensor], batch_first: bool = True) -&gt; None:\n        if not isinstance(tensors, Sequence):\n            raise ValueError(f\"NestedTensor should be initialised with a Sequence, bug got {type(values)}.\")\n        if len(tensors) == 0:\n            raise ValueError(f\"NestedTensor should be initialised with a non-empty sequence.\")\n        if not isinstance(tensors[0], Tensor):\n            tensors = tuple(torch.tensor(tensor) for tensor in tensors)\n        self.storage = tensors\n        self.batch_first = batch_first\n\n    @property\n    def tensor(self) -&gt; Tensor:\n        r\"\"\"\n        Return a single tensor by padding all the tensors.\n\n        Returns\n        -------\n        torch.Tensor\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.tensor\n        tensor([[1, 2, 3],\n                [4, 5, 0]])\n\n        ```\n        \"\"\"\n\n        return self._tensor(tuple(self.storage), self.batch_first)\n\n    @property\n    def mask(self) -&gt; Tensor:\n        r\"\"\"\n        Padding mask of `tensor`.\n\n        Returns\n        -------\n        torch.Tensor\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.mask\n        tensor([[ True,  True,  True],\n                [ True,  True, False]])\n\n        ```\n        \"\"\"\n\n        return self._mask(tuple(self.storage))\n\n    @property\n    def device(self) -&gt; torch.device:\n        r\"\"\"\n        Device of the NestedTensor.\n\n        Returns\n        -------\n        torch.Tensor\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.device\n        device(type='cpu')\n\n        ```\n        \"\"\"\n\n        return self._device(tuple(self.storage))\n\n    @property\n    def shape(self) -&gt; torch.Size:\n        r\"\"\"\n        Alias for `size`.\n\n        Returns\n        -------\n        torch.Size\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.shape\n        torch.Size([2, 3])\n        &gt;&gt;&gt; nested_tensor.storage.append(torch.tensor([6, 7, 8, 9]))\n        &gt;&gt;&gt; nested_tensor.shape\n        torch.Size([3, 4])\n\n        ```\n        \"\"\"\n\n        return self.size()\n\n    def size(self) -&gt; torch.Size:\n        r\"\"\"\n        Shape of the NestedTensor.\n\n        Returns\n        -------\n        torch.Size\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.size()\n        torch.Size([2, 3])\n        &gt;&gt;&gt; nested_tensor.storage[1] = torch.tensor([4, 5, 6, 7])\n        &gt;&gt;&gt; nested_tensor.size()\n        torch.Size([2, 4])\n\n        ```\n        \"\"\"\n\n        return self._size(tuple(self.storage))\n\n    def __getitem__(self, index) -&gt; Tuple[Tensor, Tensor]:\n        ret = self.storage[index]\n        if isinstance(ret, Tensor):\n            return ret, torch.ones_like(ret)  # pylint: disable=E1101\n        return self.tensor, self.mask\n\n    def __getattr__(self, name) -&gt; Any:\n        if not self.storage:\n            raise ValueError(f\"Unable to get {name} from an empty {self.__class__.__name__}\")\n        ret = [getattr(i, name) for i in self.storage]\n        elem = ret[0]\n        if isinstance(elem, Tensor):\n            return NestedTensor(ret)\n        if callable(elem):\n            return _TensorFuncWrapper(ret)\n        if len(set(ret)) == 1:\n            return elem\n        return ret\n\n    def __len__(self) -&gt; int:\n        return len(self.storage)\n\n    def __setstate__(self, storage) -&gt; None:\n        self.storage = storage\n\n    def __getstate__(self) -&gt; Sequence[Tensor]:\n        return self.storage\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _tensor(storage, batch_first) -&gt; Tensor:\n        return pad_sequence(storage, batch_first=batch_first)\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _mask(storage) -&gt; Tensor:\n        lens = torch.tensor([len(t) for t in storage], device=storage[0].device)\n        return torch.arange(max(lens), device=storage[0].device)[None, :] &lt; lens[:, None]  # pylint: disable=E1101\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _device(storage) -&gt; torch.device:\n        return storage[0].device\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _size(storage) -&gt; torch.Size:\n        return torch.Size(  # pylint: disable=E1101\n            [len(storage), max(t.shape[0] for t in storage), *storage[0].shape[1:]]\n        )\n</code></pre>","location":"package/#danling.NestedTensor--notes"},{"title":"<code>device()</code>  <code>property</code>","text":"<p>Device of the NestedTensor.</p> <p>Returns:</p>    Type Description      <code>torch.Tensor</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.device\ndevice(type='cpu')\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@property\ndef device(self) -&gt; torch.device:\n    r\"\"\"\n    Device of the NestedTensor.\n\n    Returns\n    -------\n    torch.Tensor\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.device\n    device(type='cpu')\n\n    ```\n    \"\"\"\n\n    return self._device(tuple(self.storage))\n</code></pre>","location":"package/#danling.tensors.nested_tensor.NestedTensor.device"},{"title":"<code>mask()</code>  <code>property</code>","text":"<p>Padding mask of <code>tensor</code>.</p> <p>Returns:</p>    Type Description      <code>torch.Tensor</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.mask\ntensor([[ True,  True,  True],\n        [ True,  True, False]])\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@property\ndef mask(self) -&gt; Tensor:\n    r\"\"\"\n    Padding mask of `tensor`.\n\n    Returns\n    -------\n    torch.Tensor\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.mask\n    tensor([[ True,  True,  True],\n            [ True,  True, False]])\n\n    ```\n    \"\"\"\n\n    return self._mask(tuple(self.storage))\n</code></pre>","location":"package/#danling.tensors.nested_tensor.NestedTensor.mask"},{"title":"<code>shape()</code>  <code>property</code>","text":"<p>Alias for <code>size</code>.</p> <p>Returns:</p>    Type Description      <code>torch.Size</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.storage.append(torch.tensor([6, 7, 8, 9]))\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([3, 4])\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@property\ndef shape(self) -&gt; torch.Size:\n    r\"\"\"\n    Alias for `size`.\n\n    Returns\n    -------\n    torch.Size\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.shape\n    torch.Size([2, 3])\n    &gt;&gt;&gt; nested_tensor.storage.append(torch.tensor([6, 7, 8, 9]))\n    &gt;&gt;&gt; nested_tensor.shape\n    torch.Size([3, 4])\n\n    ```\n    \"\"\"\n\n    return self.size()\n</code></pre>","location":"package/#danling.tensors.nested_tensor.NestedTensor.shape"},{"title":"<code>size()</code>","text":"<p>Shape of the NestedTensor.</p> <p>Returns:</p>    Type Description      <code>torch.Size</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.size()\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.storage[1] = torch.tensor([4, 5, 6, 7])\n&gt;&gt;&gt; nested_tensor.size()\ntorch.Size([2, 4])\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def size(self) -&gt; torch.Size:\n    r\"\"\"\n    Shape of the NestedTensor.\n\n    Returns\n    -------\n    torch.Size\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.size()\n    torch.Size([2, 3])\n    &gt;&gt;&gt; nested_tensor.storage[1] = torch.tensor([4, 5, 6, 7])\n    &gt;&gt;&gt; nested_tensor.size()\n    torch.Size([2, 4])\n\n    ```\n    \"\"\"\n\n    return self._size(tuple(self.storage))\n</code></pre>","location":"package/#danling.tensors.nested_tensor.NestedTensor.size"},{"title":"<code>tensor()</code>  <code>property</code>","text":"<p>Return a single tensor by padding all the tensors.</p> <p>Returns:</p>    Type Description      <code>torch.Tensor</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.tensor\ntensor([[1, 2, 3],\n        [4, 5, 0]])\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@property\ndef tensor(self) -&gt; Tensor:\n    r\"\"\"\n    Return a single tensor by padding all the tensors.\n\n    Returns\n    -------\n    torch.Tensor\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.tensor\n    tensor([[1, 2, 3],\n            [4, 5, 0]])\n\n    ```\n    \"\"\"\n\n    return self._tensor(tuple(self.storage), self.batch_first)\n</code></pre>","location":"package/#danling.tensors.nested_tensor.NestedTensor.tensor"},{"title":"<code>Registry</code>","text":"<p>         Bases: <code>NestedDict</code></p> <p><code>Registry</code> for components.</p>","location":"package/#danling.Registry"},{"title":"Notes","text":"<p><code>Registry</code> inherits from <code>NestedDict</code>.</p> <p>Therefore, <code>Registry</code> comes in a nested structure by nature. You could create a sub-registry by simply calling <code>registry.sub_registry = Registry</code>, and access through <code>registry.sub_registry.register()</code>.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; registry = Registry(\"test\")\n&gt;&gt;&gt; @registry.register\n... @registry.register(\"Module1\")\n... class Module:\n...     def __init__(self, a, b):\n...         self.a = a\n...         self.b = b\n&gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n&gt;&gt;&gt; registry\nRegistry(\n  (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n  (Module): &lt;class 'danling.registry.registry.Module'&gt;\n  (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n)\n&gt;&gt;&gt; registry.lookup(\"Module\")\n&lt;class 'danling.registry.registry.Module'&gt;\n&gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n&gt;&gt;&gt; # registry.register(Module)\n&gt;&gt;&gt; module = registry.build(config[\"module\"])\n&gt;&gt;&gt; type(module)\n&lt;class 'danling.registry.registry.Module'&gt;\n&gt;&gt;&gt; module.a\n1\n&gt;&gt;&gt; module.b\n2\n</code></pre>  Source code in <code>danling/registry/registry.py</code> Python<pre><code>class Registry(NestedDict):\n    \"\"\"\n    `Registry` for components.\n\n    Notes\n    -----\n\n    `Registry` inherits from [`NestedDict`](https://chanfig.danling.org/nested_dict/).\n\n    Therefore, `Registry` comes in a nested structure by nature.\n    You could create a sub-registry by simply calling `registry.sub_registry = Registry`,\n    and access through `registry.sub_registry.register()`.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; registry = Registry(\"test\")\n    &gt;&gt;&gt; @registry.register\n    ... @registry.register(\"Module1\")\n    ... class Module:\n    ...     def __init__(self, a, b):\n    ...         self.a = a\n    ...         self.b = b\n    &gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n    &gt;&gt;&gt; registry\n    Registry(\n      (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n      (Module): &lt;class 'danling.registry.registry.Module'&gt;\n      (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n    )\n    &gt;&gt;&gt; registry.lookup(\"Module\")\n    &lt;class 'danling.registry.registry.Module'&gt;\n    &gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n    &gt;&gt;&gt; # registry.register(Module)\n    &gt;&gt;&gt; module = registry.build(config[\"module\"])\n    &gt;&gt;&gt; type(module)\n    &lt;class 'danling.registry.registry.Module'&gt;\n    &gt;&gt;&gt; module.a\n    1\n    &gt;&gt;&gt; module.b\n    2\n\n    ```\n    \"\"\"\n\n    override: bool = False\n\n    def __init__(self, override: bool = False):\n        super().__init__()\n        self.setattr(\"override\", override)\n\n    def register(self, component: Optional[Callable] = None, name: Optional[str] = None) -&gt; Callable:\n        r\"\"\"\n        Register a new component\n\n        Parameters\n        ----------\n        component: Optional[Callable] = None\n            The component to register.\n        name: Optional[str] = component.__name__\n            The name of the component.\n\n        Returns\n        -------\n        component: Callable\n            The registered component.\n\n        Raises\n        ------\n        ValueError\n            If the component with the same name already exists and `Registry.override=False`.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; registry = Registry(\"test\")\n        &gt;&gt;&gt; @registry.register\n        ... @registry.register(\"Module1\")\n        ... class Module:\n        ...     def __init__(self, a, b):\n        ...         self.a = a\n        ...         self.b = b\n        &gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n        &gt;&gt;&gt; registry\n        Registry(\n          (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n          (Module): &lt;class 'danling.registry.registry.Module'&gt;\n          (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n        )\n\n        ```\n        \"\"\"\n\n        if isinstance(name, str) and name in self and not self.override:\n            raise ValueError(f\"Component with name {name} already exists\")\n\n        # Registry.register()\n        if name is not None:\n            self.set(name, component)\n\n        # @Registry.register()\n        @wraps(self.register)\n        def register(component, name=None):\n            if name is None:\n                name = component.__name__\n            self.set(name, component)\n            return component\n\n        # @Registry.register\n        if callable(component) and name is None:\n            return register(component)\n\n        return lambda x: register(x, component)\n\n    def lookup(self, name: str) -&gt; Any:\n        r\"\"\"\n        Lookup for a component.\n\n        Parameters\n        ----------\n        name: str\n            The name of the component.\n\n        Returns\n        -------\n        value: Any\n\n        Raises\n        ------\n        KeyError\n            If the component is not registered.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; registry = Registry(\"test\")\n        &gt;&gt;&gt; @registry.register\n        ... class Module:\n        ...     def __init__(self, a, b):\n        ...         self.a = a\n        ...         self.b = b\n        &gt;&gt;&gt; registry.lookup(\"Module\")\n        &lt;class 'danling.registry.registry.Module'&gt;\n\n        ```\n        \"\"\"\n\n        return self.get(name)\n\n    def build(self, name: str, *args, **kwargs):\n        r\"\"\"\n        Build a component.\n\n        Parameters\n        ----------\n        name: str\n            The name of the component.\n        *args\n            The arguments to pass to the component.\n        **kwargs\n            The keyword arguments to pass to the component.\n\n        Returns\n        -------\n        component: Callable\n\n        Raises\n        ------\n        KeyError\n            If the component is not registered.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; registry = Registry(\"test\")\n        &gt;&gt;&gt; @registry.register\n        ... class Module:\n        ...     def __init__(self, a, b):\n        ...         self.a = a\n        ...         self.b = b\n        &gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n        &gt;&gt;&gt; # registry.register(Module)\n        &gt;&gt;&gt; module = registry.build(config[\"module\"])\n        &gt;&gt;&gt; type(module)\n        &lt;class 'danling.registry.registry.Module'&gt;\n        &gt;&gt;&gt; module.a\n        1\n        &gt;&gt;&gt; module.b\n        2\n\n        ```\n        \"\"\"\n\n        if isinstance(name, Mapping) and not args and not kwargs:\n            name, kwargs = name.pop(\"name\"), name  # type: ignore\n        return self.get(name)(*args, **kwargs)\n\n    def __wrapped__(self):\n        pass\n</code></pre>","location":"package/#danling.Registry--notes"},{"title":"<code>build(name, *args, **kwargs)</code>","text":"<p>Build a component.</p> <p>Parameters:</p>    Name Type Description Default     <code>name</code>  <code>str</code>  <p>The name of the component.</p>  required    <code>*args</code>   <p>The arguments to pass to the component.</p>  <code>()</code>    <code>**kwargs</code>   <p>The keyword arguments to pass to the component.</p>  <code>{}</code>     <p>Returns:</p>    Name Type Description     <code>component</code>  <code>Callable</code>      <p>Raises:</p>    Type Description      <code>KeyError</code>  <p>If the component is not registered.</p>    <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; registry = Registry(\"test\")\n&gt;&gt;&gt; @registry.register\n... class Module:\n...     def __init__(self, a, b):\n...         self.a = a\n...         self.b = b\n&gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n&gt;&gt;&gt; # registry.register(Module)\n&gt;&gt;&gt; module = registry.build(config[\"module\"])\n&gt;&gt;&gt; type(module)\n&lt;class 'danling.registry.registry.Module'&gt;\n&gt;&gt;&gt; module.a\n1\n&gt;&gt;&gt; module.b\n2\n</code></pre>  Source code in <code>danling/registry/registry.py</code> Python<pre><code>def build(self, name: str, *args, **kwargs):\n    r\"\"\"\n    Build a component.\n\n    Parameters\n    ----------\n    name: str\n        The name of the component.\n    *args\n        The arguments to pass to the component.\n    **kwargs\n        The keyword arguments to pass to the component.\n\n    Returns\n    -------\n    component: Callable\n\n    Raises\n    ------\n    KeyError\n        If the component is not registered.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; registry = Registry(\"test\")\n    &gt;&gt;&gt; @registry.register\n    ... class Module:\n    ...     def __init__(self, a, b):\n    ...         self.a = a\n    ...         self.b = b\n    &gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n    &gt;&gt;&gt; # registry.register(Module)\n    &gt;&gt;&gt; module = registry.build(config[\"module\"])\n    &gt;&gt;&gt; type(module)\n    &lt;class 'danling.registry.registry.Module'&gt;\n    &gt;&gt;&gt; module.a\n    1\n    &gt;&gt;&gt; module.b\n    2\n\n    ```\n    \"\"\"\n\n    if isinstance(name, Mapping) and not args and not kwargs:\n        name, kwargs = name.pop(\"name\"), name  # type: ignore\n    return self.get(name)(*args, **kwargs)\n</code></pre>","location":"package/#danling.registry.registry.Registry.build"},{"title":"<code>lookup(name)</code>","text":"<p>Lookup for a component.</p> <p>Parameters:</p>    Name Type Description Default     <code>name</code>  <code>str</code>  <p>The name of the component.</p>  required     <p>Returns:</p>    Name Type Description     <code>value</code>  <code>Any</code>      <p>Raises:</p>    Type Description      <code>KeyError</code>  <p>If the component is not registered.</p>    <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; registry = Registry(\"test\")\n&gt;&gt;&gt; @registry.register\n... class Module:\n...     def __init__(self, a, b):\n...         self.a = a\n...         self.b = b\n&gt;&gt;&gt; registry.lookup(\"Module\")\n&lt;class 'danling.registry.registry.Module'&gt;\n</code></pre>  Source code in <code>danling/registry/registry.py</code> Python<pre><code>def lookup(self, name: str) -&gt; Any:\n    r\"\"\"\n    Lookup for a component.\n\n    Parameters\n    ----------\n    name: str\n        The name of the component.\n\n    Returns\n    -------\n    value: Any\n\n    Raises\n    ------\n    KeyError\n        If the component is not registered.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; registry = Registry(\"test\")\n    &gt;&gt;&gt; @registry.register\n    ... class Module:\n    ...     def __init__(self, a, b):\n    ...         self.a = a\n    ...         self.b = b\n    &gt;&gt;&gt; registry.lookup(\"Module\")\n    &lt;class 'danling.registry.registry.Module'&gt;\n\n    ```\n    \"\"\"\n\n    return self.get(name)\n</code></pre>","location":"package/#danling.registry.registry.Registry.lookup"},{"title":"<code>register(component=None, name=None)</code>","text":"<p>Register a new component</p> <p>Parameters:</p>    Name Type Description Default     <code>component</code>  <code>Optional[Callable]</code>  <p>The component to register.</p>  <code>None</code>    <code>name</code>  <code>Optional[str]</code>  <p>The name of the component.</p>  <code>None</code>     <p>Returns:</p>    Name Type Description     <code>component</code>  <code>Callable</code>  <p>The registered component.</p>    <p>Raises:</p>    Type Description      <code>ValueError</code>  <p>If the component with the same name already exists and <code>Registry.override=False</code>.</p>    <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; registry = Registry(\"test\")\n&gt;&gt;&gt; @registry.register\n... @registry.register(\"Module1\")\n... class Module:\n...     def __init__(self, a, b):\n...         self.a = a\n...         self.b = b\n&gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n&gt;&gt;&gt; registry\nRegistry(\n  (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n  (Module): &lt;class 'danling.registry.registry.Module'&gt;\n  (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n)\n</code></pre>  Source code in <code>danling/registry/registry.py</code> Python<pre><code>def register(self, component: Optional[Callable] = None, name: Optional[str] = None) -&gt; Callable:\n    r\"\"\"\n    Register a new component\n\n    Parameters\n    ----------\n    component: Optional[Callable] = None\n        The component to register.\n    name: Optional[str] = component.__name__\n        The name of the component.\n\n    Returns\n    -------\n    component: Callable\n        The registered component.\n\n    Raises\n    ------\n    ValueError\n        If the component with the same name already exists and `Registry.override=False`.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; registry = Registry(\"test\")\n    &gt;&gt;&gt; @registry.register\n    ... @registry.register(\"Module1\")\n    ... class Module:\n    ...     def __init__(self, a, b):\n    ...         self.a = a\n    ...         self.b = b\n    &gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n    &gt;&gt;&gt; registry\n    Registry(\n      (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n      (Module): &lt;class 'danling.registry.registry.Module'&gt;\n      (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n    )\n\n    ```\n    \"\"\"\n\n    if isinstance(name, str) and name in self and not self.override:\n        raise ValueError(f\"Component with name {name} already exists\")\n\n    # Registry.register()\n    if name is not None:\n        self.set(name, component)\n\n    # @Registry.register()\n    @wraps(self.register)\n    def register(component, name=None):\n        if name is None:\n            name = component.__name__\n        self.set(name, component)\n        return component\n\n    # @Registry.register\n    if callable(component) and name is None:\n        return register(component)\n\n    return lambda x: register(x, component)\n</code></pre>","location":"package/#danling.registry.registry.Registry.register"},{"title":"<code>SelfAttention</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Allows the model to jointly attend to information from different representation subspaces. See <code>Attention Is All You Need &lt;https://arxiv.org/abs/1706.03762&gt;</code>_ .. math::     \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O where :math:<code>head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)</code>. Args:     embed_dim: total dimension of the model.     num_heads: parallel attention heads.     dropout: a Dropout layer on attn_output_weights. Default: 0.0.     bias: add bias as module parameter. Default: True.     add_bias_kv: add bias to the key and value sequences at dim=0.     add_zero_attn: add a new batch of zeros to the key and                    value sequences at dim=1.     k_dim: total number of features in key. Default: None.     v_dim: total number of features in value. Default: None.     batch_first: If <code>False</code>, then the input and output tensors are provided         as (seq, batch, feature). Default: <code>True</code> (batch, seq, feature). Note that if :attr:<code>k_dim</code> and :attr:<code>v_dim</code> are None, they will be set to :attr:<code>embed_dim</code> such that query, key, and value have the same number of features. Examples::     &gt;&gt;&gt; multihead_attn = dl.models.SelfAttention(embed_dim, num_heads)     &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)</p>  Source code in <code>danling/models/transformer/attention/self_attention.py</code> Python<pre><code>class SelfAttention(nn.Module):\n    r\"\"\"Allows the model to jointly attend to information\n    from different representation subspaces.\n    See `Attention Is All You Need &lt;https://arxiv.org/abs/1706.03762&gt;`_\n    .. math::\n        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n    where :math:`head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n    Args:\n        embed_dim: total dimension of the model.\n        num_heads: parallel attention heads.\n        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n        bias: add bias as module parameter. Default: True.\n        add_bias_kv: add bias to the key and value sequences at dim=0.\n        add_zero_attn: add a new batch of zeros to the key and\n                       value sequences at dim=1.\n        k_dim: total number of features in key. Default: None.\n        v_dim: total number of features in value. Default: None.\n        batch_first: If ``False``, then the input and output tensors are provided\n            as (seq, batch, feature). Default: ``True`` (batch, seq, feature).\n    Note that if :attr:`k_dim` and :attr:`v_dim` are None, they will be set\n    to :attr:`embed_dim` such that query, key, and value have the same\n    number of features.\n    Examples::\n        &gt;&gt;&gt; multihead_attn = dl.models.SelfAttention(embed_dim, num_heads)\n        &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)\n    \"\"\"\n    __constants__ = [\"batch_first\"]\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        attn_dropout: float = 0.0,\n        scale_factor: float = 1.0,\n        bias: bool = True,\n        batch_first: bool = True,\n        **kwargs: Optional[Dict[str, Any]],\n    ) -&gt; None:\n        super(SelfAttention, self).__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.scale_factor = scale_factor\n        self.batch_first = batch_first\n        self.head_dim = self.embed_dim // self.num_heads\n        self.scaling = float(self.head_dim * self.scale_factor) ** -0.5\n        if not self.head_dim * self.num_heads == self.embed_dim:\n            raise ValueError(f\"embed_dim {self.embed_dim} not divisible by num_heads {self.num_heads}\")\n\n        self.in_proj = nn.Linear(self.embed_dim, self.embed_dim + self.k_dim + self.v_dim, bias=bias)\n        self.dropout = nn.Dropout(attn_dropout)\n        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=bias)\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        nn.init.xavier_uniform_(self.in_proj.weight)\n        if self.in_proj.bias is not None:\n            nn.init.constant_(self.in_proj.bias, 0.0)\n            nn.init.constant_(self.out_proj.bias, 0.0)\n\n    def forward(\n        self,\n        query: Tensor,\n        key: Tensor,\n        value: Tensor,\n        attn_bias: Optional[Tensor] = None,\n        attn_mask: Optional[Tensor] = None,\n        key_padding_mask: Optional[Tensor] = None,\n        need_weights: bool = False,\n    ) -&gt; Tuple[Tensor, Optional[Tensor]]:\n        r\"\"\"\n        Args:\n            query, key, value: map a query and a set of key-value pairs to an output.\n                See \"Attention Is All You Need\" for more details.\n            attn_bias: 2D or 3D mask that add bias to attention output weights. Used for relative positional embedding.\n                A 2D bias will be broadcasted for all the batches while a 3D mask allows to specify a different mask for\n                the entries of each batch.\n            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n                the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n            key_padding_mask: if provided, specified padding elements in the key will\n                be ignored by the attention. When given a binary mask and a value is True,\n                the corresponding value on the attention layer will be ignored. When given\n                a byte mask and a value is non-zero, the corresponding value on the attention\n                layer will be ignored\n            need_weights: output attn_output_weights.\n        Shapes for inputs:\n            - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n                the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n            - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n                the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n            - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n                the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n            - attn_bias: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n                source sequence length.\n                If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n                length, S is the source sequence length. ``attn_bias`` allows to pass pos embed directly into attention\n                If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will\n                be unchanged. If a BoolTensor is provided, positions with ``True`` is not allowed to attend while ``False``\n                values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight.\n            - attn_mask: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n                source sequence length.\n                If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n                length, S is the source sequence length. ``attn_mask`` ensure that position i is allowed to attend\n                the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n                while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n                is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n                is provided, it will be added to the attention weight.\n            - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n                If a ByteTensor is provided, the non-zero positions will be ignored while the position\n                with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the\n                value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n        Outputs:\n            - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n                E is the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n            - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n                L is the target sequence length, S is the source sequence length.\n        \"\"\"\n        if self.batch_first:\n            query, key, value = [x.transpose(0, 1) for x in (query, key, value)]\n\n        # set up shape vars\n        target_len, batch_size, embed_dim = query.shape\n        source_len, _, _ = key.shape\n        if not key.shape[:2] == value.shape[:2]:\n            raise ValueError(f\"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}\")\n\n        q, k, v = self.in_projection(query, key, value)\n\n        # prep attention mask\n        if attn_mask is not None:\n            if attn_mask.dtype == torch.uint8:\n                warnings.warn(\n                    \"attn_mask is of type uint8. This type is deprecated. Please use bool or float tensors instead.\"\n                )\n                attn_mask = attn_mask.to(torch.bool)\n            elif not (attn_mask.is_floating_point() or attn_mask.dtype == torch.bool):\n                raise ValueError(f\"attn_mask should have type float or bool, but got {attn_mask.dtype}.\")\n            # ensure attn_mask's dim is 3\n            if attn_mask.dim() == 2:\n                correct_shape = (target_len, source_len)\n                if attn_mask.shape != correct_shape:\n                    raise ValueError(f\"attn_mask should have shape {correct_shape}, but got {attn_mask.shape}.\")\n                attn_mask = attn_mask.unsqueeze(0)\n            elif attn_mask.dim() == 3:\n                correct_shape = (batch_size * self.num_heads, target_len, source_len)  # type: ignore\n                if attn_mask.shape != correct_shape:\n                    raise ValueError(f\"attn_mask should have shape {correct_shape}, but got {attn_mask.shape}.\")\n            else:\n                raise RuntimeError(f\"attn_mask should have dimension 2 or 3, bug got {attn_mask.dim()}.\")\n\n        # prep key padding mask\n        if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n            warnings.warn(\n                \"key_padding_mask is of type uint8. This type is deprecated. Please use bool or float tensors instead.\"\n            )\n            key_padding_mask = key_padding_mask.to(torch.bool)\n\n        # reshape q, k, v for multihead attention and make em batch first\n        q = q.reshape(target_len, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n        k = k.reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n        v = v.reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n\n        # merge key padding and attention masks\n        if key_padding_mask is not None:\n            if key_padding_mask.shape != (batch_size, source_len):\n                raise ValueError(\n                    f\"key_padding_mask should have shape {(batch_size, source_len)}, but got {key_padding_mask.shape}\"\n                )\n            key_padding_mask = (\n                key_padding_mask.view(batch_size, 1, 1, source_len)\n                .expand(-1, self.num_heads, -1, -1)\n                .reshape(batch_size * self.num_heads, 1, source_len)\n            )\n            if attn_mask is None:\n                attn_mask = key_padding_mask\n            elif attn_mask.dtype == torch.bool:\n                attn_mask = attn_mask.logical_or(key_padding_mask)\n            else:\n                attn_mask = attn_mask.masked_fill(key_padding_mask, float(\"-inf\"))\n\n        # convert mask to float\n        if attn_mask is not None and attn_mask.dtype == torch.bool:\n            new_attn_mask = torch.zeros_like(attn_mask, dtype=torch.float)\n            new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n            attn_mask = new_attn_mask\n\n        # (deep breath) calculate attention and out projection\n        attn_output, attn_output_weights = self.attention(q, k, v, attn_bias, attn_mask)\n        attn_output = attn_output.transpose(0, 1).reshape(target_len, batch_size, embed_dim)\n        attn_output = self.out_projection(attn_output)\n\n        # attn_output_weights is set to torch.empty(0, requires_grad=False) to avoid errors in DDP\n        attn_output_weights = (\n            attn_output_weights.view(batch_size, self.num_heads, target_len, source_len)\n            if need_weights\n            else torch.empty(0, requires_grad=False)\n        )\n\n        if self.batch_first:\n            return attn_output.transpose(0, 1), attn_output_weights\n        else:\n            return attn_output, attn_output_weights\n\n    def in_projection(self, q: Tensor, k: Tensor, v: Tensor) -&gt; Tuple[Tensor, Tensor, Tensor]:\n        r\"\"\"\n        Performs the in-projection step of the attention operation, using packed weights.\n        Output is a triple containing projection tensors for query, key and value.\n        Args:\n            q, k, v: query, key and value tensors to be projected. For self-attention,\n                these are typically the same tensor; for encoder-decoder attention,\n                k and v are typically the same tensor. (We take advantage of these\n                identities for performance if they are present.) Regardless, q, k and v\n                must share a common embedding dimension; otherwise their shapes may vary.\n        Shape:\n            Inputs:\n            - q: :math:`(..., E)` where E is the embedding dimension\n            - k: :math:`(..., E)` where E is the embedding dimension\n            - v: :math:`(..., E)` where E is the embedding dimension\n            Output:\n            - in output list :math:`[q', k', v']`, each output tensor will have the\n                same shape as the corresponding input tensor.\n        \"\"\"\n        if k is v:\n            # self-attention\n            if q is k:\n                return self.in_proj(q).split((self.embed_dim, self.k_dim, self.v_dim), dim=-1)\n            # encoder-decoder attention\n            else:\n                w_q, w_kv = self.in_proj.weight.split([self.embed_dim, self.k_dim + self.v_dim])\n                b_q, b_kv = (\n                    None\n                    if self.in_proj.bias is None\n                    else self.in_proj.bias.split([self.embed_dim, self.k_dim + self.v_dim])\n                )\n                return (F.linear(q, w_q, b_q),) + F.linear(k, w_kv, b_kv).split((self.k_dim, self.v_dim), dim=-1)\n        else:\n            w_q, w_k, w_v = self.in_proj.weight.split([self.embed_dim, self.k_dim, self.v_dim])\n            b_q, b_k, b_v = (\n                None if self.in_proj.bias is None else self.in_proj.bias.split([self.embed_dim, self.k_dim, self.v_dim])\n            )\n            return F.linear(q, w_q, b_q), F.linear(k, w_k, b_k), F.linear(v, w_v, b_v)\n\n    def attention(\n        self,\n        q: Tensor,\n        k: Tensor,\n        v: Tensor,\n        attn_bias: Optional[Tensor] = None,\n        attn_mask: Optional[Tensor] = None,\n    ) -&gt; Tuple[Tensor, Tensor]:\n        r\"\"\"\n        Computes scaled dot product attention on query, key and value tensors, using\n        an optional attention mask if passed, and applying dropout if a probability\n        greater than 0.0 is specified.\n        Returns a tensor pair containing attended values and attention weights.\n        Args:\n            q, k, v: query, key and value tensors. See Shape section for shape details.\n            attn_mask: optional tensor containing mask values to be added to calculated\n                attention. May be 2D or 3D; see Shape section for details.\n            attn_bias: optional tensor containing bias values to be added to calculated\n                attention. Used for relative positional embedding. May be 2D or 3D; see\n                Shape section for details.\n        Shape:\n            - q: :math:`(B, Nt, E)` where B is batch size, Nt is the target sequence length,\n                and E is embedding dimension.\n            - key: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n                and E is embedding dimension.\n            - value: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n                and E is embedding dimension.\n            - attn_bias: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n                shape :math:`(Nt, Ns)`.\n            - attn_mask: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n                shape :math:`(Nt, Ns)`.\n            - Output: attention values have shape :math:`(B, Nt, E)`; attention weights\n                have shape :math:`(B, Nt, Ns)`\n        \"\"\"\n        q *= self.scaling\n        # (B, Nt, E) x (B, E, Ns) -&gt; (B, Nt, Ns)\n        attn = torch.bmm(q, k.transpose(-2, -1))\n        if attn_bias is not None:\n            attn += attn_bias\n        if attn_mask is not None:\n            attn += attn_mask\n        attn = F.softmax(attn, dim=-1)\n        attn = self.dropout(attn)\n        # (B, Nt, Ns) x (B, Ns, E) -&gt; (B, Nt, E)\n        output = torch.bmm(attn, v)\n        return output, attn\n\n    def out_projection(self, attn_output: Tensor) -&gt; Tensor:\n        return self.out_proj(attn_output)\n</code></pre>","location":"package/#danling.SelfAttention"},{"title":"<code>attention(q, k, v, attn_bias=None, attn_mask=None)</code>","text":"<p>Computes scaled dot product attention on query, key and value tensors, using an optional attention mask if passed, and applying dropout if a probability greater than 0.0 is specified. Returns a tensor pair containing attended values and attention weights. Args:     q, k, v: query, key and value tensors. See Shape section for shape details.     attn_mask: optional tensor containing mask values to be added to calculated         attention. May be 2D or 3D; see Shape section for details.     attn_bias: optional tensor containing bias values to be added to calculated         attention. Used for relative positional embedding. May be 2D or 3D; see         Shape section for details. Shape:     - q: :math:<code>(B, Nt, E)</code> where B is batch size, Nt is the target sequence length,         and E is embedding dimension.     - key: :math:<code>(B, Ns, E)</code> where B is batch size, Ns is the source sequence length,         and E is embedding dimension.     - value: :math:<code>(B, Ns, E)</code> where B is batch size, Ns is the source sequence length,         and E is embedding dimension.     - attn_bias: either a 3D tensor of shape :math:<code>(B, Nt, Ns)</code> or a 2D tensor of         shape :math:<code>(Nt, Ns)</code>.     - attn_mask: either a 3D tensor of shape :math:<code>(B, Nt, Ns)</code> or a 2D tensor of         shape :math:<code>(Nt, Ns)</code>.     - Output: attention values have shape :math:<code>(B, Nt, E)</code>; attention weights         have shape :math:<code>(B, Nt, Ns)</code></p>  Source code in <code>danling/models/transformer/attention/self_attention.py</code> Python<pre><code>def attention(\n    self,\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    attn_bias: Optional[Tensor] = None,\n    attn_mask: Optional[Tensor] = None,\n) -&gt; Tuple[Tensor, Tensor]:\n    r\"\"\"\n    Computes scaled dot product attention on query, key and value tensors, using\n    an optional attention mask if passed, and applying dropout if a probability\n    greater than 0.0 is specified.\n    Returns a tensor pair containing attended values and attention weights.\n    Args:\n        q, k, v: query, key and value tensors. See Shape section for shape details.\n        attn_mask: optional tensor containing mask values to be added to calculated\n            attention. May be 2D or 3D; see Shape section for details.\n        attn_bias: optional tensor containing bias values to be added to calculated\n            attention. Used for relative positional embedding. May be 2D or 3D; see\n            Shape section for details.\n    Shape:\n        - q: :math:`(B, Nt, E)` where B is batch size, Nt is the target sequence length,\n            and E is embedding dimension.\n        - key: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n            and E is embedding dimension.\n        - value: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n            and E is embedding dimension.\n        - attn_bias: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n            shape :math:`(Nt, Ns)`.\n        - attn_mask: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n            shape :math:`(Nt, Ns)`.\n        - Output: attention values have shape :math:`(B, Nt, E)`; attention weights\n            have shape :math:`(B, Nt, Ns)`\n    \"\"\"\n    q *= self.scaling\n    # (B, Nt, E) x (B, E, Ns) -&gt; (B, Nt, Ns)\n    attn = torch.bmm(q, k.transpose(-2, -1))\n    if attn_bias is not None:\n        attn += attn_bias\n    if attn_mask is not None:\n        attn += attn_mask\n    attn = F.softmax(attn, dim=-1)\n    attn = self.dropout(attn)\n    # (B, Nt, Ns) x (B, Ns, E) -&gt; (B, Nt, E)\n    output = torch.bmm(attn, v)\n    return output, attn\n</code></pre>","location":"package/#danling.models.transformer.attention.self_attention.SelfAttention.attention"},{"title":"<code>forward(query, key, value, attn_bias=None, attn_mask=None, key_padding_mask=None, need_weights=False)</code>","text":"<p>Shapes for inputs:     - query: :math:<code>(L, N, E)</code> where L is the target sequence length, N is the batch size, E is         the embedding dimension. :math:<code>(N, L, E)</code> if <code>batch_first</code> is <code>True</code>.     - key: :math:<code>(S, N, E)</code>, where S is the source sequence length, N is the batch size, E is         the embedding dimension. :math:<code>(N, S, E)</code> if <code>batch_first</code> is <code>True</code>.     - value: :math:<code>(S, N, E)</code> where S is the source sequence length, N is the batch size, E is         the embedding dimension. :math:<code>(N, S, E)</code> if <code>batch_first</code> is <code>True</code>.     - attn_bias: if a 2D mask: :math:<code>(L, S)</code> where L is the target sequence length, S is the         source sequence length.         If a 3D mask: :math:<code>(N\\cdot\\text{num\\_heads}, L, S)</code> where N is the batch size, L is the target sequence         length, S is the source sequence length. <code>attn_bias</code> allows to pass pos embed directly into attention         If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will         be unchanged. If a BoolTensor is provided, positions with <code>True</code> is not allowed to attend while <code>False</code>         values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight.     - attn_mask: if a 2D mask: :math:<code>(L, S)</code> where L is the target sequence length, S is the         source sequence length.         If a 3D mask: :math:<code>(N\\cdot\\text{num\\_heads}, L, S)</code> where N is the batch size, L is the target sequence         length, S is the source sequence length. <code>attn_mask</code> ensure that position i is allowed to attend         the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend         while the zero positions will be unchanged. If a BoolTensor is provided, positions with <code>True</code>         is not allowed to attend while <code>False</code> values will be unchanged. If a FloatTensor         is provided, it will be added to the attention weight.     - key_padding_mask: :math:<code>(N, S)</code> where N is the batch size, S is the source sequence length.         If a ByteTensor is provided, the non-zero positions will be ignored while the position         with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the         value of <code>True</code> will be ignored while the position with the value of <code>False</code> will be unchanged. Outputs:     - attn_output: :math:<code>(L, N, E)</code> where L is the target sequence length, N is the batch size,         E is the embedding dimension. :math:<code>(N, L, E)</code> if <code>batch_first</code> is <code>True</code>.     - attn_output_weights: :math:<code>(N, L, S)</code> where N is the batch size,         L is the target sequence length, S is the source sequence length.</p>  Source code in <code>danling/models/transformer/attention/self_attention.py</code> Python<pre><code>def forward(\n    self,\n    query: Tensor,\n    key: Tensor,\n    value: Tensor,\n    attn_bias: Optional[Tensor] = None,\n    attn_mask: Optional[Tensor] = None,\n    key_padding_mask: Optional[Tensor] = None,\n    need_weights: bool = False,\n) -&gt; Tuple[Tensor, Optional[Tensor]]:\n    r\"\"\"\n    Args:\n        query, key, value: map a query and a set of key-value pairs to an output.\n            See \"Attention Is All You Need\" for more details.\n        attn_bias: 2D or 3D mask that add bias to attention output weights. Used for relative positional embedding.\n            A 2D bias will be broadcasted for all the batches while a 3D mask allows to specify a different mask for\n            the entries of each batch.\n        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n        key_padding_mask: if provided, specified padding elements in the key will\n            be ignored by the attention. When given a binary mask and a value is True,\n            the corresponding value on the attention layer will be ignored. When given\n            a byte mask and a value is non-zero, the corresponding value on the attention\n            layer will be ignored\n        need_weights: output attn_output_weights.\n    Shapes for inputs:\n        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n            the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n            the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n            the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n        - attn_bias: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n            source sequence length.\n            If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n            length, S is the source sequence length. ``attn_bias`` allows to pass pos embed directly into attention\n            If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will\n            be unchanged. If a BoolTensor is provided, positions with ``True`` is not allowed to attend while ``False``\n            values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight.\n        - attn_mask: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n            source sequence length.\n            If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n            length, S is the source sequence length. ``attn_mask`` ensure that position i is allowed to attend\n            the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n            while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n            is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n            is provided, it will be added to the attention weight.\n        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n            If a ByteTensor is provided, the non-zero positions will be ignored while the position\n            with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the\n            value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n    Outputs:\n        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n            E is the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n            L is the target sequence length, S is the source sequence length.\n    \"\"\"\n    if self.batch_first:\n        query, key, value = [x.transpose(0, 1) for x in (query, key, value)]\n\n    # set up shape vars\n    target_len, batch_size, embed_dim = query.shape\n    source_len, _, _ = key.shape\n    if not key.shape[:2] == value.shape[:2]:\n        raise ValueError(f\"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}\")\n\n    q, k, v = self.in_projection(query, key, value)\n\n    # prep attention mask\n    if attn_mask is not None:\n        if attn_mask.dtype == torch.uint8:\n            warnings.warn(\n                \"attn_mask is of type uint8. This type is deprecated. Please use bool or float tensors instead.\"\n            )\n            attn_mask = attn_mask.to(torch.bool)\n        elif not (attn_mask.is_floating_point() or attn_mask.dtype == torch.bool):\n            raise ValueError(f\"attn_mask should have type float or bool, but got {attn_mask.dtype}.\")\n        # ensure attn_mask's dim is 3\n        if attn_mask.dim() == 2:\n            correct_shape = (target_len, source_len)\n            if attn_mask.shape != correct_shape:\n                raise ValueError(f\"attn_mask should have shape {correct_shape}, but got {attn_mask.shape}.\")\n            attn_mask = attn_mask.unsqueeze(0)\n        elif attn_mask.dim() == 3:\n            correct_shape = (batch_size * self.num_heads, target_len, source_len)  # type: ignore\n            if attn_mask.shape != correct_shape:\n                raise ValueError(f\"attn_mask should have shape {correct_shape}, but got {attn_mask.shape}.\")\n        else:\n            raise RuntimeError(f\"attn_mask should have dimension 2 or 3, bug got {attn_mask.dim()}.\")\n\n    # prep key padding mask\n    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n        warnings.warn(\n            \"key_padding_mask is of type uint8. This type is deprecated. Please use bool or float tensors instead.\"\n        )\n        key_padding_mask = key_padding_mask.to(torch.bool)\n\n    # reshape q, k, v for multihead attention and make em batch first\n    q = q.reshape(target_len, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n    k = k.reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n    v = v.reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n\n    # merge key padding and attention masks\n    if key_padding_mask is not None:\n        if key_padding_mask.shape != (batch_size, source_len):\n            raise ValueError(\n                f\"key_padding_mask should have shape {(batch_size, source_len)}, but got {key_padding_mask.shape}\"\n            )\n        key_padding_mask = (\n            key_padding_mask.view(batch_size, 1, 1, source_len)\n            .expand(-1, self.num_heads, -1, -1)\n            .reshape(batch_size * self.num_heads, 1, source_len)\n        )\n        if attn_mask is None:\n            attn_mask = key_padding_mask\n        elif attn_mask.dtype == torch.bool:\n            attn_mask = attn_mask.logical_or(key_padding_mask)\n        else:\n            attn_mask = attn_mask.masked_fill(key_padding_mask, float(\"-inf\"))\n\n    # convert mask to float\n    if attn_mask is not None and attn_mask.dtype == torch.bool:\n        new_attn_mask = torch.zeros_like(attn_mask, dtype=torch.float)\n        new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n        attn_mask = new_attn_mask\n\n    # (deep breath) calculate attention and out projection\n    attn_output, attn_output_weights = self.attention(q, k, v, attn_bias, attn_mask)\n    attn_output = attn_output.transpose(0, 1).reshape(target_len, batch_size, embed_dim)\n    attn_output = self.out_projection(attn_output)\n\n    # attn_output_weights is set to torch.empty(0, requires_grad=False) to avoid errors in DDP\n    attn_output_weights = (\n        attn_output_weights.view(batch_size, self.num_heads, target_len, source_len)\n        if need_weights\n        else torch.empty(0, requires_grad=False)\n    )\n\n    if self.batch_first:\n        return attn_output.transpose(0, 1), attn_output_weights\n    else:\n        return attn_output, attn_output_weights\n</code></pre>","location":"package/#danling.models.transformer.attention.self_attention.SelfAttention.forward"},{"title":"<code>in_projection(q, k, v)</code>","text":"<p>Performs the in-projection step of the attention operation, using packed weights. Output is a triple containing projection tensors for query, key and value. Args:     q, k, v: query, key and value tensors to be projected. For self-attention,         these are typically the same tensor; for encoder-decoder attention,         k and v are typically the same tensor. (We take advantage of these         identities for performance if they are present.) Regardless, q, k and v         must share a common embedding dimension; otherwise their shapes may vary. Shape:     Inputs:     - q: :math:<code>(..., E)</code> where E is the embedding dimension     - k: :math:<code>(..., E)</code> where E is the embedding dimension     - v: :math:<code>(..., E)</code> where E is the embedding dimension     Output:     - in output list :math:<code>[q', k', v']</code>, each output tensor will have the         same shape as the corresponding input tensor.</p>  Source code in <code>danling/models/transformer/attention/self_attention.py</code> Python<pre><code>def in_projection(self, q: Tensor, k: Tensor, v: Tensor) -&gt; Tuple[Tensor, Tensor, Tensor]:\n    r\"\"\"\n    Performs the in-projection step of the attention operation, using packed weights.\n    Output is a triple containing projection tensors for query, key and value.\n    Args:\n        q, k, v: query, key and value tensors to be projected. For self-attention,\n            these are typically the same tensor; for encoder-decoder attention,\n            k and v are typically the same tensor. (We take advantage of these\n            identities for performance if they are present.) Regardless, q, k and v\n            must share a common embedding dimension; otherwise their shapes may vary.\n    Shape:\n        Inputs:\n        - q: :math:`(..., E)` where E is the embedding dimension\n        - k: :math:`(..., E)` where E is the embedding dimension\n        - v: :math:`(..., E)` where E is the embedding dimension\n        Output:\n        - in output list :math:`[q', k', v']`, each output tensor will have the\n            same shape as the corresponding input tensor.\n    \"\"\"\n    if k is v:\n        # self-attention\n        if q is k:\n            return self.in_proj(q).split((self.embed_dim, self.k_dim, self.v_dim), dim=-1)\n        # encoder-decoder attention\n        else:\n            w_q, w_kv = self.in_proj.weight.split([self.embed_dim, self.k_dim + self.v_dim])\n            b_q, b_kv = (\n                None\n                if self.in_proj.bias is None\n                else self.in_proj.bias.split([self.embed_dim, self.k_dim + self.v_dim])\n            )\n            return (F.linear(q, w_q, b_q),) + F.linear(k, w_kv, b_kv).split((self.k_dim, self.v_dim), dim=-1)\n    else:\n        w_q, w_k, w_v = self.in_proj.weight.split([self.embed_dim, self.k_dim, self.v_dim])\n        b_q, b_k, b_v = (\n            None if self.in_proj.bias is None else self.in_proj.bias.split([self.embed_dim, self.k_dim, self.v_dim])\n        )\n        return F.linear(q, w_q, b_q), F.linear(k, w_k, b_k), F.linear(v, w_v, b_v)\n</code></pre>","location":"package/#danling.models.transformer.attention.self_attention.SelfAttention.in_projection"},{"title":"<code>TorchRunner</code>","text":"<p>         Bases: <code>BaseRunner</code></p> <p>Set up everything for running a job.</p> <p>Attributes:</p>    Name Type Description     <code>accelerator</code>  <code>Accelerator</code>     <code>accelerate</code>  <code>Dict[str, Any] = {}</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>class TorchRunner(BaseRunner):\n    r\"\"\"\n    Set up everything for running a job.\n\n    Attributes\n    ----------\n    accelerator: Accelerator\n    accelerate: Dict[str, Any] = {}\n    \"\"\"\n\n    # pylint: disable=R0902\n\n    accelerator: Accelerator\n    accelerate: Dict[str, Any] = {}\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        self.accelerator = Accelerator(**self.accelerate)\n        super().__init__(*args, **kwargs)\n\n    @on_main_process\n    def init_tensorboard(self, *args, **kwargs) -&gt; None:\n        r\"\"\"\n        Set up Tensoraoard SummaryWriter.\n        \"\"\"\n        from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n        if \"log_dir\" not in kwargs:\n            kwargs[\"log_dir\"] = self.dir\n\n        self.writer = SummaryWriter(*args, **kwargs)\n\n    def set_seed(self, bias: Optional[int] = None) -&gt; None:\n        r\"\"\"\n        Set up random seed.\n\n        Parameters\n        ----------\n        bias: Optional[int] = self.rank\n            Make the seed different for each processes.\n            This avoids same data augmentation are applied on every processes.\n            Set to `False` to disable this feature.\n        \"\"\"\n\n        if self.distributed:\n            # TODO: use broadcast_object instead.\n            self.seed = (\n                self.accelerator.gather(torch.tensor(self.seed).cuda()).unsqueeze(0).flatten()[0]  # pylint: disable=E1101\n            )\n        if bias is None:\n            bias = self.rank\n        seed = self.seed + bias if bias else self.seed\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        np.random.seed(seed)\n        random.seed(seed)\n\n    def set_deterministic(self) -&gt; None:\n        r\"\"\"\n        Set up deterministic.\n        \"\"\"\n\n        cudnn.benchmark = False\n        cudnn.deterministic = True\n        if torch.__version__ &gt;= \"1.8.0\":\n            torch.use_deterministic_algorithms(True)\n\n    def init_distributed(self) -&gt; None:\n        r\"\"\"\n        Set up distributed training.\n\n        Initialise process group and set up DDP variables.\n\n        .. deprecated:: 0.1.0\n            `init_distributed` is deprecated in favor of `Accelerator()`.\n        \"\"\"\n\n        # pylint: disable=W0201\n\n        dist.init_process_group(backend=\"nccl\")\n        self.rank = dist.get_rank()\n        self.world_size = dist.get_world_size()\n        self.local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n        torch.cuda.set_device(self.local_rank)\n        self.is_main_process = self.rank == 0\n        self.is_local_main_process = self.local_rank == 0\n\n    @catch\n    def save(self, obj: Any, f: File, main_process_only: bool = True) -&gt; File:  # pylint: disable=C0103\n        r\"\"\"\n        Save object to a path or file.\n        Returns\n        -------\n        File\n        \"\"\"\n\n        if main_process_only and self.is_main_process or not on_main_process:\n            self.accelerator.save(obj, f)\n        return f\n\n    @staticmethod\n    def load(f: File, *args, **kwargs) -&gt; Any:  # pylint: disable=C0103\n        r\"\"\"\n        Load object from a path or file.\n        Returns\n        -------\n        Any\n        \"\"\"\n\n        return torch.load(f, *args, **kwargs)  # type: ignore\n\n    @property\n    def world_size(self) -&gt; int:\n        r\"\"\"\n        Number of Processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return self.accelerator.num_processes\n\n    @property\n    def rank(self) -&gt; int:\n        r\"\"\"\n        Process index in all processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return self.accelerator.process_index\n\n    @property\n    def local_rank(self) -&gt; int:\n        r\"\"\"\n        Process index in local processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return self.accelerator.local_process_index\n</code></pre>","location":"package/#danling.TorchRunner"},{"title":"<code>init_distributed()</code>","text":"<p>Set up distributed training.</p> <p>Initialise process group and set up DDP variables.</p> <p>.. deprecated:: 0.1.0     <code>init_distributed</code> is deprecated in favor of <code>Accelerator()</code>.</p>  Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def init_distributed(self) -&gt; None:\n    r\"\"\"\n    Set up distributed training.\n\n    Initialise process group and set up DDP variables.\n\n    .. deprecated:: 0.1.0\n        `init_distributed` is deprecated in favor of `Accelerator()`.\n    \"\"\"\n\n    # pylint: disable=W0201\n\n    dist.init_process_group(backend=\"nccl\")\n    self.rank = dist.get_rank()\n    self.world_size = dist.get_world_size()\n    self.local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n    torch.cuda.set_device(self.local_rank)\n    self.is_main_process = self.rank == 0\n    self.is_local_main_process = self.local_rank == 0\n</code></pre>","location":"package/#danling.runner.torch_runner.TorchRunner.init_distributed"},{"title":"<code>init_tensorboard(*args, **kwargs)</code>","text":"<p>Set up Tensoraoard SummaryWriter.</p>  Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@on_main_process\ndef init_tensorboard(self, *args, **kwargs) -&gt; None:\n    r\"\"\"\n    Set up Tensoraoard SummaryWriter.\n    \"\"\"\n    from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n    if \"log_dir\" not in kwargs:\n        kwargs[\"log_dir\"] = self.dir\n\n    self.writer = SummaryWriter(*args, **kwargs)\n</code></pre>","location":"package/#danling.runner.torch_runner.TorchRunner.init_tensorboard"},{"title":"<code>load(f, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Load object from a path or file.</p> <p>Returns:</p>    Type Description      <code>Any</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@staticmethod\ndef load(f: File, *args, **kwargs) -&gt; Any:  # pylint: disable=C0103\n    r\"\"\"\n    Load object from a path or file.\n    Returns\n    -------\n    Any\n    \"\"\"\n\n    return torch.load(f, *args, **kwargs)  # type: ignore\n</code></pre>","location":"package/#danling.runner.torch_runner.TorchRunner.load"},{"title":"<code>local_rank()</code>  <code>property</code>","text":"<p>Process index in local processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@property\ndef local_rank(self) -&gt; int:\n    r\"\"\"\n    Process index in local processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return self.accelerator.local_process_index\n</code></pre>","location":"package/#danling.runner.torch_runner.TorchRunner.local_rank"},{"title":"<code>rank()</code>  <code>property</code>","text":"<p>Process index in all processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@property\ndef rank(self) -&gt; int:\n    r\"\"\"\n    Process index in all processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return self.accelerator.process_index\n</code></pre>","location":"package/#danling.runner.torch_runner.TorchRunner.rank"},{"title":"<code>save(obj, f, main_process_only=True)</code>","text":"<p>Save object to a path or file.</p> <p>Returns:</p>    Type Description      <code>File</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@catch\ndef save(self, obj: Any, f: File, main_process_only: bool = True) -&gt; File:  # pylint: disable=C0103\n    r\"\"\"\n    Save object to a path or file.\n    Returns\n    -------\n    File\n    \"\"\"\n\n    if main_process_only and self.is_main_process or not on_main_process:\n        self.accelerator.save(obj, f)\n    return f\n</code></pre>","location":"package/#danling.runner.torch_runner.TorchRunner.save"},{"title":"<code>set_deterministic()</code>","text":"<p>Set up deterministic.</p>  Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def set_deterministic(self) -&gt; None:\n    r\"\"\"\n    Set up deterministic.\n    \"\"\"\n\n    cudnn.benchmark = False\n    cudnn.deterministic = True\n    if torch.__version__ &gt;= \"1.8.0\":\n        torch.use_deterministic_algorithms(True)\n</code></pre>","location":"package/#danling.runner.torch_runner.TorchRunner.set_deterministic"},{"title":"<code>set_seed(bias=None)</code>","text":"<p>Set up random seed.</p> <p>Parameters:</p>    Name Type Description Default     <code>bias</code>  <code>Optional[int]</code>  <p>Make the seed different for each processes. This avoids same data augmentation are applied on every processes. Set to <code>False</code> to disable this feature.</p>  <code>None</code>      Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def set_seed(self, bias: Optional[int] = None) -&gt; None:\n    r\"\"\"\n    Set up random seed.\n\n    Parameters\n    ----------\n    bias: Optional[int] = self.rank\n        Make the seed different for each processes.\n        This avoids same data augmentation are applied on every processes.\n        Set to `False` to disable this feature.\n    \"\"\"\n\n    if self.distributed:\n        # TODO: use broadcast_object instead.\n        self.seed = (\n            self.accelerator.gather(torch.tensor(self.seed).cuda()).unsqueeze(0).flatten()[0]  # pylint: disable=E1101\n        )\n    if bias is None:\n        bias = self.rank\n    seed = self.seed + bias if bias else self.seed\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n</code></pre>","location":"package/#danling.runner.torch_runner.TorchRunner.set_seed"},{"title":"<code>world_size()</code>  <code>property</code>","text":"<p>Number of Processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@property\ndef world_size(self) -&gt; int:\n    r\"\"\"\n    Number of Processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return self.accelerator.num_processes\n</code></pre>","location":"package/#danling.runner.torch_runner.TorchRunner.world_size"},{"title":"<code>TransformerDecoder</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>TransformerDecoder is a stack of N decoder layers Args:     num_layers: the number of sub-decoder-layers in the decoder (required).     layer: the sub-decoder-layer in the decoder (default=TransformerDecoderLayer).     drop_layer: the drop layer rate (default=0.0). Examples::     &gt;&gt;&gt; transformer_decoder = dl.model.TransformerDecoder(num_layers=6)     &gt;&gt;&gt; src = torch.rand(10, 32, 512)     &gt;&gt;&gt; out = transformer_decoder(src)</p>  Source code in <code>danling/models/transformer/decoder.py</code> Python<pre><code>class TransformerDecoder(nn.Module):\n    r\"\"\"TransformerDecoder is a stack of N decoder layers\n    Args:\n        num_layers: the number of sub-decoder-layers in the decoder (required).\n        layer: the sub-decoder-layer in the decoder (default=TransformerDecoderLayer).\n        drop_layer: the drop layer rate (default=0.0).\n    Examples::\n        &gt;&gt;&gt; transformer_decoder = dl.model.TransformerDecoder(num_layers=6)\n        &gt;&gt;&gt; src = torch.rand(10, 32, 512)\n        &gt;&gt;&gt; out = transformer_decoder(src)\n    \"\"\"\n    __constants__ = [\"norm\"]\n\n    def __init__(self, layer: TransformerDecoderLayer, num_layers: int = 6, **kwargs: Optional[Dict[str, Any]]) -&gt; None:\n        super().__init__()\n        self.num_layers = num_layers\n        self.layers = nn.ModuleList([])\n        self.layers.extend([layer for _ in range(self.num_layers)])\n\n    def forward(\n        self,\n        tgt: Tensor,\n        mem: Tensor,\n        tgt_bias: Optional[Tensor] = None,\n        tgt_mask: Optional[Tensor] = None,\n        tgt_key_padding_mask: Optional[Tensor] = None,\n        mem_bias: Optional[Tensor] = None,\n        mem_mask: Optional[Tensor] = None,\n        mem_key_padding_mask: Optional[Tensor] = None,\n        need_weights: bool = False,\n        gradient_checkpoint: bool = False,\n    ) -&gt; Tensor:\n        r\"\"\"Pass the input through the decoder layers in turn.\n        Args:\n            src: the sequence to the decoder (required).\n            attn_mask: the mask for the src sequence (optional).\n            key_padding_mask: the mask for the src keys per batch (optional).\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"\n        output = tgt\n        # attn_weights is set to torch.empty(0, requires_grad=False) to avoid errors in DDP\n        attn_weights = [] if need_weights else torch.empty(0, requires_grad=False)\n\n        for layer in self.layers:\n            if gradient_checkpoint and self.training:\n                layer = partial(checkpoint, layer)\n                need_weights = torch.tensor(need_weights)\n            output, weights = layer(\n                output,\n                mem,\n                tgt_bias,\n                tgt_mask,\n                tgt_key_padding_mask,\n                mem_bias,\n                mem_mask,\n                mem_key_padding_mask,\n                need_weights,\n            )\n            if need_weights:\n                attn_weights.append(weights)  # type: ignore\n\n        if need_weights:\n            attn_weights = torch.stack(attn_weights).cpu().detach()  # type: ignore\n\n        return output, attn_weights\n</code></pre>","location":"package/#danling.TransformerDecoder"},{"title":"<code>forward(tgt, mem, tgt_bias=None, tgt_mask=None, tgt_key_padding_mask=None, mem_bias=None, mem_mask=None, mem_key_padding_mask=None, need_weights=False, gradient_checkpoint=False)</code>","text":"<p>Pass the input through the decoder layers in turn. Args:     src: the sequence to the decoder (required).     attn_mask: the mask for the src sequence (optional).     key_padding_mask: the mask for the src keys per batch (optional). Shape:     see the docs in Transformer class.</p>  Source code in <code>danling/models/transformer/decoder.py</code> Python<pre><code>def forward(\n    self,\n    tgt: Tensor,\n    mem: Tensor,\n    tgt_bias: Optional[Tensor] = None,\n    tgt_mask: Optional[Tensor] = None,\n    tgt_key_padding_mask: Optional[Tensor] = None,\n    mem_bias: Optional[Tensor] = None,\n    mem_mask: Optional[Tensor] = None,\n    mem_key_padding_mask: Optional[Tensor] = None,\n    need_weights: bool = False,\n    gradient_checkpoint: bool = False,\n) -&gt; Tensor:\n    r\"\"\"Pass the input through the decoder layers in turn.\n    Args:\n        src: the sequence to the decoder (required).\n        attn_mask: the mask for the src sequence (optional).\n        key_padding_mask: the mask for the src keys per batch (optional).\n    Shape:\n        see the docs in Transformer class.\n    \"\"\"\n    output = tgt\n    # attn_weights is set to torch.empty(0, requires_grad=False) to avoid errors in DDP\n    attn_weights = [] if need_weights else torch.empty(0, requires_grad=False)\n\n    for layer in self.layers:\n        if gradient_checkpoint and self.training:\n            layer = partial(checkpoint, layer)\n            need_weights = torch.tensor(need_weights)\n        output, weights = layer(\n            output,\n            mem,\n            tgt_bias,\n            tgt_mask,\n            tgt_key_padding_mask,\n            mem_bias,\n            mem_mask,\n            mem_key_padding_mask,\n            need_weights,\n        )\n        if need_weights:\n            attn_weights.append(weights)  # type: ignore\n\n    if need_weights:\n        attn_weights = torch.stack(attn_weights).cpu().detach()  # type: ignore\n\n    return output, attn_weights\n</code></pre>","location":"package/#danling.models.transformer.decoder.TransformerDecoder.forward"},{"title":"<code>TransformerDecoderLayer</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>TransformerDecoderLayer is made up of self-attn and feedforward network. This standard decoder layer is based on the paper \u201cAttention Is All You Need\u201d. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application. Args:     embed_dim: the number of expected features in the input (required).     num_heads: the number of heads in the multi head attention models (required).     ffn_dim: the dimension of the feedforward network model (default=embed_dim*4).     dropout: the dropout value (default=0.1).     activation: the activation function of intermediate layer, relu or gelu (default=relu).     layer_norm_eps: the eps value in layer normalization components (default=1e-5).     batch_first: If <code>True</code>, then the input and output tensors are provided         as (batch, seq, feature). Default: <code>False</code>.     norm_first: if <code>True</code>, layer norm is done prior to attention and feedforward         operations, respectivaly. Otherwise it\u2019s done after. Default: <code>False</code> (after). Examples::     &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(embed_dim=512, num_heads=8)     &gt;&gt;&gt; src = torch.rand(10, 32, 512)     &gt;&gt;&gt; out = decoder_layer(src) Alternatively, when <code>batch_first</code> is <code>True</code>:     &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(embed_dim=512, num_heads=8, batch_first=True)     &gt;&gt;&gt; src = torch.rand(32, 10, 512)     &gt;&gt;&gt; out = decoder_layer(src)</p>  Source code in <code>danling/models/transformer/decoder.py</code> Python<pre><code>class TransformerDecoderLayer(nn.Module):\n    r\"\"\"TransformerDecoderLayer is made up of self-attn and feedforward network.\n    This standard decoder layer is based on the paper \"Attention Is All You Need\".\n    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n    in a different way during application.\n    Args:\n        embed_dim: the number of expected features in the input (required).\n        num_heads: the number of heads in the multi head attention models (required).\n        ffn_dim: the dimension of the feedforward network model (default=embed_dim*4).\n        dropout: the dropout value (default=0.1).\n        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n        batch_first: If ``True``, then the input and output tensors are provided\n            as (batch, seq, feature). Default: ``False``.\n        norm_first: if ``True``, layer norm is done prior to attention and feedforward\n            operations, respectivaly. Otherwise it's done after. Default: ``False`` (after).\n    Examples::\n        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(embed_dim=512, num_heads=8)\n        &gt;&gt;&gt; src = torch.rand(10, 32, 512)\n        &gt;&gt;&gt; out = decoder_layer(src)\n    Alternatively, when ``batch_first`` is ``True``:\n        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(embed_dim=512, num_heads=8, batch_first=True)\n        &gt;&gt;&gt; src = torch.rand(32, 10, 512)\n        &gt;&gt;&gt; out = decoder_layer(src)\n    \"\"\"\n    __constants__ = [\"batch_first\", \"norm_first\"]\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        ffn_dim: Optional[int] = None,\n        dropout: float = 0.1,\n        attn_dropout: float = 0.1,\n        ffn_dropout: float = 0.1,\n        activation: str = \"GELU\",\n        layer_norm_eps: float = 1e-5,\n        scale_factor: float = 1.0,\n        bias: bool = True,\n        add_bias_kv: bool = False,\n        add_zero_attn: bool = False,\n        batch_first: bool = True,\n        norm_first: bool = False,\n        Attention: nn.Module = MultiHeadAttention,\n        FeedForwardNetwork: nn.Module = FullyConnectedNetwork,\n        **kwargs: Optional[Dict[str, Any]]\n    ) -&gt; None:\n        super().__init__()\n        if ffn_dim is None:\n            ffn_dim = embed_dim * 4\n        self.norm_first = norm_first\n        self.self_attn = Attention(\n            embed_dim,\n            num_heads,\n            attn_dropout=attn_dropout,\n            scale_factor=scale_factor,\n            bias=bias,\n            add_bias_kv=add_bias_kv,\n            add_zero_attn=add_zero_attn,\n            batch_first=batch_first,\n            **kwargs\n        )\n        self.cross_attn = Attention(\n            embed_dim,\n            num_heads,\n            attn_dropout=attn_dropout,\n            scale_factor=scale_factor,\n            bias=bias,\n            add_bias_kv=add_bias_kv,\n            add_zero_attn=add_zero_attn,\n            batch_first=batch_first,\n            **kwargs\n        )\n        self.norm1 = nn.LayerNorm(embed_dim, eps=layer_norm_eps)\n        self.ffn = FeedForwardNetwork(embed_dim, ffn_dim, activation, ffn_dropout, **kwargs)\n        self.norm2 = nn.LayerNorm(embed_dim, eps=layer_norm_eps)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(\n        self,\n        tgt: Tensor,\n        mem: Tensor,\n        tgt_bias: Optional[Tensor] = None,\n        tgt_mask: Optional[Tensor] = None,\n        tgt_key_padding_mask: Optional[Tensor] = None,\n        mem_bias: Optional[Tensor] = None,\n        mem_mask: Optional[Tensor] = None,\n        mem_key_padding_mask: Optional[Tensor] = None,\n        need_weights: bool = False,\n    ) -&gt; Tensor:\n        r\"\"\"Pass the input through the decoder layer.\n        Args:\n            src: the sequence to the decoder layer (required).\n            attn_mask: the mask for the src sequence (optional).\n            key_padding_mask: the mask for the src keys per batch (optional).\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"\n\n        if self.norm_first:\n            tgt = self.norm1(tgt)\n\n        self_attn, weights = self.self_attn(\n            tgt,\n            tgt,\n            tgt,\n            attn_bias=tgt_bias,\n            attn_mask=tgt_mask,\n            key_padding_mask=tgt_key_padding_mask,\n            need_weights=need_weights,\n        )\n        self_attn = tgt + self.dropout(self_attn)\n        cross_attn, weights = self.cross_attn(\n            self_attn,\n            mem,\n            mem,\n            attn_bias=mem_bias,\n            attn_mask=mem_mask,\n            key_padding_mask=mem_key_padding_mask,\n            need_weights=need_weights,\n        )\n        cross_attn = self_attn + self.dropout(cross_attn)\n        attn = self.norm1(cross_attn) if not self.norm_first else self.norm2(cross_attn)\n\n        ffn = self.ffn(attn)\n        ffn = attn + self.dropout(ffn)\n\n        if not self.norm_first:\n            ffn = self.norm2(ffn)\n\n        return ffn, weights\n</code></pre>","location":"package/#danling.TransformerDecoderLayer"},{"title":"<code>forward(tgt, mem, tgt_bias=None, tgt_mask=None, tgt_key_padding_mask=None, mem_bias=None, mem_mask=None, mem_key_padding_mask=None, need_weights=False)</code>","text":"<p>Pass the input through the decoder layer. Args:     src: the sequence to the decoder layer (required).     attn_mask: the mask for the src sequence (optional).     key_padding_mask: the mask for the src keys per batch (optional). Shape:     see the docs in Transformer class.</p>  Source code in <code>danling/models/transformer/decoder.py</code> Python<pre><code>def forward(\n    self,\n    tgt: Tensor,\n    mem: Tensor,\n    tgt_bias: Optional[Tensor] = None,\n    tgt_mask: Optional[Tensor] = None,\n    tgt_key_padding_mask: Optional[Tensor] = None,\n    mem_bias: Optional[Tensor] = None,\n    mem_mask: Optional[Tensor] = None,\n    mem_key_padding_mask: Optional[Tensor] = None,\n    need_weights: bool = False,\n) -&gt; Tensor:\n    r\"\"\"Pass the input through the decoder layer.\n    Args:\n        src: the sequence to the decoder layer (required).\n        attn_mask: the mask for the src sequence (optional).\n        key_padding_mask: the mask for the src keys per batch (optional).\n    Shape:\n        see the docs in Transformer class.\n    \"\"\"\n\n    if self.norm_first:\n        tgt = self.norm1(tgt)\n\n    self_attn, weights = self.self_attn(\n        tgt,\n        tgt,\n        tgt,\n        attn_bias=tgt_bias,\n        attn_mask=tgt_mask,\n        key_padding_mask=tgt_key_padding_mask,\n        need_weights=need_weights,\n    )\n    self_attn = tgt + self.dropout(self_attn)\n    cross_attn, weights = self.cross_attn(\n        self_attn,\n        mem,\n        mem,\n        attn_bias=mem_bias,\n        attn_mask=mem_mask,\n        key_padding_mask=mem_key_padding_mask,\n        need_weights=need_weights,\n    )\n    cross_attn = self_attn + self.dropout(cross_attn)\n    attn = self.norm1(cross_attn) if not self.norm_first else self.norm2(cross_attn)\n\n    ffn = self.ffn(attn)\n    ffn = attn + self.dropout(ffn)\n\n    if not self.norm_first:\n        ffn = self.norm2(ffn)\n\n    return ffn, weights\n</code></pre>","location":"package/#danling.models.transformer.decoder.TransformerDecoderLayer.forward"},{"title":"<code>TransformerEncoder</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>TransformerEncoder is a stack of N encoder layers Args:     num_layers: the number of sub-encoder-layers in the encoder (required).     layer: the sub-encoder-layer in the encoder (default=TransformerEncoderLayer).     drop_layer: the drop layer rate (default=0.0). Examples::     &gt;&gt;&gt; transformer_encoder = dl.model.TransformerEncoder(num_layers=6)     &gt;&gt;&gt; src = torch.rand(10, 32, 512)     &gt;&gt;&gt; out = transformer_encoder(src)</p>  Source code in <code>danling/models/transformer/encoder.py</code> Python<pre><code>class TransformerEncoder(nn.Module):\n    r\"\"\"TransformerEncoder is a stack of N encoder layers\n    Args:\n        num_layers: the number of sub-encoder-layers in the encoder (required).\n        layer: the sub-encoder-layer in the encoder (default=TransformerEncoderLayer).\n        drop_layer: the drop layer rate (default=0.0).\n    Examples::\n        &gt;&gt;&gt; transformer_encoder = dl.model.TransformerEncoder(num_layers=6)\n        &gt;&gt;&gt; src = torch.rand(10, 32, 512)\n        &gt;&gt;&gt; out = transformer_encoder(src)\n    \"\"\"\n    __constants__ = [\"norm\"]\n\n    def __init__(self, layer: TransformerEncoderLayer, num_layers: int = 6, **kwargs: Optional[Dict[str, Any]]) -&gt; None:\n        super().__init__()\n        self.num_layers = num_layers\n        self.layers = nn.ModuleList([])\n        self.layers.extend([layer for _ in range(self.num_layers)])\n\n    def forward(\n        self,\n        src: Tensor,\n        attn_bias: Optional[Tensor] = None,\n        attn_mask: Optional[Tensor] = None,\n        key_padding_mask: Optional[Tensor] = None,\n        need_weights: bool = False,\n        gradient_checkpoint: bool = False,\n    ) -&gt; Tensor:\n        r\"\"\"Pass the input through the encoder layers in turn.\n        Args:\n            src: the sequence to the encoder (required).\n            attn_mask: the mask for the src sequence (optional).\n            key_padding_mask: the mask for the src keys per batch (optional).\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"\n        output = src\n        # attn_weights is set to torch.empty(0, requires_grad=False) to avoid errors in DDP\n        attn_weights = [] if need_weights else torch.empty(0, requires_grad=False)\n\n        for layer in self.layers:\n            if gradient_checkpoint and self.training:\n                layer = partial(checkpoint, layer)\n                need_weights = torch.tensor(need_weights)\n            output, weights = layer(output, attn_bias, attn_mask, key_padding_mask, need_weights)\n            if need_weights:\n                attn_weights.append(weights)  # type: ignore\n\n        if need_weights:\n            attn_weights = torch.stack(attn_weights).cpu().detach()  # type: ignore\n\n        return output, attn_weights\n</code></pre>","location":"package/#danling.TransformerEncoder"},{"title":"<code>forward(src, attn_bias=None, attn_mask=None, key_padding_mask=None, need_weights=False, gradient_checkpoint=False)</code>","text":"<p>Pass the input through the encoder layers in turn. Args:     src: the sequence to the encoder (required).     attn_mask: the mask for the src sequence (optional).     key_padding_mask: the mask for the src keys per batch (optional). Shape:     see the docs in Transformer class.</p>  Source code in <code>danling/models/transformer/encoder.py</code> Python<pre><code>def forward(\n    self,\n    src: Tensor,\n    attn_bias: Optional[Tensor] = None,\n    attn_mask: Optional[Tensor] = None,\n    key_padding_mask: Optional[Tensor] = None,\n    need_weights: bool = False,\n    gradient_checkpoint: bool = False,\n) -&gt; Tensor:\n    r\"\"\"Pass the input through the encoder layers in turn.\n    Args:\n        src: the sequence to the encoder (required).\n        attn_mask: the mask for the src sequence (optional).\n        key_padding_mask: the mask for the src keys per batch (optional).\n    Shape:\n        see the docs in Transformer class.\n    \"\"\"\n    output = src\n    # attn_weights is set to torch.empty(0, requires_grad=False) to avoid errors in DDP\n    attn_weights = [] if need_weights else torch.empty(0, requires_grad=False)\n\n    for layer in self.layers:\n        if gradient_checkpoint and self.training:\n            layer = partial(checkpoint, layer)\n            need_weights = torch.tensor(need_weights)\n        output, weights = layer(output, attn_bias, attn_mask, key_padding_mask, need_weights)\n        if need_weights:\n            attn_weights.append(weights)  # type: ignore\n\n    if need_weights:\n        attn_weights = torch.stack(attn_weights).cpu().detach()  # type: ignore\n\n    return output, attn_weights\n</code></pre>","location":"package/#danling.models.transformer.encoder.TransformerEncoder.forward"},{"title":"<code>TransformerEncoderLayer</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>TransformerEncoderLayer is made up of self-attn and feedforward network. This standard encoder layer is based on the paper \u201cAttention Is All You Need\u201d. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application. Args:     embed_dim: the number of expected features in the input (required).     num_heads: the number of heads in the multi head attention models (required).     ffn_dim: the dimension of the feedforward network model (default=embed_dim*4).     dropout: the dropout value (default=0.1).     activation: the activation function of intermediate layer, relu or gelu (default=relu).     layer_norm_eps: the eps value in layer normalization components (default=1e-5).     batch_first: If <code>True</code>, then the input and output tensors are provided         as (batch, seq, feature). Default: <code>False</code>.     norm_first: if <code>True</code>, layer norm is done prior to attention and feedforward         operations, respectivaly. Otherwise it\u2019s done after. Default: <code>False</code> (after). Examples::     &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(embed_dim=512, num_heads=8)     &gt;&gt;&gt; src = torch.rand(10, 32, 512)     &gt;&gt;&gt; out = encoder_layer(src) Alternatively, when <code>batch_first</code> is <code>True</code>:     &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(embed_dim=512, num_heads=8, batch_first=True)     &gt;&gt;&gt; src = torch.rand(32, 10, 512)     &gt;&gt;&gt; out = encoder_layer(src)</p>  Source code in <code>danling/models/transformer/encoder.py</code> Python<pre><code>class TransformerEncoderLayer(nn.Module):\n    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n    in a different way during application.\n    Args:\n        embed_dim: the number of expected features in the input (required).\n        num_heads: the number of heads in the multi head attention models (required).\n        ffn_dim: the dimension of the feedforward network model (default=embed_dim*4).\n        dropout: the dropout value (default=0.1).\n        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n        batch_first: If ``True``, then the input and output tensors are provided\n            as (batch, seq, feature). Default: ``False``.\n        norm_first: if ``True``, layer norm is done prior to attention and feedforward\n            operations, respectivaly. Otherwise it's done after. Default: ``False`` (after).\n    Examples::\n        &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(embed_dim=512, num_heads=8)\n        &gt;&gt;&gt; src = torch.rand(10, 32, 512)\n        &gt;&gt;&gt; out = encoder_layer(src)\n    Alternatively, when ``batch_first`` is ``True``:\n        &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(embed_dim=512, num_heads=8, batch_first=True)\n        &gt;&gt;&gt; src = torch.rand(32, 10, 512)\n        &gt;&gt;&gt; out = encoder_layer(src)\n    \"\"\"\n    __constants__ = [\"batch_first\", \"norm_first\"]\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        ffn_dim: Optional[int] = None,\n        dropout: float = 0.1,\n        attn_dropout: float = 0.1,\n        ffn_dropout: float = 0.1,\n        activation: str = \"GELU\",\n        layer_norm_eps: float = 1e-5,\n        scale_factor: float = 1.0,\n        bias: bool = True,\n        add_bias_kv: bool = False,\n        add_zero_attn: bool = False,\n        batch_first: bool = True,\n        norm_first: bool = False,\n        Attention: nn.Module = MultiHeadAttention,\n        FeedForwardNetwork: nn.Module = FullyConnectedNetwork,\n        **kwargs: Optional[Dict[str, Any]]\n    ) -&gt; None:\n        super().__init__()\n        if ffn_dim is None:\n            ffn_dim = embed_dim * 4\n        self.norm_first = norm_first\n        self.attn = Attention(\n            embed_dim,\n            num_heads,\n            attn_dropout=attn_dropout,\n            scale_factor=scale_factor,\n            bias=bias,\n            add_bias_kv=add_bias_kv,\n            add_zero_attn=add_zero_attn,\n            batch_first=batch_first,\n            **kwargs\n        )\n        self.norm1 = nn.LayerNorm(embed_dim, eps=layer_norm_eps)\n        self.ffn = FeedForwardNetwork(embed_dim, ffn_dim, activation, ffn_dropout, **kwargs)\n        self.norm2 = nn.LayerNorm(embed_dim, eps=layer_norm_eps)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(\n        self,\n        src: Tensor,\n        attn_bias: Optional[Tensor] = None,\n        attn_mask: Optional[Tensor] = None,\n        key_padding_mask: Optional[Tensor] = None,\n        need_weights: bool = False,\n    ) -&gt; Tensor:\n        r\"\"\"Pass the input through the encoder layer.\n        Args:\n            src: the sequence to the encoder layer (required).\n            attn_mask: the mask for the src sequence (optional).\n            key_padding_mask: the mask for the src keys per batch (optional).\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"\n\n        if self.norm_first:\n            src = self.norm1(src)\n\n        attn, weights = self.attn(\n            src,\n            src,\n            src,\n            attn_bias=attn_bias,\n            attn_mask=attn_mask,\n            key_padding_mask=key_padding_mask,\n            need_weights=need_weights,\n        )\n        attn = src + self.dropout(attn)\n        attn = self.norm1(attn) if not self.norm_first else self.norm2(attn)\n\n        ffn = self.ffn(attn)\n        ffn = attn + self.dropout(ffn)\n\n        if not self.norm_first:\n            ffn = self.norm2(ffn)\n\n        return ffn, weights\n</code></pre>","location":"package/#danling.TransformerEncoderLayer"},{"title":"<code>forward(src, attn_bias=None, attn_mask=None, key_padding_mask=None, need_weights=False)</code>","text":"<p>Pass the input through the encoder layer. Args:     src: the sequence to the encoder layer (required).     attn_mask: the mask for the src sequence (optional).     key_padding_mask: the mask for the src keys per batch (optional). Shape:     see the docs in Transformer class.</p>  Source code in <code>danling/models/transformer/encoder.py</code> Python<pre><code>def forward(\n    self,\n    src: Tensor,\n    attn_bias: Optional[Tensor] = None,\n    attn_mask: Optional[Tensor] = None,\n    key_padding_mask: Optional[Tensor] = None,\n    need_weights: bool = False,\n) -&gt; Tensor:\n    r\"\"\"Pass the input through the encoder layer.\n    Args:\n        src: the sequence to the encoder layer (required).\n        attn_mask: the mask for the src sequence (optional).\n        key_padding_mask: the mask for the src keys per batch (optional).\n    Shape:\n        see the docs in Transformer class.\n    \"\"\"\n\n    if self.norm_first:\n        src = self.norm1(src)\n\n    attn, weights = self.attn(\n        src,\n        src,\n        src,\n        attn_bias=attn_bias,\n        attn_mask=attn_mask,\n        key_padding_mask=key_padding_mask,\n        need_weights=need_weights,\n    )\n    attn = src + self.dropout(attn)\n    attn = self.norm1(attn) if not self.norm_first else self.norm2(attn)\n\n    ffn = self.ffn(attn)\n    ffn = attn + self.dropout(ffn)\n\n    if not self.norm_first:\n        ffn = self.norm2(ffn)\n\n    return ffn, weights\n</code></pre>","location":"package/#danling.models.transformer.encoder.TransformerEncoderLayer.forward"},{"title":"<code>UnitedPositionEmbedding</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>United Position Embedding See <code>Rethinking Positional Encoding in Language Pre-training &lt;https://arxiv.org/abs/2006.15595&gt;</code>_ .. math::     \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O where :math:<code>head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)</code>. Args:     embed_dim: total dimension of the model.     num_heads: parallel attention heads.     dropout: a Dropout layer on attn_output_weights. Default: 0.0.     bias: add bias as module parameter. Default: True.     add_bias_kv: add bias to the key and value sequences at dim=0.     add_zero_attn: add a new batch of zeros to the key and                    value sequences at dim=1.     k_dim: total number of features in key. Default: None.     v_dim: total number of features in value. Default: None.     batch_first: If <code>True</code>, then the input and output tensors are provided         as (batch, seq, feature). Default: <code>False</code> (seq, batch, feature). Note that if :attr:<code>k_dim</code> and :attr:<code>v_dim</code> are None, they will be set to :attr:<code>embed_dim</code> such that query, key, and value have the same number of features. Examples::     &gt;&gt;&gt; multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)     &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)</p>  Source code in <code>danling/models/transformer/pos_embed/pos_embed.py</code> Python<pre><code>class UnitedPositionEmbedding(nn.Module):\n    r\"\"\"United Position Embedding\n    See `Rethinking Positional Encoding in Language Pre-training &lt;https://arxiv.org/abs/2006.15595&gt;`_\n    .. math::\n        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n    where :math:`head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n    Args:\n        embed_dim: total dimension of the model.\n        num_heads: parallel attention heads.\n        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n        bias: add bias as module parameter. Default: True.\n        add_bias_kv: add bias to the key and value sequences at dim=0.\n        add_zero_attn: add a new batch of zeros to the key and\n                       value sequences at dim=1.\n        k_dim: total number of features in key. Default: None.\n        v_dim: total number of features in value. Default: None.\n        batch_first: If ``True``, then the input and output tensors are provided\n            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n    Note that if :attr:`k_dim` and :attr:`v_dim` are None, they will be set\n    to :attr:`embed_dim` such that query, key, and value have the same\n    number of features.\n    Examples::\n        &gt;&gt;&gt; multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n        &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        seq_len_max: int,\n        rel_pos_embed: bool = False,\n        rel_pos_embed_buckets: int = 32,\n        rel_pos_embed_max: int = 128,\n        pos_embed_dropout: float = 0.0,\n        pos_scale_factor: int = 1,\n        has_cls_token: bool = True,\n    ) -&gt; None:\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.seq_len_max = seq_len_max\n        self.has_cls_token = has_cls_token\n        self.dropout = nn.Dropout(pos_embed_dropout)\n        if self.has_cls_token:\n            # make room for [CLS]-to-others and others-to-[CLS]\n            self.seq_len_max += 2\n        self.abs_pos_embed = nn.Parameter(torch.randn(self.seq_len_max, self.embed_dim))\n        self.ln = nn.LayerNorm(self.embed_dim)\n        self.in_proj = nn.Linear(self.embed_dim, self.embed_dim * 2)\n        self.scaling = (embed_dim / num_heads * pos_scale_factor) ** -0.5\n\n        self.rel_pos_embed = None\n        if rel_pos_embed:\n            assert rel_pos_embed_buckets % 2 == 0\n            self.rel_pos_embed_buckets = rel_pos_embed_buckets\n            self.rel_pos_embed_max = rel_pos_embed_max\n            self.rel_pos_embed = nn.Embedding(self.rel_pos_embed_buckets + 1, self.num_heads)\n            self.rel_pos_embed_bucket = relative_position_bucket(\n                seq_len_max=self.seq_len_max,\n                num_buckets=self.rel_pos_embed_buckets,\n                max_distance=self.rel_pos_embed_max,\n            )\n\n    def forward(self, src: Tensor, cls_token_index: Optional[Tensor] = None) -&gt; Tensor:\n        B, N, C = src.shape\n        # 0 is for others-to-[CLS] 1 is for [CLS]-to-others\n        # Assume the input is ordered. If your input token is permuted, you may need to update this accordingly\n        if self.has_cls_token:\n            # only plus 1 here since because [CLS] already plused 1\n            N += 1\n        weight = self.ln(self.abs_pos_embed[:N, :])\n        q, k = self.in_proj(weight).reshape(N, 2, self.num_heads, C // self.num_heads).permute(1, 2, 0, 3)\n        q = q * self.scaling\n        pos_embed = torch.bmm(q, k.transpose(1, 2))\n        if self.has_cls_token:\n            # p_0 \\dot p_0 is [CLS]-to-others\n            cls_2_others = pos_embed[:, 0, 0]\n            # p_1 \\dot p_1 is others-to-[CLS]\n            others_2_cls = pos_embed[:, 1, 1]\n            # offset\n            pos_embed = pos_embed[:, 1:, 1:]\n            # if [CLS] is not the first token\n            if cls_token_index is not None:\n                pos_embed = pos_embed.repeat(B, 1, 1, 1)\n                pos_embed[torch.arange(B), :, cls_token_index, :] = cls_2_others.expand(B, -1).unsqueeze(-1)\n                pos_embed[torch.arange(B), :, :, cls_token_index] = others_2_cls.expand(B, -1).unsqueeze(-1)\n            else:\n                pos_embed[:, 0, :] = cls_2_others.unsqueeze(-1)\n                pos_embed[:, :, 0] = others_2_cls.unsqueeze(-1)\n            N -= 1\n        rel_pos_embed = torch.zeros_like(pos_embed)\n        if self.rel_pos_embed:\n            rel_pos_embed_bucket = self.rel_pos_embed_bucket[:N, :N]\n            if self.has_cls_token:\n                if cls_token_index is not None:\n                    rel_pos_embed_bucket = rel_pos_embed_bucket.repeat(B, 1, 1)\n                    rel_pos_embed_bucket[torch.arange(B), cls_token_index, :] = self.rel_pos_embed_buckets // 2\n                    rel_pos_embed_bucket[torch.arange(B), :, cls_token_index] = self.rel_pos_embed_buckets\n                    rel_pos_embed = self.rel_pos_embed(rel_pos_embed_bucket).permute(0, 3, 1, 2)\n                else:\n                    rel_pos_embed_bucket[0, :] = self.rel_pos_embed_buckets // 2\n                    rel_pos_embed_bucket[:, 0] = self.rel_pos_embed_buckets\n                    rel_pos_embed = self.rel_pos_embed(rel_pos_embed_bucket).permute(2, 0, 1)\n            else:\n                rel_pos_embed = self.rel_pos_embed(rel_pos_embed_bucket).permute(2, 0, 1)\n            pos_embed += rel_pos_embed\n\n        pos_embed = (\n            pos_embed.view(-1, *pos_embed.shape[2:]) if cls_token_index is not None else pos_embed.repeat(B, 1, 1)\n        )\n        return self.dropout(pos_embed)\n</code></pre>","location":"package/#danling.UnitedPositionEmbedding"},{"title":"<code>catch(error=Exception, exclude=None, print_args=False)</code>","text":"<p>Decorator to catch <code>error</code> except for <code>exclude</code>. Detailed traceback will be printed to <code>stdout</code>.</p> <p><code>catch</code> is extremely useful for unfatal errors. For example, <code>Runner</code> by de</p> <p>Parameters:</p>    Name Type Description Default     <code>error</code>  <code>Exception</code>    <code>Exception</code>    <code>exclude</code>  <code>Optional[Exception]</code>    <code>None</code>    <code>print_args</code>  <code>bool</code>  <p>Whether to print the arguments passed to the function.</p>  <code>False</code>      Source code in <code>danling/utils/decorator.py</code> Python<pre><code>@flexible_decorator\ndef catch(error: Exception = Exception, exclude: Optional[Exception] = None, print_args: bool = False):\n    \"\"\"\n    Decorator to catch `error` except for `exclude`.\n    Detailed traceback will be printed to `stdout`.\n\n    `catch` is extremely useful for unfatal errors.\n    For example, `Runner` by de\n\n    Parameters\n    ----------\n    error: Exception\n    exclude: Optional[Exception] = None\n    print_args: bool = False\n        Whether to print the arguments passed to the function.\n    \"\"\"\n\n    def decorator(func, error: Exception = Exception, exclude: Optional[Exception] = None, print_args: bool = False):\n        @wraps(func)\n        def wrapper(*args, **kwargs):  # pylint: disable=R1710\n            try:\n                return func(*args, **kwargs)\n            except error as exc:  # pylint: disable=W0703\n                if exclude is not None and isinstance(exc, exclude):\n                    raise exc\n                message = format_exc()\n                message += f\"\\nencoutered when calling {func}\"\n                if print_args:\n                    message += (f\"with args {args} and kwargs {kwargs}\",)\n                print(message, file=stderr, force=True)\n\n        return wrapper\n\n    return lambda func: decorator(func, error, exclude, print_args)\n</code></pre>","location":"package/#danling.catch"},{"title":"<code>load(path, *args, **kwargs)</code>","text":"<p>Load any file with supported extensions.</p>  Source code in <code>danling/utils/io.py</code> Python<pre><code>def load(path: str, *args: List[Any], **kwargs: Dict[str, Any]) -&gt; Any:\n    \"\"\"\n    Load any file with supported extensions.\n    \"\"\"\n    if not os.path.isfile(path):\n        raise ValueError(f\"Trying to load {path} but it is not a file.\")\n    extension = os.path.splitext(path)[-1].lower()[1:]\n    if extension in PYTORCH:\n        if torch_load is None:\n            raise ImportError(f\"Trying to load {path} but torch is not installed.\")\n        return torch_load(path)\n    if extension in NUMPY:\n        if numpy_load is None:\n            raise ImportError(f\"Trying to load {path} but numpy is not installed.\")\n        return numpy_load(path, allow_pickle=True)\n    if extension in CSV:\n        if read_csv is None:\n            raise ImportError(f\"Trying to load {path} but pandas is not installed.\")\n        return read_csv(path, *args, **kwargs)\n    if extension in JSON:\n        with open(path, \"r\") as fp:  # pylint: disable=W1514, C0103\n            return json_load(fp, *args, **kwargs)  # type: ignore\n    if extension in PICKLE:\n        with open(path, \"rb\") as fp:  # pylint: disable=C0103\n            return pickle_load(fp, *args, **kwargs)  # type: ignore\n    raise ValueError(f\"Tying to load {path} with unsupported extension={extension}\")\n</code></pre>","location":"package/#danling.load"},{"title":"Registry","text":"<p>         Bases: <code>NestedDict</code></p> <p><code>Registry</code> for components.</p>","location":"registry/"},{"title":"Notes","text":"<p><code>Registry</code> inherits from <code>NestedDict</code>.</p> <p>Therefore, <code>Registry</code> comes in a nested structure by nature. You could create a sub-registry by simply calling <code>registry.sub_registry = Registry</code>, and access through <code>registry.sub_registry.register()</code>.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; registry = Registry(\"test\")\n&gt;&gt;&gt; @registry.register\n... @registry.register(\"Module1\")\n... class Module:\n...     def __init__(self, a, b):\n...         self.a = a\n...         self.b = b\n&gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n&gt;&gt;&gt; registry\nRegistry(\n  (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n  (Module): &lt;class 'danling.registry.registry.Module'&gt;\n  (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n)\n&gt;&gt;&gt; registry.lookup(\"Module\")\n&lt;class 'danling.registry.registry.Module'&gt;\n&gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n&gt;&gt;&gt; # registry.register(Module)\n&gt;&gt;&gt; module = registry.build(config[\"module\"])\n&gt;&gt;&gt; type(module)\n&lt;class 'danling.registry.registry.Module'&gt;\n&gt;&gt;&gt; module.a\n1\n&gt;&gt;&gt; module.b\n2\n</code></pre>  Source code in <code>danling/registry/registry.py</code> Python<pre><code>class Registry(NestedDict):\n    \"\"\"\n    `Registry` for components.\n\n    Notes\n    -----\n\n    `Registry` inherits from [`NestedDict`](https://chanfig.danling.org/nested_dict/).\n\n    Therefore, `Registry` comes in a nested structure by nature.\n    You could create a sub-registry by simply calling `registry.sub_registry = Registry`,\n    and access through `registry.sub_registry.register()`.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; registry = Registry(\"test\")\n    &gt;&gt;&gt; @registry.register\n    ... @registry.register(\"Module1\")\n    ... class Module:\n    ...     def __init__(self, a, b):\n    ...         self.a = a\n    ...         self.b = b\n    &gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n    &gt;&gt;&gt; registry\n    Registry(\n      (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n      (Module): &lt;class 'danling.registry.registry.Module'&gt;\n      (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n    )\n    &gt;&gt;&gt; registry.lookup(\"Module\")\n    &lt;class 'danling.registry.registry.Module'&gt;\n    &gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n    &gt;&gt;&gt; # registry.register(Module)\n    &gt;&gt;&gt; module = registry.build(config[\"module\"])\n    &gt;&gt;&gt; type(module)\n    &lt;class 'danling.registry.registry.Module'&gt;\n    &gt;&gt;&gt; module.a\n    1\n    &gt;&gt;&gt; module.b\n    2\n\n    ```\n    \"\"\"\n\n    override: bool = False\n\n    def __init__(self, override: bool = False):\n        super().__init__()\n        self.setattr(\"override\", override)\n\n    def register(self, component: Optional[Callable] = None, name: Optional[str] = None) -&gt; Callable:\n        r\"\"\"\n        Register a new component\n\n        Parameters\n        ----------\n        component: Optional[Callable] = None\n            The component to register.\n        name: Optional[str] = component.__name__\n            The name of the component.\n\n        Returns\n        -------\n        component: Callable\n            The registered component.\n\n        Raises\n        ------\n        ValueError\n            If the component with the same name already exists and `Registry.override=False`.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; registry = Registry(\"test\")\n        &gt;&gt;&gt; @registry.register\n        ... @registry.register(\"Module1\")\n        ... class Module:\n        ...     def __init__(self, a, b):\n        ...         self.a = a\n        ...         self.b = b\n        &gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n        &gt;&gt;&gt; registry\n        Registry(\n          (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n          (Module): &lt;class 'danling.registry.registry.Module'&gt;\n          (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n        )\n\n        ```\n        \"\"\"\n\n        if isinstance(name, str) and name in self and not self.override:\n            raise ValueError(f\"Component with name {name} already exists\")\n\n        # Registry.register()\n        if name is not None:\n            self.set(name, component)\n\n        # @Registry.register()\n        @wraps(self.register)\n        def register(component, name=None):\n            if name is None:\n                name = component.__name__\n            self.set(name, component)\n            return component\n\n        # @Registry.register\n        if callable(component) and name is None:\n            return register(component)\n\n        return lambda x: register(x, component)\n\n    def lookup(self, name: str) -&gt; Any:\n        r\"\"\"\n        Lookup for a component.\n\n        Parameters\n        ----------\n        name: str\n            The name of the component.\n\n        Returns\n        -------\n        value: Any\n\n        Raises\n        ------\n        KeyError\n            If the component is not registered.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; registry = Registry(\"test\")\n        &gt;&gt;&gt; @registry.register\n        ... class Module:\n        ...     def __init__(self, a, b):\n        ...         self.a = a\n        ...         self.b = b\n        &gt;&gt;&gt; registry.lookup(\"Module\")\n        &lt;class 'danling.registry.registry.Module'&gt;\n\n        ```\n        \"\"\"\n\n        return self.get(name)\n\n    def build(self, name: str, *args, **kwargs):\n        r\"\"\"\n        Build a component.\n\n        Parameters\n        ----------\n        name: str\n            The name of the component.\n        *args\n            The arguments to pass to the component.\n        **kwargs\n            The keyword arguments to pass to the component.\n\n        Returns\n        -------\n        component: Callable\n\n        Raises\n        ------\n        KeyError\n            If the component is not registered.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; registry = Registry(\"test\")\n        &gt;&gt;&gt; @registry.register\n        ... class Module:\n        ...     def __init__(self, a, b):\n        ...         self.a = a\n        ...         self.b = b\n        &gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n        &gt;&gt;&gt; # registry.register(Module)\n        &gt;&gt;&gt; module = registry.build(config[\"module\"])\n        &gt;&gt;&gt; type(module)\n        &lt;class 'danling.registry.registry.Module'&gt;\n        &gt;&gt;&gt; module.a\n        1\n        &gt;&gt;&gt; module.b\n        2\n\n        ```\n        \"\"\"\n\n        if isinstance(name, Mapping) and not args and not kwargs:\n            name, kwargs = name.pop(\"name\"), name  # type: ignore\n        return self.get(name)(*args, **kwargs)\n\n    def __wrapped__(self):\n        pass\n</code></pre>","location":"registry/#danling.registry.Registry--notes"},{"title":"<code>register(component=None, name=None)</code>","text":"<p>Register a new component</p> <p>Parameters:</p>    Name Type Description Default     <code>component</code>  <code>Optional[Callable]</code>  <p>The component to register.</p>  <code>None</code>    <code>name</code>  <code>Optional[str]</code>  <p>The name of the component.</p>  <code>None</code>     <p>Returns:</p>    Name Type Description     <code>component</code>  <code>Callable</code>  <p>The registered component.</p>    <p>Raises:</p>    Type Description      <code>ValueError</code>  <p>If the component with the same name already exists and <code>Registry.override=False</code>.</p>    <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; registry = Registry(\"test\")\n&gt;&gt;&gt; @registry.register\n... @registry.register(\"Module1\")\n... class Module:\n...     def __init__(self, a, b):\n...         self.a = a\n...         self.b = b\n&gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n&gt;&gt;&gt; registry\nRegistry(\n  (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n  (Module): &lt;class 'danling.registry.registry.Module'&gt;\n  (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n)\n</code></pre>  Source code in <code>danling/registry/registry.py</code> Python<pre><code>def register(self, component: Optional[Callable] = None, name: Optional[str] = None) -&gt; Callable:\n    r\"\"\"\n    Register a new component\n\n    Parameters\n    ----------\n    component: Optional[Callable] = None\n        The component to register.\n    name: Optional[str] = component.__name__\n        The name of the component.\n\n    Returns\n    -------\n    component: Callable\n        The registered component.\n\n    Raises\n    ------\n    ValueError\n        If the component with the same name already exists and `Registry.override=False`.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; registry = Registry(\"test\")\n    &gt;&gt;&gt; @registry.register\n    ... @registry.register(\"Module1\")\n    ... class Module:\n    ...     def __init__(self, a, b):\n    ...         self.a = a\n    ...         self.b = b\n    &gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n    &gt;&gt;&gt; registry\n    Registry(\n      (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n      (Module): &lt;class 'danling.registry.registry.Module'&gt;\n      (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n    )\n\n    ```\n    \"\"\"\n\n    if isinstance(name, str) and name in self and not self.override:\n        raise ValueError(f\"Component with name {name} already exists\")\n\n    # Registry.register()\n    if name is not None:\n        self.set(name, component)\n\n    # @Registry.register()\n    @wraps(self.register)\n    def register(component, name=None):\n        if name is None:\n            name = component.__name__\n        self.set(name, component)\n        return component\n\n    # @Registry.register\n    if callable(component) and name is None:\n        return register(component)\n\n    return lambda x: register(x, component)\n</code></pre>","location":"registry/#danling.registry.registry.Registry.register"},{"title":"<code>lookup(name)</code>","text":"<p>Lookup for a component.</p> <p>Parameters:</p>    Name Type Description Default     <code>name</code>  <code>str</code>  <p>The name of the component.</p>  required     <p>Returns:</p>    Name Type Description     <code>value</code>  <code>Any</code>      <p>Raises:</p>    Type Description      <code>KeyError</code>  <p>If the component is not registered.</p>    <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; registry = Registry(\"test\")\n&gt;&gt;&gt; @registry.register\n... class Module:\n...     def __init__(self, a, b):\n...         self.a = a\n...         self.b = b\n&gt;&gt;&gt; registry.lookup(\"Module\")\n&lt;class 'danling.registry.registry.Module'&gt;\n</code></pre>  Source code in <code>danling/registry/registry.py</code> Python<pre><code>def lookup(self, name: str) -&gt; Any:\n    r\"\"\"\n    Lookup for a component.\n\n    Parameters\n    ----------\n    name: str\n        The name of the component.\n\n    Returns\n    -------\n    value: Any\n\n    Raises\n    ------\n    KeyError\n        If the component is not registered.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; registry = Registry(\"test\")\n    &gt;&gt;&gt; @registry.register\n    ... class Module:\n    ...     def __init__(self, a, b):\n    ...         self.a = a\n    ...         self.b = b\n    &gt;&gt;&gt; registry.lookup(\"Module\")\n    &lt;class 'danling.registry.registry.Module'&gt;\n\n    ```\n    \"\"\"\n\n    return self.get(name)\n</code></pre>","location":"registry/#danling.registry.registry.Registry.lookup"},{"title":"<code>build(name, *args, **kwargs)</code>","text":"<p>Build a component.</p> <p>Parameters:</p>    Name Type Description Default     <code>name</code>  <code>str</code>  <p>The name of the component.</p>  required    <code>*args</code>   <p>The arguments to pass to the component.</p>  <code>()</code>    <code>**kwargs</code>   <p>The keyword arguments to pass to the component.</p>  <code>{}</code>     <p>Returns:</p>    Name Type Description     <code>component</code>  <code>Callable</code>      <p>Raises:</p>    Type Description      <code>KeyError</code>  <p>If the component is not registered.</p>    <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; registry = Registry(\"test\")\n&gt;&gt;&gt; @registry.register\n... class Module:\n...     def __init__(self, a, b):\n...         self.a = a\n...         self.b = b\n&gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n&gt;&gt;&gt; # registry.register(Module)\n&gt;&gt;&gt; module = registry.build(config[\"module\"])\n&gt;&gt;&gt; type(module)\n&lt;class 'danling.registry.registry.Module'&gt;\n&gt;&gt;&gt; module.a\n1\n&gt;&gt;&gt; module.b\n2\n</code></pre>  Source code in <code>danling/registry/registry.py</code> Python<pre><code>def build(self, name: str, *args, **kwargs):\n    r\"\"\"\n    Build a component.\n\n    Parameters\n    ----------\n    name: str\n        The name of the component.\n    *args\n        The arguments to pass to the component.\n    **kwargs\n        The keyword arguments to pass to the component.\n\n    Returns\n    -------\n    component: Callable\n\n    Raises\n    ------\n    KeyError\n        If the component is not registered.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; registry = Registry(\"test\")\n    &gt;&gt;&gt; @registry.register\n    ... class Module:\n    ...     def __init__(self, a, b):\n    ...         self.a = a\n    ...         self.b = b\n    &gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n    &gt;&gt;&gt; # registry.register(Module)\n    &gt;&gt;&gt; module = registry.build(config[\"module\"])\n    &gt;&gt;&gt; type(module)\n    &lt;class 'danling.registry.registry.Module'&gt;\n    &gt;&gt;&gt; module.a\n    1\n    &gt;&gt;&gt; module.b\n    2\n\n    ```\n    \"\"\"\n\n    if isinstance(name, Mapping) and not args and not kwargs:\n        name, kwargs = name.pop(\"name\"), name  # type: ignore\n    return self.get(name)(*args, **kwargs)\n</code></pre>","location":"registry/#danling.registry.registry.Registry.build"},{"title":"DanLing","text":"","location":"blog/"},{"title":"AverageMeter","text":"<p>Computes and stores the average and current value.</p> <p>Attributes:</p>    Name Type Description     <code>val</code>  <code>int</code>  <p>Current value.</p>   <code>avg</code>  <code>float</code>  <p>Average value.</p>   <code>sum</code>  <code>float</code>  <p>Sum of values.</p>   <code>count</code>  <code>int</code>  <p>Number of values.</p>    <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.update(0.9)\n&gt;&gt;&gt; meter.val\n0.9\n&gt;&gt;&gt; meter.avg\n0.8\n&gt;&gt;&gt; meter.sum\n1.6\n&gt;&gt;&gt; meter.count\n2\n&gt;&gt;&gt; meter.reset()\n&gt;&gt;&gt; meter.val\n0\n&gt;&gt;&gt; meter.avg\n0\n</code></pre>  Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>class AverageMeter:\n    r\"\"\"\n    Computes and stores the average and current value.\n\n    Attributes\n    ----------\n    val: int\n        Current value.\n    avg: float\n        Average value.\n    sum: float\n        Sum of values.\n    count: int\n        Number of values.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; meter = AverageMeter()\n    &gt;&gt;&gt; meter.update(0.7)\n    &gt;&gt;&gt; meter.val\n    0.7\n    &gt;&gt;&gt; meter.avg\n    0.7\n    &gt;&gt;&gt; meter.update(0.9)\n    &gt;&gt;&gt; meter.val\n    0.9\n    &gt;&gt;&gt; meter.avg\n    0.8\n    &gt;&gt;&gt; meter.sum\n    1.6\n    &gt;&gt;&gt; meter.count\n    2\n    &gt;&gt;&gt; meter.reset()\n    &gt;&gt;&gt; meter.val\n    0\n    &gt;&gt;&gt; meter.avg\n    0\n\n    ```\n    \"\"\"\n\n    val: int = 0\n    avg: float = 0\n    sum: int = 0\n    count: int = 0\n\n    def __init__(self) -&gt; None:\n        self.reset()\n\n    def reset(self) -&gt; None:\n        r\"\"\"\n        Resets the meter.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; meter = AverageMeter()\n        &gt;&gt;&gt; meter.update(0.7)\n        &gt;&gt;&gt; meter.val\n        0.7\n        &gt;&gt;&gt; meter.avg\n        0.7\n        &gt;&gt;&gt; meter.reset()\n        &gt;&gt;&gt; meter.val\n        0\n        &gt;&gt;&gt; meter.avg\n        0\n\n        ```\n        \"\"\"\n\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n: int = 1) -&gt; None:\n        r\"\"\"\n        Updates the average and current value in the meter.\n\n        Parameters\n        ----------\n        val: int\n            Value to be added to the average.\n        n: int = 1\n            Number of values to be added.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; meter = AverageMeter()\n        &gt;&gt;&gt; meter.update(0.7)\n        &gt;&gt;&gt; meter.val\n        0.7\n        &gt;&gt;&gt; meter.avg\n        0.7\n        &gt;&gt;&gt; meter.update(0.9)\n        &gt;&gt;&gt; meter.val\n        0.9\n        &gt;&gt;&gt; meter.avg\n        0.8\n        &gt;&gt;&gt; meter.sum\n        1.6\n        &gt;&gt;&gt; meter.count\n        2\n\n        ```\n        \"\"\"\n\n        # pylint: disable=C0103\n\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n</code></pre>","location":"metrics/average_meter/"},{"title":"<code>reset()</code>","text":"<p>Resets the meter.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.reset()\n&gt;&gt;&gt; meter.val\n0\n&gt;&gt;&gt; meter.avg\n0\n</code></pre>  Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>def reset(self) -&gt; None:\n    r\"\"\"\n    Resets the meter.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; meter = AverageMeter()\n    &gt;&gt;&gt; meter.update(0.7)\n    &gt;&gt;&gt; meter.val\n    0.7\n    &gt;&gt;&gt; meter.avg\n    0.7\n    &gt;&gt;&gt; meter.reset()\n    &gt;&gt;&gt; meter.val\n    0\n    &gt;&gt;&gt; meter.avg\n    0\n\n    ```\n    \"\"\"\n\n    self.val = 0\n    self.avg = 0\n    self.sum = 0\n    self.count = 0\n</code></pre>","location":"metrics/average_meter/#danling.metrics.average_meter.AverageMeter.reset"},{"title":"<code>update(val, n=1)</code>","text":"<p>Updates the average and current value in the meter.</p> <p>Parameters:</p>    Name Type Description Default     <code>val</code>   <p>Value to be added to the average.</p>  required    <code>n</code>  <code>int</code>  <p>Number of values to be added.</p>  <code>1</code>     <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.update(0.9)\n&gt;&gt;&gt; meter.val\n0.9\n&gt;&gt;&gt; meter.avg\n0.8\n&gt;&gt;&gt; meter.sum\n1.6\n&gt;&gt;&gt; meter.count\n2\n</code></pre>  Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>def update(self, val, n: int = 1) -&gt; None:\n    r\"\"\"\n    Updates the average and current value in the meter.\n\n    Parameters\n    ----------\n    val: int\n        Value to be added to the average.\n    n: int = 1\n        Number of values to be added.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; meter = AverageMeter()\n    &gt;&gt;&gt; meter.update(0.7)\n    &gt;&gt;&gt; meter.val\n    0.7\n    &gt;&gt;&gt; meter.avg\n    0.7\n    &gt;&gt;&gt; meter.update(0.9)\n    &gt;&gt;&gt; meter.val\n    0.9\n    &gt;&gt;&gt; meter.avg\n    0.8\n    &gt;&gt;&gt; meter.sum\n    1.6\n    &gt;&gt;&gt; meter.count\n    2\n\n    ```\n    \"\"\"\n\n    # pylint: disable=C0103\n\n    self.val = val\n    self.sum += val * n\n    self.count += n\n    self.avg = self.sum / self.count\n</code></pre>","location":"metrics/average_meter/#danling.metrics.average_meter.AverageMeter.update"},{"title":"Runner","text":"<p>The Runner of DanLing sets up the basic environment for running neural networks.</p>","location":"runner/"},{"title":"Components","text":"<p>For readability and maintainability, there are three levels of Runner:</p>","location":"runner/#components"},{"title":"<code>RunnerBase</code>","text":"<p><code>RunnerBase</code> gives you a basic instinct on what attributes and properties are provided by the Runner.</p> <p>It works in an AbstractBaseClass manner and should neither be used directly nor be inherited from.</p>","location":"runner/#runnerbase"},{"title":"<code>BaseRunner</code>","text":"<p><code>BaseRunner</code> contains core methods of general basic functionality, such as <code>init_logging</code>, <code>append_result</code>, <code>print_result</code>.</p>","location":"runner/#baserunner"},{"title":"<code>Runner</code>.","text":"<p><code>Runner</code> should only contain platform-specific features. Currently, only <code>TorchRunner</code> is supported.</p>","location":"runner/#runner_1"},{"title":"Philosophy","text":"<p>DanLing Runner is designed for a 3.5-level experiments management system: Project, Group, Experiment, and, Run.</p>","location":"runner/#philosophy"},{"title":"Project","text":"<p>Project corresponds to your project.</p> <p>Generally speaking, there should be only one project for each repository.</p>","location":"runner/#project"},{"title":"Group","text":"<p>A group groups multiple experiments with similar characteristics.</p> <p>For example, if you run multiple experiments on learning rate, you may want to group them into a group.</p> <p>Note that Group is a virtual level (which is why it only counts 0.5) and does not corresponds to anything.</p>","location":"runner/#group"},{"title":"Experiment","text":"<p>Experiment is the basic unit of experiments.</p> <p>Experiment usually corresponds to a commit, which means the code should be consistent across the experiment.</p>","location":"runner/#experiment"},{"title":"Run","text":"<p>Run is the basic unit of running.</p> <p>Run corresponds to a single run of an experiment, each run may have different hyperparameters.</p>","location":"runner/#run"},{"title":"Attributes &amp; Properties","text":"","location":"runner/#attributes-properties"},{"title":"General","text":"<ul> <li>id</li> <li>name</li> <li>seed</li> <li>deterministic</li> </ul>","location":"runner/#general"},{"title":"Progress","text":"<ul> <li>iters</li> <li>steps</li> <li>epochs</li> <li>iter_end</li> <li>step_end</li> <li>epoch_end</li> <li>progress</li> </ul> <p>Note that generally you should only use one of <code>iter</code>, <code>step</code>, <code>epoch</code> to indicate the progress.</p>","location":"runner/#progress"},{"title":"Model","text":"<ul> <li>model</li> <li>criterion</li> <li>optimizer</li> <li>scheduler</li> </ul>","location":"runner/#model"},{"title":"Data","text":"<ul> <li>datasets</li> <li>datasamplers</li> <li>dataloaders</li> <li>batch_size</li> <li>batch_size_equivalent</li> </ul> <p><code>datasets</code>, <code>datasamplers</code>, <code>dataloaders</code> should be a dict with the same keys. Their keys should be <code>split</code> (e.g. <code>train</code>, <code>val</code>, <code>test</code>).</p>","location":"runner/#data"},{"title":"Results","text":"<ul> <li>results</li> <li>latest_result</li> <li>best_result</li> <li>scores</li> <li>latest_score</li> <li>best_score</li> <li>index_set</li> <li>index</li> <li>is_best</li> </ul> <p><code>results</code> should be a list of <code>result</code>. <code>result</code> should be a dict with the same <code>split</code> as keys, like <code>dataloaders</code>. A typical <code>result</code> might look like this: Python<pre><code>{\n    \"train\": {\n        \"loss\": 0.1,\n        \"accuracy\": 0.9,\n    },\n    \"val\": {\n        \"loss\": 0.2,\n        \"accuracy\": 0.8,\n    },\n    \"test\": {\n        \"loss\": 0.3,\n        \"accuracy\": 0.7,\n    },\n}\n</code></pre></p> <p><code>scores</code> are usually a list of <code>float</code>, and are dynamically extracted from <code>results</code> by <code>index_set</code> and <code>index</code>. If <code>index_set = \"val\"</code>, <code>index = \"accuracy\"</code>, then <code>scores = 0.9</code>.</p>","location":"runner/#results"},{"title":"DDP","text":"<ul> <li>world_size</li> <li>rank</li> <li>local_rank</li> <li>distributed</li> <li>is_main_process</li> <li>is_local_main_process</li> </ul>","location":"runner/#ddp"},{"title":"IO","text":"<ul> <li>experiments_root</li> <li>dir</li> <li>checkpoint_dir</li> <li>log_path</li> <li>checkpoint_dir_name</li> </ul> <p><code>experiments_root</code> is the root directory of all experiments, and should be consistent across the Project.</p> <p><code>dir</code> is the directory of a certain Run.</p> <p>There is no attributes/properties for Group and Experiment.</p> <p><code>checkpoint_dir_name</code> is relative to <code>dir</code>, and is passed to generate <code>checkpoint_dir</code> (<code>checkpoint_dir = os.path.join(dir, checkpoint_dir_name)</code>). In practice, <code>checkpoint_dir_name</code> is rarely called.</p>","location":"runner/#io"},{"title":"logging","text":"<ul> <li>log</li> <li>logger</li> <li>tensorboard</li> <li>writer</li> </ul>","location":"runner/#logging"},{"title":"BaseRunner","text":"<p>         Bases: <code>RunnerBase</code></p> <p>Set up everything for running a job.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>class BaseRunner(RunnerBase):\n    r\"\"\"\n    Set up everything for running a job.\n    \"\"\"\n\n    # pylint: disable=R0902\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        if self.seed is not None:\n            self.set_seed()\n        if self.deterministic:\n            self.set_deterministic()\n        if self.log:\n            self.init_logging()\n        self.init_print()\n        if self.tensorboard:\n            self.init_tensorboard()\n\n        atexit.register(self.print_result)\n\n    @on_main_process\n    def init_logging(self) -&gt; None:\n        r\"\"\"\n        Set up logging.\n        \"\"\"\n\n        # Why is setting up proper logging so !@?#! ugly?\n        logging.config.dictConfig(\n            {\n                \"version\": 1,\n                \"disable_existing_loggers\": False,\n                \"formatters\": {\n                    \"standard\": {\"format\": \"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"},\n                },\n                \"handlers\": {\n                    \"stdout\": {\n                        \"level\": \"INFO\",\n                        \"formatter\": \"standard\",\n                        \"class\": \"logging.StreamHandler\",\n                        \"stream\": \"ext://sys.stdout\",\n                    },\n                    \"logfile\": {\n                        \"level\": \"DEBUG\",\n                        \"formatter\": \"standard\",\n                        \"class\": \"logging.FileHandler\",\n                        \"filename\": self.log_path,\n                        \"mode\": \"a\",\n                    },\n                },\n                \"loggers\": {\n                    \"\": {\n                        \"handlers\": [\"stdout\", \"logfile\"],\n                        \"level\": \"DEBUG\",\n                        \"propagate\": True,\n                    },\n                },\n            }\n        )\n        logging.captureWarnings(True)\n        self.logger = logging.getLogger(\"runner\")\n        self.logger.flush = lambda: [h.flush() for h in self.logger.handlers]  # type: ignore\n\n    def init_print(self, process: int = 0) -&gt; None:\n        r\"\"\"\n        Set up print.\n\n        Only print on a specific process or when force is indicated.\n\n        Parameters\n        ----------\n        process: int, optional\n            The process to print on.\n\n        Notes\n        -----\n        If `self.log = True`, the default `print` function will be override by `logging.info`.\n        \"\"\"\n\n        logger = logging.getLogger(\"print\")\n        logger.flush = lambda: [h.flush for h in logger.handlers]  # type: ignore\n        import builtins as __builtin__  # pylint: disable=C0415\n\n        builtin_print = __builtin__.print\n\n        @catch\n        def print(*args, force=False, end=\"\\n\", file=None, flush=False, **kwargs):  # pylint: disable=W0622\n            if self.rank == process or force:\n                if self.log:\n                    logger.info(*args, **kwargs)\n                else:\n                    builtin_print(*args, end=end, file=file, flush=flush, **kwargs)\n\n        __builtin__.print = print\n\n    def set_deterministic(self) -&gt; None:\n        r\"\"\"\n        Set up deterministic.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def set_seed(self, bias: Optional[int] = None) -&gt; None:\n        r\"\"\"\n        Set up random seed.\n\n        Parameters\n        ----------\n        bias: Optional[int] = self.rank\n            Make the seed different for each processes.\n            This avoids same data augmentation are applied on every processes.\n            Set to `False` to disable this feature.\n        \"\"\"\n\n        if bias is None:\n            bias = self.rank\n        seed = self.seed + bias if bias else self.seed\n        np.random.seed(seed)\n        random.seed(seed)\n\n    def scale_lr(\n        self,\n        lr_scale_factor: Optional[float] = None,\n        batch_size_base: Optional[int] = None,\n    ) -&gt; None:\n        r\"\"\"\n        Scale learning rate according to [linear scaling rule](https://arxiv.org/abs/1706.02677).\n        \"\"\"\n\n        # pylint: disable=W0201\n\n        if lr_scale_factor is None:\n            if batch_size_base is None:\n                if batch_size_base := getattr(self, \"batch_size_base\", None) is None:\n                    raise ValueError(\"batch_size_base must be specified to auto scale lr\")\n            lr_scale_factor = self.batch_size_equivalent / batch_size_base\n        self.lr_scale_factor = lr_scale_factor\n        self.lr = self.lr * self.lr_scale_factor  # type: float  # pylint: disable=C0103\n        self.lr_final = self.lr_final * self.lr_scale_factor  # type: float\n\n    def step(self, zero_grad: bool = True) -&gt; None:\n        r\"\"\"\n        Step optimizer and scheduler.\n\n        This method also increment the `self.steps` attribute.\n\n        Parameters\n        ----------\n        zero_grad: bool, optional\n            Whether to zero the gradients.\n        \"\"\"\n\n        if self.optimizer is not None:\n            self.optimizer.step()\n            if zero_grad:\n                self.optimizer.zero_grad()\n        if self.scheduler is not None:\n            self.scheduler.step()\n        self.steps += 1\n        # TODO: Support `drop_last = False`\n        self.iters += self.batch_size_equivalent\n\n    def state_dict(self, cls: Callable = dict) -&gt; Mapping:\n        r\"\"\"\n        Return dict of all attributes for checkpoint.\n        \"\"\"\n\n        if self.model is None:\n            raise ValueError(\"Model must be defined when calling state_dict\")\n        model = self.accelerator.unwrap_model(self.model)\n        return cls(\n            runner=self.dict(),\n            model=model.state_dict(),\n            optimizer=self.optimizer.state_dict() if self.optimizer else None,\n            scheduler=self.scheduler.state_dict() if self.scheduler else None,\n        )\n\n    @catch\n    @on_main_process\n    def save_checkpoint(self) -&gt; None:\n        r\"\"\"\n        Save checkpoint to `runner.checkpoint_dir`.\n\n        The checkpoint will be saved to `runner.checkpoint_dir/latest.pth`.\n\n        If `save_freq` is specified and `self.epochs + 1` is a multiple of `save_freq`,\n        the checkpoint will also be copied to `runner.checkpoint_dir/epoch-{self.epochs}.pth`.\n\n        If `self.is_best` is `True`, the checkpoint will also be copied to `runner.checkpoint_dir/best.pth`.\n        \"\"\"\n\n        latest_path = os.path.join(self.checkpoint_dir, \"latest.pth\")\n        self.save(self.state_dict(), latest_path)\n        if hasattr(self, \"save_freq\") and (self.epochs + 1) % self.save_freq == 0:\n            save_path = os.path.join(self.checkpoint_dir, f\"epoch-{self.epochs}.pth\")\n            shutil.copy(latest_path, save_path)\n        if self.is_best:\n            best_path = os.path.join(self.checkpoint_dir, \"best.pth\")\n            shutil.copy(latest_path, best_path)\n\n    def load_checkpoint(  # pylint: disable=W1113\n        self, checkpoint: Optional[Union[Mapping, str]] = None, override_config: bool = True, *args, **kwargs\n    ) -&gt; None:\n        \"\"\"\n        Load info from checkpoint.\n\n        Parameters\n        ----------\n        checkpoint: Optional[Union[Mapping, str]] = latest_checkpoint\n            Checkpoint (or its path) to load.\n        override_config: bool = True\n            If True, override runner config with checkpoint config.\n        *args, **kwargs\n            Additional arguments to pass to `runner.load`.\n\n        Raises\n        ------\n        FileNotFoundError\n            If `checkpoint` does not exists.\n\n        See also\n        --------\n        from_checkpoint: Build runner from checkpoint.\n        \"\"\"\n\n        if checkpoint is None:\n            checkpoint = os.path.join(self.checkpoint_dir, \"latest.pth\")\n        # TODO: Support loading checkpoints in other format\n        if isinstance(checkpoint, str):\n            if not os.path.exists(checkpoint):\n                raise FileNotFoundError(f\"checkpoint is set to {checkpoint} but does not exist.\")\n            state_dict = self.load(checkpoint, *args, **kwargs)\n        # TODO: Wrap state_dict in a dataclass\n        if override_config:\n            self.__dict__.update(state_dict[\"runner\"])  # type: ignore\n        if self.model is not None and \"model\" in state_dict:  # type: ignore\n            self.model.load_state_dict(state_dict[\"model\"])  # type: ignore\n        if self.optimizer is not None and \"optimizer\" in state_dict:  # type: ignore\n            self.optimizer.load_state_dict(state_dict[\"optimizer\"])  # type: ignore\n        if self.scheduler is not None and \"scheduler\" in state_dict:  # type: ignore\n            self.scheduler.load_state_dict(state_dict[\"scheduler\"])  # type: ignore\n        self.checkpoint = checkpoint  # pylint: disable=W0201\n\n    @classmethod\n    def from_checkpoint(cls, checkpoint: Union[Mapping, str], *args, **kwargs) -&gt; BaseRunner:\n        r\"\"\"\n        Build BaseRunner from checkpoint.\n\n        Parameters\n        ----------\n        checkpoint: Optional[Union[Mapping, str]] = latest_checkpoint\n            Checkpoint (or its path) to load.\n        *args, **kwargs\n            Additional arguments to pass to `runner.load`.\n\n        Returns\n        -------\n        BaseRunner\n        \"\"\"\n\n        if isinstance(checkpoint, str):\n            checkpoint = cls.load(checkpoint, *args, **kwargs)\n        runner = cls(**checkpoint[\"runner\"])  # type: ignore\n        runner.load_checkpoint(checkpoint, override_config=False)\n        return runner\n\n    def append_result(self, result) -&gt; None:\n        r\"\"\"\n        Append result to `self.results`.\n\n        Warnings\n        --------\n        `self.results` is heavily relied upon for computing metrics.\n\n        Failed to use this method may lead to unexpected behavior.\n        \"\"\"\n\n        self.results.append(result)\n\n    def print_result(self) -&gt; None:\n        r\"\"\"\n        Print latest and best result.\n        \"\"\"\n\n        print(f\"latest result: {self.latest_result}\")\n        print(f\"best result: {self.best_result}\")\n\n    @catch\n    @on_main_process\n    def save_result(self) -&gt; None:\n        r\"\"\"\n        Save result to `runner.dir`.\n\n        This method will save latest and best result to\n        `runner.dir/latest.json` and `runner.dir/best.json` respectively.\n        \"\"\"\n\n        ret = {\"id\": self.id, \"name\": self.name}\n        result = self.latest_result  # type: ignore\n        if isinstance(result, FlatDict):\n            result = result.dict()  # type: ignore\n        # This is slower but ensure id is the first key\n        ret.update(result)  # type: ignore\n        latest_path = os.path.join(self.dir, \"latest.json\")\n        with FlatDict.open(latest_path, \"w\") as f:  # pylint: disable=C0103\n            json.dump(ret, f, indent=4)\n        if self.is_best:\n            best_path = os.path.join(self.dir, \"best.json\")\n            shutil.copy(latest_path, best_path)\n</code></pre>","location":"runner/base_runner/"},{"title":"<code>step(zero_grad=True)</code>","text":"<p>Step optimizer and scheduler.</p> <p>This method also increment the <code>self.steps</code> attribute.</p> <p>Parameters:</p>    Name Type Description Default     <code>zero_grad</code>  <code>bool</code>  <p>Whether to zero the gradients.</p>  <code>True</code>      Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def step(self, zero_grad: bool = True) -&gt; None:\n    r\"\"\"\n    Step optimizer and scheduler.\n\n    This method also increment the `self.steps` attribute.\n\n    Parameters\n    ----------\n    zero_grad: bool, optional\n        Whether to zero the gradients.\n    \"\"\"\n\n    if self.optimizer is not None:\n        self.optimizer.step()\n        if zero_grad:\n            self.optimizer.zero_grad()\n    if self.scheduler is not None:\n        self.scheduler.step()\n    self.steps += 1\n    # TODO: Support `drop_last = False`\n    self.iters += self.batch_size_equivalent\n</code></pre>","location":"runner/base_runner/#danling.runner.base_runner.BaseRunner.step"},{"title":"<code>append_result(result)</code>","text":"<p>Append result to <code>self.results</code>.</p>","location":"runner/base_runner/#danling.runner.base_runner.BaseRunner.append_result"},{"title":"Warnings","text":"<p><code>self.results</code> is heavily relied upon for computing metrics.</p> <p>Failed to use this method may lead to unexpected behavior.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def append_result(self, result) -&gt; None:\n    r\"\"\"\n    Append result to `self.results`.\n\n    Warnings\n    --------\n    `self.results` is heavily relied upon for computing metrics.\n\n    Failed to use this method may lead to unexpected behavior.\n    \"\"\"\n\n    self.results.append(result)\n</code></pre>","location":"runner/base_runner/#danling.runner.base_runner.BaseRunner.append_result--warnings"},{"title":"<code>print_result()</code>","text":"<p>Print latest and best result.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def print_result(self) -&gt; None:\n    r\"\"\"\n    Print latest and best result.\n    \"\"\"\n\n    print(f\"latest result: {self.latest_result}\")\n    print(f\"best result: {self.best_result}\")\n</code></pre>","location":"runner/base_runner/#danling.runner.base_runner.BaseRunner.print_result"},{"title":"<code>save_result()</code>","text":"<p>Save result to <code>runner.dir</code>.</p> <p>This method will save latest and best result to <code>runner.dir/latest.json</code> and <code>runner.dir/best.json</code> respectively.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\n@on_main_process\ndef save_result(self) -&gt; None:\n    r\"\"\"\n    Save result to `runner.dir`.\n\n    This method will save latest and best result to\n    `runner.dir/latest.json` and `runner.dir/best.json` respectively.\n    \"\"\"\n\n    ret = {\"id\": self.id, \"name\": self.name}\n    result = self.latest_result  # type: ignore\n    if isinstance(result, FlatDict):\n        result = result.dict()  # type: ignore\n    # This is slower but ensure id is the first key\n    ret.update(result)  # type: ignore\n    latest_path = os.path.join(self.dir, \"latest.json\")\n    with FlatDict.open(latest_path, \"w\") as f:  # pylint: disable=C0103\n        json.dump(ret, f, indent=4)\n    if self.is_best:\n        best_path = os.path.join(self.dir, \"best.json\")\n        shutil.copy(latest_path, best_path)\n</code></pre>","location":"runner/base_runner/#danling.runner.base_runner.BaseRunner.save_result"},{"title":"<code>state_dict(cls=dict)</code>","text":"<p>Return dict of all attributes for checkpoint.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def state_dict(self, cls: Callable = dict) -&gt; Mapping:\n    r\"\"\"\n    Return dict of all attributes for checkpoint.\n    \"\"\"\n\n    if self.model is None:\n        raise ValueError(\"Model must be defined when calling state_dict\")\n    model = self.accelerator.unwrap_model(self.model)\n    return cls(\n        runner=self.dict(),\n        model=model.state_dict(),\n        optimizer=self.optimizer.state_dict() if self.optimizer else None,\n        scheduler=self.scheduler.state_dict() if self.scheduler else None,\n    )\n</code></pre>","location":"runner/base_runner/#danling.runner.base_runner.BaseRunner.state_dict"},{"title":"<code>save_checkpoint()</code>","text":"<p>Save checkpoint to <code>runner.checkpoint_dir</code>.</p> <p>The checkpoint will be saved to <code>runner.checkpoint_dir/latest.pth</code>.</p> <p>If <code>save_freq</code> is specified and <code>self.epochs + 1</code> is a multiple of <code>save_freq</code>, the checkpoint will also be copied to <code>runner.checkpoint_dir/epoch-{self.epochs}.pth</code>.</p> <p>If <code>self.is_best</code> is <code>True</code>, the checkpoint will also be copied to <code>runner.checkpoint_dir/best.pth</code>.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\n@on_main_process\ndef save_checkpoint(self) -&gt; None:\n    r\"\"\"\n    Save checkpoint to `runner.checkpoint_dir`.\n\n    The checkpoint will be saved to `runner.checkpoint_dir/latest.pth`.\n\n    If `save_freq` is specified and `self.epochs + 1` is a multiple of `save_freq`,\n    the checkpoint will also be copied to `runner.checkpoint_dir/epoch-{self.epochs}.pth`.\n\n    If `self.is_best` is `True`, the checkpoint will also be copied to `runner.checkpoint_dir/best.pth`.\n    \"\"\"\n\n    latest_path = os.path.join(self.checkpoint_dir, \"latest.pth\")\n    self.save(self.state_dict(), latest_path)\n    if hasattr(self, \"save_freq\") and (self.epochs + 1) % self.save_freq == 0:\n        save_path = os.path.join(self.checkpoint_dir, f\"epoch-{self.epochs}.pth\")\n        shutil.copy(latest_path, save_path)\n    if self.is_best:\n        best_path = os.path.join(self.checkpoint_dir, \"best.pth\")\n        shutil.copy(latest_path, best_path)\n</code></pre>","location":"runner/base_runner/#danling.runner.base_runner.BaseRunner.save_checkpoint"},{"title":"<code>load_checkpoint(checkpoint=None, override_config=True, *args, **kwargs)</code>","text":"<p>Load info from checkpoint.</p> <p>Parameters:</p>    Name Type Description Default     <code>checkpoint</code>  <code>Optional[Union[Mapping, str]]</code>  <p>Checkpoint (or its path) to load.</p>  <code>None</code>    <code>override_config</code>  <code>bool</code>  <p>If True, override runner config with checkpoint config.</p>  <code>True</code>    <code>*args</code>   <p>Additional arguments to pass to <code>runner.load</code>.</p>  <code>()</code>    <code>**kwargs</code>   <p>Additional arguments to pass to <code>runner.load</code>.</p>  <code>()</code>     <p>Raises:</p>    Type Description      <code>FileNotFoundError</code>  <p>If <code>checkpoint</code> does not exists.</p>","location":"runner/base_runner/#danling.runner.base_runner.BaseRunner.load_checkpoint"},{"title":"See also","text":"<p>from_checkpoint: Build runner from checkpoint.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def load_checkpoint(  # pylint: disable=W1113\n    self, checkpoint: Optional[Union[Mapping, str]] = None, override_config: bool = True, *args, **kwargs\n) -&gt; None:\n    \"\"\"\n    Load info from checkpoint.\n\n    Parameters\n    ----------\n    checkpoint: Optional[Union[Mapping, str]] = latest_checkpoint\n        Checkpoint (or its path) to load.\n    override_config: bool = True\n        If True, override runner config with checkpoint config.\n    *args, **kwargs\n        Additional arguments to pass to `runner.load`.\n\n    Raises\n    ------\n    FileNotFoundError\n        If `checkpoint` does not exists.\n\n    See also\n    --------\n    from_checkpoint: Build runner from checkpoint.\n    \"\"\"\n\n    if checkpoint is None:\n        checkpoint = os.path.join(self.checkpoint_dir, \"latest.pth\")\n    # TODO: Support loading checkpoints in other format\n    if isinstance(checkpoint, str):\n        if not os.path.exists(checkpoint):\n            raise FileNotFoundError(f\"checkpoint is set to {checkpoint} but does not exist.\")\n        state_dict = self.load(checkpoint, *args, **kwargs)\n    # TODO: Wrap state_dict in a dataclass\n    if override_config:\n        self.__dict__.update(state_dict[\"runner\"])  # type: ignore\n    if self.model is not None and \"model\" in state_dict:  # type: ignore\n        self.model.load_state_dict(state_dict[\"model\"])  # type: ignore\n    if self.optimizer is not None and \"optimizer\" in state_dict:  # type: ignore\n        self.optimizer.load_state_dict(state_dict[\"optimizer\"])  # type: ignore\n    if self.scheduler is not None and \"scheduler\" in state_dict:  # type: ignore\n        self.scheduler.load_state_dict(state_dict[\"scheduler\"])  # type: ignore\n    self.checkpoint = checkpoint  # pylint: disable=W0201\n</code></pre>","location":"runner/base_runner/#danling.runner.base_runner.BaseRunner.load_checkpoint--see-also"},{"title":"<code>from_checkpoint(checkpoint, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Build BaseRunner from checkpoint.</p> <p>Parameters:</p>    Name Type Description Default     <code>checkpoint</code>  <code>Union[Mapping, str]</code>  <p>Checkpoint (or its path) to load.</p>  required    <code>*args</code>   <p>Additional arguments to pass to <code>runner.load</code>.</p>  <code>()</code>    <code>**kwargs</code>   <p>Additional arguments to pass to <code>runner.load</code>.</p>  <code>()</code>     <p>Returns:</p>    Type Description      <code>BaseRunner</code>       Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@classmethod\ndef from_checkpoint(cls, checkpoint: Union[Mapping, str], *args, **kwargs) -&gt; BaseRunner:\n    r\"\"\"\n    Build BaseRunner from checkpoint.\n\n    Parameters\n    ----------\n    checkpoint: Optional[Union[Mapping, str]] = latest_checkpoint\n        Checkpoint (or its path) to load.\n    *args, **kwargs\n        Additional arguments to pass to `runner.load`.\n\n    Returns\n    -------\n    BaseRunner\n    \"\"\"\n\n    if isinstance(checkpoint, str):\n        checkpoint = cls.load(checkpoint, *args, **kwargs)\n    runner = cls(**checkpoint[\"runner\"])  # type: ignore\n    runner.load_checkpoint(checkpoint, override_config=False)\n    return runner\n</code></pre>","location":"runner/base_runner/#danling.runner.base_runner.BaseRunner.from_checkpoint"},{"title":"<code>init_logging()</code>","text":"<p>Set up logging.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@on_main_process\ndef init_logging(self) -&gt; None:\n    r\"\"\"\n    Set up logging.\n    \"\"\"\n\n    # Why is setting up proper logging so !@?#! ugly?\n    logging.config.dictConfig(\n        {\n            \"version\": 1,\n            \"disable_existing_loggers\": False,\n            \"formatters\": {\n                \"standard\": {\"format\": \"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"},\n            },\n            \"handlers\": {\n                \"stdout\": {\n                    \"level\": \"INFO\",\n                    \"formatter\": \"standard\",\n                    \"class\": \"logging.StreamHandler\",\n                    \"stream\": \"ext://sys.stdout\",\n                },\n                \"logfile\": {\n                    \"level\": \"DEBUG\",\n                    \"formatter\": \"standard\",\n                    \"class\": \"logging.FileHandler\",\n                    \"filename\": self.log_path,\n                    \"mode\": \"a\",\n                },\n            },\n            \"loggers\": {\n                \"\": {\n                    \"handlers\": [\"stdout\", \"logfile\"],\n                    \"level\": \"DEBUG\",\n                    \"propagate\": True,\n                },\n            },\n        }\n    )\n    logging.captureWarnings(True)\n    self.logger = logging.getLogger(\"runner\")\n    self.logger.flush = lambda: [h.flush() for h in self.logger.handlers]  # type: ignore\n</code></pre>","location":"runner/base_runner/#danling.runner.base_runner.BaseRunner.init_logging"},{"title":"<code>init_print(process=0)</code>","text":"<p>Set up print.</p> <p>Only print on a specific process or when force is indicated.</p> <p>Parameters:</p>    Name Type Description Default     <code>process</code>  <code>int</code>  <p>The process to print on.</p>  <code>0</code>","location":"runner/base_runner/#danling.runner.base_runner.BaseRunner.init_print"},{"title":"Notes","text":"<p>If <code>self.log = True</code>, the default <code>print</code> function will be override by <code>logging.info</code>.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def init_print(self, process: int = 0) -&gt; None:\n    r\"\"\"\n    Set up print.\n\n    Only print on a specific process or when force is indicated.\n\n    Parameters\n    ----------\n    process: int, optional\n        The process to print on.\n\n    Notes\n    -----\n    If `self.log = True`, the default `print` function will be override by `logging.info`.\n    \"\"\"\n\n    logger = logging.getLogger(\"print\")\n    logger.flush = lambda: [h.flush for h in logger.handlers]  # type: ignore\n    import builtins as __builtin__  # pylint: disable=C0415\n\n    builtin_print = __builtin__.print\n\n    @catch\n    def print(*args, force=False, end=\"\\n\", file=None, flush=False, **kwargs):  # pylint: disable=W0622\n        if self.rank == process or force:\n            if self.log:\n                logger.info(*args, **kwargs)\n            else:\n                builtin_print(*args, end=end, file=file, flush=flush, **kwargs)\n\n    __builtin__.print = print\n</code></pre>","location":"runner/base_runner/#danling.runner.base_runner.BaseRunner.init_print--notes"},{"title":"<code>set_deterministic()</code>","text":"<p>Set up deterministic.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def set_deterministic(self) -&gt; None:\n    r\"\"\"\n    Set up deterministic.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>","location":"runner/base_runner/#danling.runner.base_runner.BaseRunner.set_deterministic"},{"title":"<code>set_seed(bias=None)</code>","text":"<p>Set up random seed.</p> <p>Parameters:</p>    Name Type Description Default     <code>bias</code>  <code>Optional[int]</code>  <p>Make the seed different for each processes. This avoids same data augmentation are applied on every processes. Set to <code>False</code> to disable this feature.</p>  <code>None</code>      Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def set_seed(self, bias: Optional[int] = None) -&gt; None:\n    r\"\"\"\n    Set up random seed.\n\n    Parameters\n    ----------\n    bias: Optional[int] = self.rank\n        Make the seed different for each processes.\n        This avoids same data augmentation are applied on every processes.\n        Set to `False` to disable this feature.\n    \"\"\"\n\n    if bias is None:\n        bias = self.rank\n    seed = self.seed + bias if bias else self.seed\n    np.random.seed(seed)\n    random.seed(seed)\n</code></pre>","location":"runner/base_runner/#danling.runner.base_runner.BaseRunner.set_seed"},{"title":"<code>scale_lr(lr_scale_factor=None, batch_size_base=None)</code>","text":"<p>Scale learning rate according to linear scaling rule.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def scale_lr(\n    self,\n    lr_scale_factor: Optional[float] = None,\n    batch_size_base: Optional[int] = None,\n) -&gt; None:\n    r\"\"\"\n    Scale learning rate according to [linear scaling rule](https://arxiv.org/abs/1706.02677).\n    \"\"\"\n\n    # pylint: disable=W0201\n\n    if lr_scale_factor is None:\n        if batch_size_base is None:\n            if batch_size_base := getattr(self, \"batch_size_base\", None) is None:\n                raise ValueError(\"batch_size_base must be specified to auto scale lr\")\n        lr_scale_factor = self.batch_size_equivalent / batch_size_base\n    self.lr_scale_factor = lr_scale_factor\n    self.lr = self.lr * self.lr_scale_factor  # type: float  # pylint: disable=C0103\n    self.lr_final = self.lr_final * self.lr_scale_factor  # type: float\n</code></pre>","location":"runner/base_runner/#danling.runner.base_runner.BaseRunner.scale_lr"},{"title":"RunnerBase","text":"<p>Base class for all runners.</p> <p><code>RunnerBase</code> is designed as a \u201cdataclass\u201d.</p> <p>It defines all basic attributes and relevant properties such as <code>scores</code>, <code>progress</code>, etc.</p> <p><code>RunnerBase</code> also defines basic IO operations such as <code>save</code>, <code>load</code>, <code>json</code>, <code>yaml</code>, etc.</p> <p>Attributes:</p>    Name Type Description     <code>id</code>  <code>str = f\"{self.name}-{self.seed}\"</code>     <code>name</code>  <code>str = \"danling\"</code>     <code>seed</code>  <code>int = randint(0, 2**32 - 1)</code>     <code>deterministic</code>  <code>bool = False</code>  <p>Ensure deterministic operations.</p>   <code>iters</code>  <code>int = 0</code>  <p>Current running iters. Iters refers to the number of data samples processed. Iters equals to steps when batch size is 1.</p>   <code>steps</code>  <code>int = 0</code>  <p>Current running steps. Steps refers to the number of <code>step</code> calls.</p>   <code>epochs</code>  <code>int = 0</code>  <p>Current running epochs. Epochs refers to the number of complete passes over the datasets.</p>   <code>iter_end</code>  <code>int</code>  <p>End running iters. Note that <code>step_end</code> not initialised since this variable may not apply to some Runners.</p>   <code>step_end</code>  <code>int</code>  <p>End running steps. Note that <code>step_end</code> not initialised since this variable may not apply to some Runners.</p>   <code>epoch_end</code>  <code>int</code>  <p>End running epochs. Note that <code>epoch_end</code> not initialised since this variable may not apply to some Runners.</p>   <code>model</code>  <code>Optional = None</code>     <code>criterion</code>  <code>Optional = None</code>     <code>optimizer</code>  <code>Optional = None</code>     <code>scheduler</code>  <code>Optional = None</code>     <code>datasets</code>  <code>FlatDict</code>  <p>All datasets, should be in the form of <code>{subset: dataset}</code>.</p>   <code>datasamplers</code>  <code>FlatDict</code>  <p>All datasamplers, should be in the form of <code>{subset: datasampler}</code>.</p>   <code>dataloaders</code>  <code>FlatDict</code>  <p>All dataloaders, should be in the form of <code>{subset: dataloader}</code>.</p>   <code>batch_size</code>  <code>int = 1</code>     <code>results</code>  <code>List[NestedDict] = []</code>  <p>All results, should be in the form of <code>[{subset: {index: score}}]</code>.</p>   <code>index_set</code>  <code>str = 'val'</code>  <p>The subset to calculate the core score.</p>   <code>index</code>  <code>str = 'loss'</code>  <p>The index to calculate the core score.</p>   <code>experiments_root</code>  <code>str = \"experiments\"</code>  <p>The root directory for all experiments.</p>   <code>checkpoint_dir_name</code>  <code>str = \"checkpoints\"</code>  <p>The name of the directory under <code>runner.dir</code> to save checkpoints.</p>   <code>log</code>  <code>bool = True</code>  <p>Whether to log the results.</p>   <code>logger</code>  <code>Optional[logging.Logger] = None</code>     <code>tensorboard</code>  <code>bool = False</code>  <p>Whether to use tensorboard.</p>   <code>writer</code>  <code>Optional[SummaryWriter] = None</code>","location":"runner/bases/"},{"title":"Notes","text":"<p>The <code>RunnerBase</code> class is not intended to be used directly, nor to be directly inherit from.</p> <p>This is because <code>RunnerBase</code> is designed as a \u201cdataclass\u201d, and is meant for demonstrating all attributes and properties only.</p>","location":"runner/bases/#danling.runner.bases.RunnerBase--notes"},{"title":"See Also","text":"<p><code>BaseRunner</code>: The base runner class.</p>  Source code in <code>danling/runner/bases.py</code> Python<pre><code>class RunnerBase:\n    r\"\"\"\n    Base class for all runners.\n\n    `RunnerBase` is designed as a \"dataclass\".\n\n    It defines all basic attributes and relevant properties such as `scores`, `progress`, etc.\n\n    `RunnerBase` also defines basic IO operations such as `save`, `load`, `json`, `yaml`, etc.\n\n    Attributes\n    ----------\n    id: str = f\"{self.name}-{self.seed}\"\n    name: str = \"danling\"\n    seed: int = randint(0, 2**32 - 1)\n    deterministic: bool = False\n        Ensure [deterministic](https://pytorch.org/docs/stable/notes/randomness.html) operations.\n    iters: int = 0\n        Current running iters.\n        Iters refers to the number of data samples processed.\n        Iters equals to steps when batch size is 1.\n    steps: int = 0\n        Current running steps.\n        Steps refers to the number of `step` calls.\n    epochs: int = 0\n        Current running epochs.\n        Epochs refers to the number of complete passes over the datasets.\n    iter_end: int\n        End running iters.\n        Note that `step_end` not initialised since this variable may not apply to some Runners.\n    step_end: int\n        End running steps.\n        Note that `step_end` not initialised since this variable may not apply to some Runners.\n    epoch_end: int\n        End running epochs.\n        Note that `epoch_end` not initialised since this variable may not apply to some Runners.\n    model: Optional = None\n    criterion: Optional = None\n    optimizer: Optional = None\n    scheduler: Optional = None\n    datasets: FlatDict\n        All datasets, should be in the form of ``{subset: dataset}``.\n    datasamplers: FlatDict\n        All datasamplers, should be in the form of ``{subset: datasampler}``.\n    dataloaders: FlatDict\n        All dataloaders, should be in the form of ``{subset: dataloader}``.\n    batch_size: int = 1\n    results: List[NestedDict] = []\n        All results, should be in the form of ``[{subset: {index: score}}]``.\n    index_set: str = 'val'\n        The subset to calculate the core score.\n    index: str = 'loss'\n        The index to calculate the core score.\n    experiments_root: str = \"experiments\"\n        The root directory for all experiments.\n    checkpoint_dir_name: str = \"checkpoints\"\n        The name of the directory under `runner.dir` to save checkpoints.\n    log: bool = True\n        Whether to log the results.\n    logger: Optional[logging.Logger] = None\n    tensorboard: bool = False\n        Whether to use tensorboard.\n    writer: Optional[SummaryWriter] = None\n\n    Notes\n    -----\n    The `RunnerBase` class is not intended to be used directly, nor to be directly inherit from.\n\n    This is because `RunnerBase` is designed as a \"dataclass\",\n    and is meant for demonstrating all attributes and properties only.\n\n    See Also\n    --------\n    [`BaseRunner`][danling.base_runner.BaseRunner]: The base runner class.\n    \"\"\"\n\n    # pylint: disable=R0902, R0904\n\n    id: str = \"\"\n    name: str = \"DanLing\"\n\n    seed: int\n    deterministic: bool\n\n    iters: int\n    steps: int\n    epochs: int\n    # iter_begin: int  # Deprecated\n    # step_begin: int  # Deprecated\n    # epoch_begin: int  # Deprecated\n    iter_end: int\n    step_end: int\n    epoch_end: int\n\n    model: Optional = None\n    criterion: Optional = None\n    optimizer: Optional = None\n    scheduler: Optional = None\n\n    datasets: FlatDict\n    datasamplers: FlatDict\n    dataloaders: FlatDict\n\n    batch_size: int\n\n    results: List[NestedDict] = []\n    index_set: Optional[str]\n    index: str\n\n    experiments_root: str = \"experiments\"\n    checkpoint_dir_name: str = \"checkpoints\"\n    log: bool\n    logger: Optional[logging.Logger] = None\n    tensorboard: bool\n    writer: Optional = None\n\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        # Init attributes that should be kept in checkpoint inside `__init__`.\n        # Note that attributes should be init before redefine `self.__dict__`.\n        self.deterministic = False\n        self.iters = 0\n        self.steps = 0\n        self.epochs = 0\n        self.batch_size = 1\n        self.seed = randint(0, 2**32 - 1)\n        self.datasets = FlatDict()\n        self.datasamplers = FlatDict()\n        self.dataloaders = FlatDict()\n        self.index_set = None\n        self.index = \"loss\"\n        if len(args) == 1 and isinstance(args[0], FlatDict) and not kwargs:\n            args, kwargs = (), args[0]\n        self.__dict__ = NestedDict(**self.__dict__)\n        self.__dict__.update(args)\n        self.__dict__.update(kwargs)\n        if not self.id:\n            self.id = f\"{self.name}-{self.seed}\"  # pylint: disable=C0103\n\n    @property\n    def progress(self) -&gt; float:\n        r\"\"\"\n        Training Progress.\n\n        Returns\n        -------\n        float\n\n        Raises\n        ------\n        RuntimeError\n            If no terminal is defined.\n        \"\"\"\n\n        if hasattr(self, \"iter_end\"):\n            return self.iters / self.iter_end\n        if hasattr(self, \"step_end\"):\n            return self.steps / self.step_end\n        if hasattr(self, \"epoch_end\"):\n            return self.epochs / self.epoch_end\n        raise RuntimeError(\"DanLing cannot determine progress since no terminal is defined.\")\n\n    @property\n    def batch_size_equivalent(self) -&gt; int:\n        r\"\"\"\n        Actual batch size.\n\n        `batch_size` * `world_size` * `accum_steps`\n        \"\"\"\n\n        return self.batch_size * self.world_size * getattr(self, \"accum_steps\", 1)\n\n    @property\n    def latest_result(self) -&gt; Optional[NestedDict]:\n        r\"\"\"\n        Latest result.\n\n        Returns\n        -------\n        Optional[NestedDict]\n        \"\"\"\n\n        return self.results[-1] if self.results else None\n\n    @property\n    def best_result(self) -&gt; Optional[NestedDict]:\n        r\"\"\"\n        Best result.\n\n        Returns\n        -------\n        Optional[NestedDict]\n        \"\"\"\n\n        return self.results[-1 - self.scores[::-1].index(self.best_score)] if self.results else None  # type: ignore\n\n    @property\n    def scores(self) -&gt; List[float]:\n        r\"\"\"\n        All scores.\n\n        Scores are extracted from results by `index_set` and `runner.index`,\n        following `[r[index_set][self.index] for r in self.results]`.\n\n        By default, `index_set` points to `self.index_set` and is set to `val`,\n        if `self.index_set` is not set, it will be the last key of the last result.\n\n        Scores are considered as the index of the performance of the model.\n        It is useful to determine the best model and the best hyper-parameters.\n\n        Returns\n        -------\n        List[float]\n        \"\"\"\n\n        if not self.results:\n            return []\n        index_set = self.index_set or next(reversed(self.results[-1]))\n        return [r[index_set][self.index] for r in self.results]\n\n    @property\n    def latest_score(self) -&gt; Optional[float]:\n        r\"\"\"\n        Latest score.\n\n        Returns\n        -------\n        Optional[float]\n        \"\"\"\n\n        return self.scores[-1] if self.results else None\n\n    @property\n    def best_score(self) -&gt; Optional[float]:\n        r\"\"\"\n        Best score.\n\n        Returns\n        -------\n        Optional[float]\n        \"\"\"\n\n        return self.best_fn(self.scores) if self.results else None\n\n    @staticmethod\n    def best_fn(scores: Sequence[float], fn: Callable = max) -&gt; float:  # pylint: disable=C0103\n        r\"\"\"\n        Function to determine the best score from a list of scores.\n\n        Subclass can override this method to accommodate needs, such as `min(scores)`.\n\n        Parameters\n        ----------\n        scores: Sequence[float]\n            List of scores.\n        fn: Callable = max\n            Function to determine the best score from a list of scores.\n\n        Returns\n        -------\n        best_score: float\n            The best score from a list of scores.\n        \"\"\"\n\n        return fn(scores)\n\n    @property\n    def is_best(self) -&gt; bool:\n        r\"\"\"\n        If current epoch is the best epoch.\n\n        Returns\n        -------\n        bool\n        \"\"\"\n\n        return abs(self.latest_score - self.best_score) &lt; 1e-7\n        # return self.latest_score == self.best_score\n\n    @property\n    def world_size(self) -&gt; int:\n        r\"\"\"\n        Number of Processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return 1\n\n    @property\n    def rank(self) -&gt; int:\n        r\"\"\"\n        Process index in all processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return 0\n\n    @property\n    def local_rank(self) -&gt; int:\n        r\"\"\"\n        Process index in local processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return 0\n\n    @property\n    def distributed(self) -&gt; bool:\n        r\"\"\"\n        If runner is running in distributed mode.\n        \"\"\"\n\n        return self.world_size &gt; 1\n\n    @property\n    def is_main_process(self) -&gt; bool:\n        r\"\"\"\n        If current process is the main process.\n\n        Returns\n        -------\n        bool\n        \"\"\"\n\n        return self.rank == 0\n\n    @property\n    def is_local_main_process(self) -&gt; bool:\n        r\"\"\"\n        If current process is the main process in local.\n\n        Returns\n        -------\n        bool\n        \"\"\"\n\n        return self.local_rank == 0\n\n    @property\n    @ensure_dir\n    def dir(self) -&gt; str:\n        r\"\"\"\n        Directory of the experiment.\n\n        Returns\n        -------\n        str\n        \"\"\"\n\n        return os.path.join(self.experiments_root, self.id)\n\n    @property\n    def log_path(self) -&gt; str:\n        r\"\"\"\n        Path of log file.\n\n        Returns\n        -------\n        str\n        \"\"\"\n\n        return os.path.join(self.dir, \"run.log\")\n\n    @property\n    @ensure_dir\n    def checkpoint_dir(self) -&gt; str:\n        r\"\"\"\n        Directory of checkpoints.\n\n        Returns\n        -------\n        str\n        \"\"\"\n\n        return os.path.join(self.dir, self.checkpoint_dir_name)\n\n    @catch\n    def save(self, obj: Any, f: File, main_process_only: bool = True) -&gt; File:  # pylint: disable=C0103\n        r\"\"\"\n        Save object to a path or file.\n\n        Returns\n        -------\n        File\n        \"\"\"\n\n        raise NotImplementedError\n\n    @staticmethod\n    def load(f: File, *args, **kwargs) -&gt; Any:  # pylint: disable=C0103\n        r\"\"\"\n        Load object from a path or file.\n\n        Returns\n        -------\n        Any\n        \"\"\"\n\n        raise NotImplementedError\n\n    def dict(self, cls: Callable = dict, only_json_serializable: bool = True) -&gt; Mapping:\n        r\"\"\"\n        Convert config to Mapping.\n\n        Note that all non-json-serializable objects will be removed.\n\n        Parameters\n        ----------\n        cls : Callable = dict\n            Class to convert to.\n        only_json_serializable : bool = True\n            If only json serializable objects should be kept.\n\n        Returns\n        -------\n        Mapping\n        \"\"\"\n\n        # pylint: disable=C0103\n\n        ret = cls()\n        for k, v in self.__dict__.items():\n            if isinstance(v, FlatDict):\n                v = v.dict(cls)\n            if not only_json_serializable or is_json_serializable(v):\n                ret[k] = v\n        return ret\n\n    @catch\n    def json(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=W1113\n        r\"\"\"\n        Dump Runner to json file.\n        \"\"\"\n\n        if main_process_only and self.is_main_process or not main_process_only:\n            with FlatDict.open(file, mode=\"w\") as fp:  # pylint: disable=C0103\n                fp.write(self.jsons(*args, **kwargs))\n\n    @classmethod\n    def from_json(cls, file: File, *args, **kwargs) -&gt; RunnerBase:\n        r\"\"\"\n        Construct Runner from json file.\n\n        This function calls `self.from_jsons()` to construct object from json string.\n        You may overwrite `from_jsons` in case something is not json serializable.\n\n        Returns\n        -------\n        RunnerBase\n        \"\"\"\n\n        with FlatDict.open(file) as fp:  # pylint: disable=C0103\n            return cls.from_jsons(fp.read(), *args, **kwargs)\n\n    def jsons(self, *args, **kwargs) -&gt; str:\n        r\"\"\"\n        Dump Runner to json string.\n\n        Returns\n        -------\n        json: str\n        \"\"\"\n\n        if \"cls\" not in kwargs:\n            kwargs[\"cls\"] = JsonEncoder\n        return json_dumps(self.dict(), *args, **kwargs)\n\n    @classmethod\n    def from_jsons(cls, string: str, *args, **kwargs) -&gt; RunnerBase:\n        r\"\"\"\n        Construct Runner from json string.\n\n        Returns\n        -------\n        RunnerBase\n        \"\"\"\n\n        return cls(**Config.from_jsons(string, *args, **kwargs))\n\n    @catch\n    def yaml(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=W1113\n        r\"\"\"\n        Dump Runner to yaml file.\n        \"\"\"\n\n        if main_process_only and self.is_main_process or not main_process_only:\n            with FlatDict.open(file, mode=\"w\") as fp:  # pylint: disable=C0103\n                self.yamls(fp, *args, **kwargs)\n\n    @classmethod\n    def from_yaml(cls, file: File, *args, **kwargs) -&gt; RunnerBase:\n        r\"\"\"\n        Construct Runner from yaml file.\n\n        This function calls `self.from_yamls()` to construct object from yaml string.\n        You may overwrite `from_yamls` in case something is not yaml serializable.\n\n        Returns\n        -------\n        RunnerBase\n        \"\"\"\n\n        with FlatDict.open(file) as fp:  # pylint: disable=C0103\n            return cls.from_yamls(fp.read(), *args, **kwargs)\n\n    def yamls(self, *args, **kwargs) -&gt; str:\n        r\"\"\"\n        Dump Runner to yaml string.\n\n        Returns\n        -------\n        yaml: str\n        \"\"\"\n\n        if \"Dumper\" not in kwargs:\n            kwargs[\"Dumper\"] = YamlDumper\n        return yaml_dump(self.dict(), *args, **kwargs)  # type: ignore\n\n    @classmethod\n    def from_yamls(cls, string: str, *args, **kwargs) -&gt; RunnerBase:\n        r\"\"\"\n        Construct Runner from yaml string.\n\n        Returns\n        -------\n        RunnerBase\n        \"\"\"\n\n        return cls(**Config.from_yamls(string, *args, **kwargs))\n\n    def __getattr__(self, name) -&gt; Any:\n        if \"accelerator\" not in self:\n            raise RuntimeError(f\"{self.__class__.__name__} is not properly initialised\")\n        if hasattr(self.accelerator, name):\n            return getattr(self.accelerator, name)\n        raise AttributeError(f\"{self.__class__.__name__} does not contain {name}\")\n\n    def __contains__(self, name) -&gt; bool:\n        return name in self.__dict__\n\n    def __repr__(self):\n        lines = []\n        for key, value in self.__dict__.items():\n            value_str = repr(value)\n            value_str = self._add_indent(value_str)\n            lines.append(\"(\" + key + \"): \" + value_str)\n\n        main_str = self.__class__.__name__ + \"(\"\n        if lines:\n            main_str += \"\\n  \" + \"\\n  \".join(lines) + \"\\n\"\n\n        main_str += \")\"\n        return main_str\n\n    def _add_indent(self, text):\n        lines = text.split(\"\\n\")\n        # don't do anything for single-line stuff\n        if len(lines) == 1:\n            return text\n        first = lines.pop(0)\n        # add 2 spaces to each line but the first\n        lines = [(2 * \" \") + line for line in lines]\n        lines = \"\\n\".join(lines)\n        lines = first + \"\\n\" + lines\n        return lines\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase--see-also"},{"title":"<code>progress()</code>  <code>property</code>","text":"<p>Training Progress.</p> <p>Returns:</p>    Type Description      <code>float</code>      <p>Raises:</p>    Type Description      <code>RuntimeError</code>  <p>If no terminal is defined.</p>     Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef progress(self) -&gt; float:\n    r\"\"\"\n    Training Progress.\n\n    Returns\n    -------\n    float\n\n    Raises\n    ------\n    RuntimeError\n        If no terminal is defined.\n    \"\"\"\n\n    if hasattr(self, \"iter_end\"):\n        return self.iters / self.iter_end\n    if hasattr(self, \"step_end\"):\n        return self.steps / self.step_end\n    if hasattr(self, \"epoch_end\"):\n        return self.epochs / self.epoch_end\n    raise RuntimeError(\"DanLing cannot determine progress since no terminal is defined.\")\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.progress"},{"title":"<code>batch_size_equivalent()</code>  <code>property</code>","text":"<p>Actual batch size.</p> <p><code>batch_size</code> * <code>world_size</code> * <code>accum_steps</code></p>  Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef batch_size_equivalent(self) -&gt; int:\n    r\"\"\"\n    Actual batch size.\n\n    `batch_size` * `world_size` * `accum_steps`\n    \"\"\"\n\n    return self.batch_size * self.world_size * getattr(self, \"accum_steps\", 1)\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.batch_size_equivalent"},{"title":"<code>best_fn(scores, fn=max)</code>  <code>staticmethod</code>","text":"<p>Function to determine the best score from a list of scores.</p> <p>Subclass can override this method to accommodate needs, such as <code>min(scores)</code>.</p> <p>Parameters:</p>    Name Type Description Default     <code>scores</code>  <code>Sequence[float]</code>  <p>List of scores.</p>  required    <code>fn</code>  <code>Callable</code>  <p>Function to determine the best score from a list of scores.</p>  <code>max</code>     <p>Returns:</p>    Name Type Description     <code>best_score</code>  <code>float</code>  <p>The best score from a list of scores.</p>     Source code in <code>danling/runner/bases.py</code> Python<pre><code>@staticmethod\ndef best_fn(scores: Sequence[float], fn: Callable = max) -&gt; float:  # pylint: disable=C0103\n    r\"\"\"\n    Function to determine the best score from a list of scores.\n\n    Subclass can override this method to accommodate needs, such as `min(scores)`.\n\n    Parameters\n    ----------\n    scores: Sequence[float]\n        List of scores.\n    fn: Callable = max\n        Function to determine the best score from a list of scores.\n\n    Returns\n    -------\n    best_score: float\n        The best score from a list of scores.\n    \"\"\"\n\n    return fn(scores)\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.best_fn"},{"title":"<code>latest_result()</code>  <code>property</code>","text":"<p>Latest result.</p> <p>Returns:</p>    Type Description      <code>Optional[NestedDict]</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef latest_result(self) -&gt; Optional[NestedDict]:\n    r\"\"\"\n    Latest result.\n\n    Returns\n    -------\n    Optional[NestedDict]\n    \"\"\"\n\n    return self.results[-1] if self.results else None\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.latest_result"},{"title":"<code>best_result()</code>  <code>property</code>","text":"<p>Best result.</p> <p>Returns:</p>    Type Description      <code>Optional[NestedDict]</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef best_result(self) -&gt; Optional[NestedDict]:\n    r\"\"\"\n    Best result.\n\n    Returns\n    -------\n    Optional[NestedDict]\n    \"\"\"\n\n    return self.results[-1 - self.scores[::-1].index(self.best_score)] if self.results else None  # type: ignore\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.best_result"},{"title":"<code>scores()</code>  <code>property</code>","text":"<p>All scores.</p> <p>Scores are extracted from results by <code>index_set</code> and <code>runner.index</code>, following <code>[r[index_set][self.index] for r in self.results]</code>.</p> <p>By default, <code>index_set</code> points to <code>self.index_set</code> and is set to <code>val</code>, if <code>self.index_set</code> is not set, it will be the last key of the last result.</p> <p>Scores are considered as the index of the performance of the model. It is useful to determine the best model and the best hyper-parameters.</p> <p>Returns:</p>    Type Description      <code>List[float]</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef scores(self) -&gt; List[float]:\n    r\"\"\"\n    All scores.\n\n    Scores are extracted from results by `index_set` and `runner.index`,\n    following `[r[index_set][self.index] for r in self.results]`.\n\n    By default, `index_set` points to `self.index_set` and is set to `val`,\n    if `self.index_set` is not set, it will be the last key of the last result.\n\n    Scores are considered as the index of the performance of the model.\n    It is useful to determine the best model and the best hyper-parameters.\n\n    Returns\n    -------\n    List[float]\n    \"\"\"\n\n    if not self.results:\n        return []\n    index_set = self.index_set or next(reversed(self.results[-1]))\n    return [r[index_set][self.index] for r in self.results]\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.scores"},{"title":"<code>latest_score()</code>  <code>property</code>","text":"<p>Latest score.</p> <p>Returns:</p>    Type Description      <code>Optional[float]</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef latest_score(self) -&gt; Optional[float]:\n    r\"\"\"\n    Latest score.\n\n    Returns\n    -------\n    Optional[float]\n    \"\"\"\n\n    return self.scores[-1] if self.results else None\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.latest_score"},{"title":"<code>best_score()</code>  <code>property</code>","text":"<p>Best score.</p> <p>Returns:</p>    Type Description      <code>Optional[float]</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef best_score(self) -&gt; Optional[float]:\n    r\"\"\"\n    Best score.\n\n    Returns\n    -------\n    Optional[float]\n    \"\"\"\n\n    return self.best_fn(self.scores) if self.results else None\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.best_score"},{"title":"<code>is_best()</code>  <code>property</code>","text":"<p>If current epoch is the best epoch.</p> <p>Returns:</p>    Type Description      <code>bool</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef is_best(self) -&gt; bool:\n    r\"\"\"\n    If current epoch is the best epoch.\n\n    Returns\n    -------\n    bool\n    \"\"\"\n\n    return abs(self.latest_score - self.best_score) &lt; 1e-7\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.is_best"},{"title":"<code>world_size()</code>  <code>property</code>","text":"<p>Number of Processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef world_size(self) -&gt; int:\n    r\"\"\"\n    Number of Processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return 1\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.world_size"},{"title":"<code>rank()</code>  <code>property</code>","text":"<p>Process index in all processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef rank(self) -&gt; int:\n    r\"\"\"\n    Process index in all processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return 0\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.rank"},{"title":"<code>local_rank()</code>  <code>property</code>","text":"<p>Process index in local processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef local_rank(self) -&gt; int:\n    r\"\"\"\n    Process index in local processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return 0\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.local_rank"},{"title":"<code>distributed()</code>  <code>property</code>","text":"<p>If runner is running in distributed mode.</p>  Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef distributed(self) -&gt; bool:\n    r\"\"\"\n    If runner is running in distributed mode.\n    \"\"\"\n\n    return self.world_size &gt; 1\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.distributed"},{"title":"<code>is_main_process()</code>  <code>property</code>","text":"<p>If current process is the main process.</p> <p>Returns:</p>    Type Description      <code>bool</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef is_main_process(self) -&gt; bool:\n    r\"\"\"\n    If current process is the main process.\n\n    Returns\n    -------\n    bool\n    \"\"\"\n\n    return self.rank == 0\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.is_main_process"},{"title":"<code>is_local_main_process()</code>  <code>property</code>","text":"<p>If current process is the main process in local.</p> <p>Returns:</p>    Type Description      <code>bool</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef is_local_main_process(self) -&gt; bool:\n    r\"\"\"\n    If current process is the main process in local.\n\n    Returns\n    -------\n    bool\n    \"\"\"\n\n    return self.local_rank == 0\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.is_local_main_process"},{"title":"<code>dir()</code>  <code>property</code>","text":"<p>Directory of the experiment.</p> <p>Returns:</p>    Type Description      <code>str</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\n@ensure_dir\ndef dir(self) -&gt; str:\n    r\"\"\"\n    Directory of the experiment.\n\n    Returns\n    -------\n    str\n    \"\"\"\n\n    return os.path.join(self.experiments_root, self.id)\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.dir"},{"title":"<code>log_path()</code>  <code>property</code>","text":"<p>Path of log file.</p> <p>Returns:</p>    Type Description      <code>str</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef log_path(self) -&gt; str:\n    r\"\"\"\n    Path of log file.\n\n    Returns\n    -------\n    str\n    \"\"\"\n\n    return os.path.join(self.dir, \"run.log\")\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.log_path"},{"title":"<code>checkpoint_dir()</code>  <code>property</code>","text":"<p>Directory of checkpoints.</p> <p>Returns:</p>    Type Description      <code>str</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\n@ensure_dir\ndef checkpoint_dir(self) -&gt; str:\n    r\"\"\"\n    Directory of checkpoints.\n\n    Returns\n    -------\n    str\n    \"\"\"\n\n    return os.path.join(self.dir, self.checkpoint_dir_name)\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.checkpoint_dir"},{"title":"<code>save(obj, f, main_process_only=True)</code>","text":"<p>Save object to a path or file.</p> <p>Returns:</p>    Type Description      <code>File</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@catch\ndef save(self, obj: Any, f: File, main_process_only: bool = True) -&gt; File:  # pylint: disable=C0103\n    r\"\"\"\n    Save object to a path or file.\n\n    Returns\n    -------\n    File\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.save"},{"title":"<code>load(f, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Load object from a path or file.</p> <p>Returns:</p>    Type Description      <code>Any</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@staticmethod\ndef load(f: File, *args, **kwargs) -&gt; Any:  # pylint: disable=C0103\n    r\"\"\"\n    Load object from a path or file.\n\n    Returns\n    -------\n    Any\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.load"},{"title":"<code>dict(cls=dict, only_json_serializable=True)</code>","text":"<p>Convert config to Mapping.</p> <p>Note that all non-json-serializable objects will be removed.</p> <p>Parameters:</p>    Name Type Description Default     <code>cls</code>  <code>Callable</code>  <p>Class to convert to.</p>  <code>dict</code>    <code>only_json_serializable</code>  <code>bool</code>  <p>If only json serializable objects should be kept.</p>  <code>True</code>     <p>Returns:</p>    Type Description      <code>Mapping</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>def dict(self, cls: Callable = dict, only_json_serializable: bool = True) -&gt; Mapping:\n    r\"\"\"\n    Convert config to Mapping.\n\n    Note that all non-json-serializable objects will be removed.\n\n    Parameters\n    ----------\n    cls : Callable = dict\n        Class to convert to.\n    only_json_serializable : bool = True\n        If only json serializable objects should be kept.\n\n    Returns\n    -------\n    Mapping\n    \"\"\"\n\n    # pylint: disable=C0103\n\n    ret = cls()\n    for k, v in self.__dict__.items():\n        if isinstance(v, FlatDict):\n            v = v.dict(cls)\n        if not only_json_serializable or is_json_serializable(v):\n            ret[k] = v\n    return ret\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.dict"},{"title":"<code>json(file, main_process_only=True, *args, **kwargs)</code>","text":"<p>Dump Runner to json file.</p>  Source code in <code>danling/runner/bases.py</code> Python<pre><code>@catch\ndef json(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=W1113\n    r\"\"\"\n    Dump Runner to json file.\n    \"\"\"\n\n    if main_process_only and self.is_main_process or not main_process_only:\n        with FlatDict.open(file, mode=\"w\") as fp:  # pylint: disable=C0103\n            fp.write(self.jsons(*args, **kwargs))\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.json"},{"title":"<code>from_json(file, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from json file.</p> <p>This function calls <code>self.from_jsons()</code> to construct object from json string. You may overwrite <code>from_jsons</code> in case something is not json serializable.</p> <p>Returns:</p>    Type Description      <code>RunnerBase</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@classmethod\ndef from_json(cls, file: File, *args, **kwargs) -&gt; RunnerBase:\n    r\"\"\"\n    Construct Runner from json file.\n\n    This function calls `self.from_jsons()` to construct object from json string.\n    You may overwrite `from_jsons` in case something is not json serializable.\n\n    Returns\n    -------\n    RunnerBase\n    \"\"\"\n\n    with FlatDict.open(file) as fp:  # pylint: disable=C0103\n        return cls.from_jsons(fp.read(), *args, **kwargs)\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.from_json"},{"title":"<code>jsons(*args, **kwargs)</code>","text":"<p>Dump Runner to json string.</p> <p>Returns:</p>    Name Type Description     <code>json</code>  <code>str</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>def jsons(self, *args, **kwargs) -&gt; str:\n    r\"\"\"\n    Dump Runner to json string.\n\n    Returns\n    -------\n    json: str\n    \"\"\"\n\n    if \"cls\" not in kwargs:\n        kwargs[\"cls\"] = JsonEncoder\n    return json_dumps(self.dict(), *args, **kwargs)\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.jsons"},{"title":"<code>from_jsons(string, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from json string.</p> <p>Returns:</p>    Type Description      <code>RunnerBase</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@classmethod\ndef from_jsons(cls, string: str, *args, **kwargs) -&gt; RunnerBase:\n    r\"\"\"\n    Construct Runner from json string.\n\n    Returns\n    -------\n    RunnerBase\n    \"\"\"\n\n    return cls(**Config.from_jsons(string, *args, **kwargs))\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.from_jsons"},{"title":"<code>yaml(file, main_process_only=True, *args, **kwargs)</code>","text":"<p>Dump Runner to yaml file.</p>  Source code in <code>danling/runner/bases.py</code> Python<pre><code>@catch\ndef yaml(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=W1113\n    r\"\"\"\n    Dump Runner to yaml file.\n    \"\"\"\n\n    if main_process_only and self.is_main_process or not main_process_only:\n        with FlatDict.open(file, mode=\"w\") as fp:  # pylint: disable=C0103\n            self.yamls(fp, *args, **kwargs)\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.yaml"},{"title":"<code>from_yaml(file, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from yaml file.</p> <p>This function calls <code>self.from_yamls()</code> to construct object from yaml string. You may overwrite <code>from_yamls</code> in case something is not yaml serializable.</p> <p>Returns:</p>    Type Description      <code>RunnerBase</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@classmethod\ndef from_yaml(cls, file: File, *args, **kwargs) -&gt; RunnerBase:\n    r\"\"\"\n    Construct Runner from yaml file.\n\n    This function calls `self.from_yamls()` to construct object from yaml string.\n    You may overwrite `from_yamls` in case something is not yaml serializable.\n\n    Returns\n    -------\n    RunnerBase\n    \"\"\"\n\n    with FlatDict.open(file) as fp:  # pylint: disable=C0103\n        return cls.from_yamls(fp.read(), *args, **kwargs)\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.from_yaml"},{"title":"<code>yamls(*args, **kwargs)</code>","text":"<p>Dump Runner to yaml string.</p> <p>Returns:</p>    Name Type Description     <code>yaml</code>  <code>str</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>def yamls(self, *args, **kwargs) -&gt; str:\n    r\"\"\"\n    Dump Runner to yaml string.\n\n    Returns\n    -------\n    yaml: str\n    \"\"\"\n\n    if \"Dumper\" not in kwargs:\n        kwargs[\"Dumper\"] = YamlDumper\n    return yaml_dump(self.dict(), *args, **kwargs)  # type: ignore\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.yamls"},{"title":"<code>from_yamls(string, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from yaml string.</p> <p>Returns:</p>    Type Description      <code>RunnerBase</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@classmethod\ndef from_yamls(cls, string: str, *args, **kwargs) -&gt; RunnerBase:\n    r\"\"\"\n    Construct Runner from yaml string.\n\n    Returns\n    -------\n    RunnerBase\n    \"\"\"\n\n    return cls(**Config.from_yamls(string, *args, **kwargs))\n</code></pre>","location":"runner/bases/#danling.runner.bases.RunnerBase.from_yamls"},{"title":"TorchRunner","text":"<p>         Bases: <code>BaseRunner</code></p> <p>Set up everything for running a job.</p> <p>Attributes:</p>    Name Type Description     <code>accelerator</code>  <code>Accelerator</code>     <code>accelerate</code>  <code>Dict[str, Any] = {}</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>class TorchRunner(BaseRunner):\n    r\"\"\"\n    Set up everything for running a job.\n\n    Attributes\n    ----------\n    accelerator: Accelerator\n    accelerate: Dict[str, Any] = {}\n    \"\"\"\n\n    # pylint: disable=R0902\n\n    accelerator: Accelerator\n    accelerate: Dict[str, Any] = {}\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        self.accelerator = Accelerator(**self.accelerate)\n        super().__init__(*args, **kwargs)\n\n    @on_main_process\n    def init_tensorboard(self, *args, **kwargs) -&gt; None:\n        r\"\"\"\n        Set up Tensoraoard SummaryWriter.\n        \"\"\"\n        from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n        if \"log_dir\" not in kwargs:\n            kwargs[\"log_dir\"] = self.dir\n\n        self.writer = SummaryWriter(*args, **kwargs)\n\n    def set_seed(self, bias: Optional[int] = None) -&gt; None:\n        r\"\"\"\n        Set up random seed.\n\n        Parameters\n        ----------\n        bias: Optional[int] = self.rank\n            Make the seed different for each processes.\n            This avoids same data augmentation are applied on every processes.\n            Set to `False` to disable this feature.\n        \"\"\"\n\n        if self.distributed:\n            # TODO: use broadcast_object instead.\n            self.seed = (\n                self.accelerator.gather(torch.tensor(self.seed).cuda()).unsqueeze(0).flatten()[0]  # pylint: disable=E1101\n            )\n        if bias is None:\n            bias = self.rank\n        seed = self.seed + bias if bias else self.seed\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        np.random.seed(seed)\n        random.seed(seed)\n\n    def set_deterministic(self) -&gt; None:\n        r\"\"\"\n        Set up deterministic.\n        \"\"\"\n\n        cudnn.benchmark = False\n        cudnn.deterministic = True\n        if torch.__version__ &gt;= \"1.8.0\":\n            torch.use_deterministic_algorithms(True)\n\n    def init_distributed(self) -&gt; None:\n        r\"\"\"\n        Set up distributed training.\n\n        Initialise process group and set up DDP variables.\n\n        .. deprecated:: 0.1.0\n            `init_distributed` is deprecated in favor of `Accelerator()`.\n        \"\"\"\n\n        # pylint: disable=W0201\n\n        dist.init_process_group(backend=\"nccl\")\n        self.rank = dist.get_rank()\n        self.world_size = dist.get_world_size()\n        self.local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n        torch.cuda.set_device(self.local_rank)\n        self.is_main_process = self.rank == 0\n        self.is_local_main_process = self.local_rank == 0\n\n    @catch\n    def save(self, obj: Any, f: File, main_process_only: bool = True) -&gt; File:  # pylint: disable=C0103\n        r\"\"\"\n        Save object to a path or file.\n        Returns\n        -------\n        File\n        \"\"\"\n\n        if main_process_only and self.is_main_process or not on_main_process:\n            self.accelerator.save(obj, f)\n        return f\n\n    @staticmethod\n    def load(f: File, *args, **kwargs) -&gt; Any:  # pylint: disable=C0103\n        r\"\"\"\n        Load object from a path or file.\n        Returns\n        -------\n        Any\n        \"\"\"\n\n        return torch.load(f, *args, **kwargs)  # type: ignore\n\n    @property\n    def world_size(self) -&gt; int:\n        r\"\"\"\n        Number of Processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return self.accelerator.num_processes\n\n    @property\n    def rank(self) -&gt; int:\n        r\"\"\"\n        Process index in all processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return self.accelerator.process_index\n\n    @property\n    def local_rank(self) -&gt; int:\n        r\"\"\"\n        Process index in local processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return self.accelerator.local_process_index\n</code></pre>","location":"runner/torch_runner/"},{"title":"<code>init_tensorboard(*args, **kwargs)</code>","text":"<p>Set up Tensoraoard SummaryWriter.</p>  Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@on_main_process\ndef init_tensorboard(self, *args, **kwargs) -&gt; None:\n    r\"\"\"\n    Set up Tensoraoard SummaryWriter.\n    \"\"\"\n    from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n    if \"log_dir\" not in kwargs:\n        kwargs[\"log_dir\"] = self.dir\n\n    self.writer = SummaryWriter(*args, **kwargs)\n</code></pre>","location":"runner/torch_runner/#danling.runner.torch_runner.TorchRunner.init_tensorboard"},{"title":"<code>save(obj, f, main_process_only=True)</code>","text":"<p>Save object to a path or file.</p> <p>Returns:</p>    Type Description      <code>File</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@catch\ndef save(self, obj: Any, f: File, main_process_only: bool = True) -&gt; File:  # pylint: disable=C0103\n    r\"\"\"\n    Save object to a path or file.\n    Returns\n    -------\n    File\n    \"\"\"\n\n    if main_process_only and self.is_main_process or not on_main_process:\n        self.accelerator.save(obj, f)\n    return f\n</code></pre>","location":"runner/torch_runner/#danling.runner.torch_runner.TorchRunner.save"},{"title":"<code>load(f, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Load object from a path or file.</p> <p>Returns:</p>    Type Description      <code>Any</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@staticmethod\ndef load(f: File, *args, **kwargs) -&gt; Any:  # pylint: disable=C0103\n    r\"\"\"\n    Load object from a path or file.\n    Returns\n    -------\n    Any\n    \"\"\"\n\n    return torch.load(f, *args, **kwargs)  # type: ignore\n</code></pre>","location":"runner/torch_runner/#danling.runner.torch_runner.TorchRunner.load"},{"title":"<code>world_size()</code>  <code>property</code>","text":"<p>Number of Processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@property\ndef world_size(self) -&gt; int:\n    r\"\"\"\n    Number of Processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return self.accelerator.num_processes\n</code></pre>","location":"runner/torch_runner/#danling.runner.torch_runner.TorchRunner.world_size"},{"title":"<code>rank()</code>  <code>property</code>","text":"<p>Process index in all processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@property\ndef rank(self) -&gt; int:\n    r\"\"\"\n    Process index in all processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return self.accelerator.process_index\n</code></pre>","location":"runner/torch_runner/#danling.runner.torch_runner.TorchRunner.rank"},{"title":"<code>local_rank()</code>  <code>property</code>","text":"<p>Process index in local processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@property\ndef local_rank(self) -&gt; int:\n    r\"\"\"\n    Process index in local processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return self.accelerator.local_process_index\n</code></pre>","location":"runner/torch_runner/#danling.runner.torch_runner.TorchRunner.local_rank"},{"title":"<code>set_deterministic()</code>","text":"<p>Set up deterministic.</p>  Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def set_deterministic(self) -&gt; None:\n    r\"\"\"\n    Set up deterministic.\n    \"\"\"\n\n    cudnn.benchmark = False\n    cudnn.deterministic = True\n    if torch.__version__ &gt;= \"1.8.0\":\n        torch.use_deterministic_algorithms(True)\n</code></pre>","location":"runner/torch_runner/#danling.runner.torch_runner.TorchRunner.set_deterministic"},{"title":"<code>set_seed(bias=None)</code>","text":"<p>Set up random seed.</p> <p>Parameters:</p>    Name Type Description Default     <code>bias</code>  <code>Optional[int]</code>  <p>Make the seed different for each processes. This avoids same data augmentation are applied on every processes. Set to <code>False</code> to disable this feature.</p>  <code>None</code>      Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def set_seed(self, bias: Optional[int] = None) -&gt; None:\n    r\"\"\"\n    Set up random seed.\n\n    Parameters\n    ----------\n    bias: Optional[int] = self.rank\n        Make the seed different for each processes.\n        This avoids same data augmentation are applied on every processes.\n        Set to `False` to disable this feature.\n    \"\"\"\n\n    if self.distributed:\n        # TODO: use broadcast_object instead.\n        self.seed = (\n            self.accelerator.gather(torch.tensor(self.seed).cuda()).unsqueeze(0).flatten()[0]  # pylint: disable=E1101\n        )\n    if bias is None:\n        bias = self.rank\n    seed = self.seed + bias if bias else self.seed\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n</code></pre>","location":"runner/torch_runner/#danling.runner.torch_runner.TorchRunner.set_seed"},{"title":"<code>init_tensorboard(*args, **kwargs)</code>","text":"<p>Set up Tensoraoard SummaryWriter.</p>  Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@on_main_process\ndef init_tensorboard(self, *args, **kwargs) -&gt; None:\n    r\"\"\"\n    Set up Tensoraoard SummaryWriter.\n    \"\"\"\n    from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n    if \"log_dir\" not in kwargs:\n        kwargs[\"log_dir\"] = self.dir\n\n    self.writer = SummaryWriter(*args, **kwargs)\n</code></pre>","location":"runner/torch_runner/#danling.runner.torch_runner.TorchRunner.init_tensorboard"},{"title":"Runner","text":"<p>The Runner of DanLing sets up the basic environment for running neural networks.</p>","location":"tensors/"},{"title":"Components","text":"<p>For readability and maintainability, there are three levels of Runner:</p>","location":"tensors/#components"},{"title":"<code>RunnerBase</code>","text":"<p><code>RunnerBase</code> gives you a basic instinct on what attributes and properties are provided by the Runner.</p> <p>It works in an AbstractBaseClass manner and should neither be used directly nor be inherited from.</p>","location":"tensors/#runnerbase"},{"title":"<code>BaseRunner</code>","text":"<p><code>BaseRunner</code> contains core methods of general basic functionality, such as <code>init_logging</code>, <code>append_result</code>, <code>print_result</code>.</p>","location":"tensors/#baserunner"},{"title":"<code>Runner</code>.","text":"<p><code>Runner</code> should only contain platform-specific features. Currently, only <code>TorchRunner</code> is supported.</p>","location":"tensors/#runner_1"},{"title":"Philosophy","text":"<p>DanLing Runner is designed for a 3.5-level experiments management system: Project, Group, Experiment, and, Run.</p>","location":"tensors/#philosophy"},{"title":"Project","text":"<p>Project corresponds to your project.</p> <p>Generally speaking, there should be only one project for each repository.</p>","location":"tensors/#project"},{"title":"Group","text":"<p>A group groups multiple experiments with similar characteristics.</p> <p>For example, if you run multiple experiments on learning rate, you may want to group them into a group.</p> <p>Note that Group is a virtual level (which is why it only counts 0.5) and does not corresponds to anything.</p>","location":"tensors/#group"},{"title":"Experiment","text":"<p>Experiment is the basic unit of experiments.</p> <p>Experiment usually corresponds to a commit, which means the code should be consistent across the experiment.</p>","location":"tensors/#experiment"},{"title":"Run","text":"<p>Run is the basic unit of running.</p> <p>Run corresponds to a single run of an experiment, each run may have different hyperparameters.</p>","location":"tensors/#run"},{"title":"Attributes &amp; Properties","text":"","location":"tensors/#attributes-properties"},{"title":"General","text":"<ul> <li>id</li> <li>name</li> <li>seed</li> <li>deterministic</li> </ul>","location":"tensors/#general"},{"title":"Progress","text":"<ul> <li>iters</li> <li>steps</li> <li>epochs</li> <li>iter_end</li> <li>step_end</li> <li>epoch_end</li> <li>progress</li> </ul> <p>Note that generally you should only use one of <code>iter</code>, <code>step</code>, <code>epoch</code> to indicate the progress.</p>","location":"tensors/#progress"},{"title":"Model","text":"<ul> <li>model</li> <li>criterion</li> <li>optimizer</li> <li>scheduler</li> </ul>","location":"tensors/#model"},{"title":"Data","text":"<ul> <li>datasets</li> <li>datasamplers</li> <li>dataloaders</li> <li>batch_size</li> <li>batch_size_equivalent</li> </ul> <p><code>datasets</code>, <code>datasamplers</code>, <code>dataloaders</code> should be a dict with the same keys. Their keys should be <code>split</code> (e.g. <code>train</code>, <code>val</code>, <code>test</code>).</p>","location":"tensors/#data"},{"title":"Results","text":"<ul> <li>results</li> <li>latest_result</li> <li>best_result</li> <li>scores</li> <li>latest_score</li> <li>best_score</li> <li>index_set</li> <li>index</li> <li>is_best</li> </ul> <p><code>results</code> should be a list of <code>result</code>. <code>result</code> should be a dict with the same <code>split</code> as keys, like <code>dataloaders</code>. A typical <code>result</code> might look like this: Python<pre><code>{\n    \"train\": {\n        \"loss\": 0.1,\n        \"accuracy\": 0.9,\n    },\n    \"val\": {\n        \"loss\": 0.2,\n        \"accuracy\": 0.8,\n    },\n    \"test\": {\n        \"loss\": 0.3,\n        \"accuracy\": 0.7,\n    },\n}\n</code></pre></p> <p><code>scores</code> are usually a list of <code>float</code>, and are dynamically extracted from <code>results</code> by <code>index_set</code> and <code>index</code>. If <code>index_set = \"val\"</code>, <code>index = \"accuracy\"</code>, then <code>scores = 0.9</code>.</p>","location":"tensors/#results"},{"title":"DDP","text":"<ul> <li>world_size</li> <li>rank</li> <li>local_rank</li> <li>distributed</li> <li>is_main_process</li> <li>is_local_main_process</li> </ul>","location":"tensors/#ddp"},{"title":"IO","text":"<ul> <li>experiments_root</li> <li>dir</li> <li>checkpoint_dir</li> <li>log_path</li> <li>checkpoint_dir_name</li> </ul> <p><code>experiments_root</code> is the root directory of all experiments, and should be consistent across the Project.</p> <p><code>dir</code> is the directory of a certain Run.</p> <p>There is no attributes/properties for Group and Experiment.</p> <p><code>checkpoint_dir_name</code> is relative to <code>dir</code>, and is passed to generate <code>checkpoint_dir</code> (<code>checkpoint_dir = os.path.join(dir, checkpoint_dir_name)</code>). In practice, <code>checkpoint_dir_name</code> is rarely called.</p>","location":"tensors/#io"},{"title":"logging","text":"<ul> <li>log</li> <li>logger</li> <li>tensorboard</li> <li>writer</li> </ul>","location":"tensors/#logging"},{"title":"NestedTensor","text":"<p>Wrap a sequence of tensors into a single tensor with a mask.</p> <p>In sequence to sequence tasks, elements of a batch are usually not of the same length. This made it tricky to use a single tensor to represent a batch of sequences.</p> <p>NestedTensor allows to store a sequence of tensors of different lengths in a single object. It also provides a mask that can be used to retrieve the original sequence of tensors.</p> <p>Attributes:</p>    Name Type Description     <code>storage</code>  <code>Sequence[torch.Tensor]</code>  <p>The sequence of tensors.</p>   <code>batch_first</code>  <code>bool = True</code>  <p>Whether the first dimension of the tensors is the batch dimension.</p> <p>If <code>True</code>, the first dimension is the batch dimension, i.e., <code>B, N, *</code>.</p> <p>If <code>False</code>, the first dimension is the sequence dimension, i.e., <code>N, B, *</code></p>    <p>Parameters:</p>    Name Type Description Default     <code>tensors</code>  <code>Sequence[Tensor]</code>    required    <code>batch_first</code>  <code>bool</code>    <code>True</code>     <p>Raises:</p>    Type Description      <code>ValueError</code>  <p>If <code>tensors</code> is not a sequence.</p> <p>If <code>tensors</code> is empty.</p>","location":"tensors/nested_tensor/"},{"title":"Notes","text":"<p>We have rewritten the <code>__getattr__</code> function to support as much native tensor operations as possible. However, not all operations are tested.</p> <p>Please file an issue if you find any bugs.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.device\ndevice(type='cpu')\n&gt;&gt;&gt; nested_tensor.dtype\ntorch.int64\n&gt;&gt;&gt; nested_tensor.tensor\ntensor([[1, 2, 3],\n        [4, 5, 0]])\n&gt;&gt;&gt; nested_tensor.mask\ntensor([[ True,  True,  True],\n        [ True,  True, False]])\n&gt;&gt;&gt; nested_tensor.to(torch.float).tensor\ntensor([[1., 2., 3.],\n        [4., 5., 0.]])\n&gt;&gt;&gt; nested_tensor.half().tensor\ntensor([[1., 2., 3.],\n        [4., 5., 0.]], dtype=torch.float16)\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>class NestedTensor:\n    r\"\"\"\n    Wrap a sequence of tensors into a single tensor with a mask.\n\n    In sequence to sequence tasks, elements of a batch are usually not of the same length.\n    This made it tricky to use a single tensor to represent a batch of sequences.\n\n    NestedTensor allows to store a sequence of tensors of different lengths in a single object.\n    It also provides a mask that can be used to retrieve the original sequence of tensors.\n\n    Attributes\n    ----------\n    storage: Sequence[torch.Tensor]\n        The sequence of tensors.\n    batch_first: bool = True\n        Whether the first dimension of the tensors is the batch dimension.\n\n        If `True`, the first dimension is the batch dimension, i.e., `B, N, *`.\n\n        If `False`, the first dimension is the sequence dimension, i.e., `N, B, *`\n\n    Parameters\n    ----------\n    tensors: Sequence[torch.Tensor]\n    batch_first: bool = True\n\n    Raises\n    ------\n    ValueError\n        If `tensors` is not a sequence.\n\n        If `tensors` is empty.\n\n    Notes\n    -----\n    We have rewritten the `__getattr__` function to support as much native tensor operations as possible.\n    However, not all operations are tested.\n\n    Please file an issue if you find any bugs.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.shape\n    torch.Size([2, 3])\n    &gt;&gt;&gt; nested_tensor.device\n    device(type='cpu')\n    &gt;&gt;&gt; nested_tensor.dtype\n    torch.int64\n    &gt;&gt;&gt; nested_tensor.tensor\n    tensor([[1, 2, 3],\n            [4, 5, 0]])\n    &gt;&gt;&gt; nested_tensor.mask\n    tensor([[ True,  True,  True],\n            [ True,  True, False]])\n    &gt;&gt;&gt; nested_tensor.to(torch.float).tensor\n    tensor([[1., 2., 3.],\n            [4., 5., 0.]])\n    &gt;&gt;&gt; nested_tensor.half().tensor\n    tensor([[1., 2., 3.],\n            [4., 5., 0.]], dtype=torch.float16)\n\n    ```\n    \"\"\"\n\n    storage: Sequence[Tensor] = []\n    batch_first: bool = True\n\n    def __init__(self, tensors: Sequence[Tensor], batch_first: bool = True) -&gt; None:\n        if not isinstance(tensors, Sequence):\n            raise ValueError(f\"NestedTensor should be initialised with a Sequence, bug got {type(values)}.\")\n        if len(tensors) == 0:\n            raise ValueError(f\"NestedTensor should be initialised with a non-empty sequence.\")\n        if not isinstance(tensors[0], Tensor):\n            tensors = tuple(torch.tensor(tensor) for tensor in tensors)\n        self.storage = tensors\n        self.batch_first = batch_first\n\n    @property\n    def tensor(self) -&gt; Tensor:\n        r\"\"\"\n        Return a single tensor by padding all the tensors.\n\n        Returns\n        -------\n        torch.Tensor\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.tensor\n        tensor([[1, 2, 3],\n                [4, 5, 0]])\n\n        ```\n        \"\"\"\n\n        return self._tensor(tuple(self.storage), self.batch_first)\n\n    @property\n    def mask(self) -&gt; Tensor:\n        r\"\"\"\n        Padding mask of `tensor`.\n\n        Returns\n        -------\n        torch.Tensor\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.mask\n        tensor([[ True,  True,  True],\n                [ True,  True, False]])\n\n        ```\n        \"\"\"\n\n        return self._mask(tuple(self.storage))\n\n    @property\n    def device(self) -&gt; torch.device:\n        r\"\"\"\n        Device of the NestedTensor.\n\n        Returns\n        -------\n        torch.Tensor\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.device\n        device(type='cpu')\n\n        ```\n        \"\"\"\n\n        return self._device(tuple(self.storage))\n\n    @property\n    def shape(self) -&gt; torch.Size:\n        r\"\"\"\n        Alias for `size`.\n\n        Returns\n        -------\n        torch.Size\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.shape\n        torch.Size([2, 3])\n        &gt;&gt;&gt; nested_tensor.storage.append(torch.tensor([6, 7, 8, 9]))\n        &gt;&gt;&gt; nested_tensor.shape\n        torch.Size([3, 4])\n\n        ```\n        \"\"\"\n\n        return self.size()\n\n    def size(self) -&gt; torch.Size:\n        r\"\"\"\n        Shape of the NestedTensor.\n\n        Returns\n        -------\n        torch.Size\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.size()\n        torch.Size([2, 3])\n        &gt;&gt;&gt; nested_tensor.storage[1] = torch.tensor([4, 5, 6, 7])\n        &gt;&gt;&gt; nested_tensor.size()\n        torch.Size([2, 4])\n\n        ```\n        \"\"\"\n\n        return self._size(tuple(self.storage))\n\n    def __getitem__(self, index) -&gt; Tuple[Tensor, Tensor]:\n        ret = self.storage[index]\n        if isinstance(ret, Tensor):\n            return ret, torch.ones_like(ret)  # pylint: disable=E1101\n        return self.tensor, self.mask\n\n    def __getattr__(self, name) -&gt; Any:\n        if not self.storage:\n            raise ValueError(f\"Unable to get {name} from an empty {self.__class__.__name__}\")\n        ret = [getattr(i, name) for i in self.storage]\n        elem = ret[0]\n        if isinstance(elem, Tensor):\n            return NestedTensor(ret)\n        if callable(elem):\n            return _TensorFuncWrapper(ret)\n        if len(set(ret)) == 1:\n            return elem\n        return ret\n\n    def __len__(self) -&gt; int:\n        return len(self.storage)\n\n    def __setstate__(self, storage) -&gt; None:\n        self.storage = storage\n\n    def __getstate__(self) -&gt; Sequence[Tensor]:\n        return self.storage\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _tensor(storage, batch_first) -&gt; Tensor:\n        return pad_sequence(storage, batch_first=batch_first)\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _mask(storage) -&gt; Tensor:\n        lens = torch.tensor([len(t) for t in storage], device=storage[0].device)\n        return torch.arange(max(lens), device=storage[0].device)[None, :] &lt; lens[:, None]  # pylint: disable=E1101\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _device(storage) -&gt; torch.device:\n        return storage[0].device\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _size(storage) -&gt; torch.Size:\n        return torch.Size(  # pylint: disable=E1101\n            [len(storage), max(t.shape[0] for t in storage), *storage[0].shape[1:]]\n        )\n</code></pre>","location":"tensors/nested_tensor/#danling.tensors.NestedTensor--notes"},{"title":"<code>device()</code>  <code>property</code>","text":"<p>Device of the NestedTensor.</p> <p>Returns:</p>    Type Description      <code>torch.Tensor</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.device\ndevice(type='cpu')\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@property\ndef device(self) -&gt; torch.device:\n    r\"\"\"\n    Device of the NestedTensor.\n\n    Returns\n    -------\n    torch.Tensor\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.device\n    device(type='cpu')\n\n    ```\n    \"\"\"\n\n    return self._device(tuple(self.storage))\n</code></pre>","location":"tensors/nested_tensor/#danling.tensors.nested_tensor.NestedTensor.device"},{"title":"<code>mask()</code>  <code>property</code>","text":"<p>Padding mask of <code>tensor</code>.</p> <p>Returns:</p>    Type Description      <code>torch.Tensor</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.mask\ntensor([[ True,  True,  True],\n        [ True,  True, False]])\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@property\ndef mask(self) -&gt; Tensor:\n    r\"\"\"\n    Padding mask of `tensor`.\n\n    Returns\n    -------\n    torch.Tensor\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.mask\n    tensor([[ True,  True,  True],\n            [ True,  True, False]])\n\n    ```\n    \"\"\"\n\n    return self._mask(tuple(self.storage))\n</code></pre>","location":"tensors/nested_tensor/#danling.tensors.nested_tensor.NestedTensor.mask"},{"title":"<code>shape()</code>  <code>property</code>","text":"<p>Alias for <code>size</code>.</p> <p>Returns:</p>    Type Description      <code>torch.Size</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.storage.append(torch.tensor([6, 7, 8, 9]))\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([3, 4])\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@property\ndef shape(self) -&gt; torch.Size:\n    r\"\"\"\n    Alias for `size`.\n\n    Returns\n    -------\n    torch.Size\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.shape\n    torch.Size([2, 3])\n    &gt;&gt;&gt; nested_tensor.storage.append(torch.tensor([6, 7, 8, 9]))\n    &gt;&gt;&gt; nested_tensor.shape\n    torch.Size([3, 4])\n\n    ```\n    \"\"\"\n\n    return self.size()\n</code></pre>","location":"tensors/nested_tensor/#danling.tensors.nested_tensor.NestedTensor.shape"},{"title":"<code>size()</code>","text":"<p>Shape of the NestedTensor.</p> <p>Returns:</p>    Type Description      <code>torch.Size</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.size()\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.storage[1] = torch.tensor([4, 5, 6, 7])\n&gt;&gt;&gt; nested_tensor.size()\ntorch.Size([2, 4])\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def size(self) -&gt; torch.Size:\n    r\"\"\"\n    Shape of the NestedTensor.\n\n    Returns\n    -------\n    torch.Size\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.size()\n    torch.Size([2, 3])\n    &gt;&gt;&gt; nested_tensor.storage[1] = torch.tensor([4, 5, 6, 7])\n    &gt;&gt;&gt; nested_tensor.size()\n    torch.Size([2, 4])\n\n    ```\n    \"\"\"\n\n    return self._size(tuple(self.storage))\n</code></pre>","location":"tensors/nested_tensor/#danling.tensors.nested_tensor.NestedTensor.size"},{"title":"<code>tensor()</code>  <code>property</code>","text":"<p>Return a single tensor by padding all the tensors.</p> <p>Returns:</p>    Type Description      <code>torch.Tensor</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.tensor\ntensor([[1, 2, 3],\n        [4, 5, 0]])\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@property\ndef tensor(self) -&gt; Tensor:\n    r\"\"\"\n    Return a single tensor by padding all the tensors.\n\n    Returns\n    -------\n    torch.Tensor\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.tensor\n    tensor([[1, 2, 3],\n            [4, 5, 0]])\n\n    ```\n    \"\"\"\n\n    return self._tensor(tuple(self.storage), self.batch_first)\n</code></pre>","location":"tensors/nested_tensor/#danling.tensors.nested_tensor.NestedTensor.tensor"},{"title":"Decorator","text":"","location":"utils/decorator/"},{"title":"<code>catch(error=Exception, exclude=None, print_args=False)</code>","text":"<p>Decorator to catch <code>error</code> except for <code>exclude</code>. Detailed traceback will be printed to <code>stdout</code>.</p> <p><code>catch</code> is extremely useful for unfatal errors. For example, <code>Runner</code> by de</p> <p>Parameters:</p>    Name Type Description Default     <code>error</code>  <code>Exception</code>    <code>Exception</code>    <code>exclude</code>  <code>Optional[Exception]</code>    <code>None</code>    <code>print_args</code>  <code>bool</code>  <p>Whether to print the arguments passed to the function.</p>  <code>False</code>      Source code in <code>danling/utils/decorator.py</code> Python<pre><code>@flexible_decorator\ndef catch(error: Exception = Exception, exclude: Optional[Exception] = None, print_args: bool = False):\n    \"\"\"\n    Decorator to catch `error` except for `exclude`.\n    Detailed traceback will be printed to `stdout`.\n\n    `catch` is extremely useful for unfatal errors.\n    For example, `Runner` by de\n\n    Parameters\n    ----------\n    error: Exception\n    exclude: Optional[Exception] = None\n    print_args: bool = False\n        Whether to print the arguments passed to the function.\n    \"\"\"\n\n    def decorator(func, error: Exception = Exception, exclude: Optional[Exception] = None, print_args: bool = False):\n        @wraps(func)\n        def wrapper(*args, **kwargs):  # pylint: disable=R1710\n            try:\n                return func(*args, **kwargs)\n            except error as exc:  # pylint: disable=W0703\n                if exclude is not None and isinstance(exc, exclude):\n                    raise exc\n                message = format_exc()\n                message += f\"\\nencoutered when calling {func}\"\n                if print_args:\n                    message += (f\"with args {args} and kwargs {kwargs}\",)\n                print(message, file=stderr, force=True)\n\n        return wrapper\n\n    return lambda func: decorator(func, error, exclude, print_args)\n</code></pre>","location":"utils/decorator/#danling.utils.decorator.catch"},{"title":"<code>ensure_dir(func)</code>","text":"<p>Decorator to ensure a directory property exists.</p>  Source code in <code>danling/utils/decorator.py</code> Python<pre><code>def ensure_dir(func):\n    \"\"\"\n    Decorator to ensure a directory property exists.\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        path = abspath(func(*args, **kwargs))\n        makedirs(path, exist_ok=True)\n        return path\n\n    return wrapper\n</code></pre>","location":"utils/decorator/#danling.utils.decorator.ensure_dir"},{"title":"<code>flexible_decorator(maybe_decorator=None)</code>","text":"<p>Decorator to allow bracket-less when no arguments are passed.</p> <p>Examples:</p> <p>For decorator defined as follows:</p> Python Console Session<pre><code>&gt;&gt;&gt; @flexible_decorator\n&gt;&gt;&gt; def decorator(*args, **kwargs):\n...     pass\n</code></pre> <p>The following two are equivalent:</p> Python Console Session<pre><code>&gt;&gt;&gt; @decorator\n&gt;&gt;&gt; def func(*args, **kwargs):\n...     pass\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; @decorator()\n&gt;&gt;&gt; def func(*args, **kwargs):\n...     pass\n</code></pre>  Source code in <code>danling/utils/decorator.py</code> Python<pre><code>def flexible_decorator(maybe_decorator: Optional[Callable] = None):\n    \"\"\"\n    Decorator to allow bracket-less when no arguments are passed.\n\n    Examples\n    --------\n    For decorator defined as follows:\n    &gt;&gt;&gt; @flexible_decorator\n    &gt;&gt;&gt; def decorator(*args, **kwargs):\n    ...     pass\n\n    The following two are equivalent:\n    &gt;&gt;&gt; @decorator\n    &gt;&gt;&gt; def func(*args, **kwargs):\n    ...     pass\n\n    &gt;&gt;&gt; @decorator()\n    &gt;&gt;&gt; def func(*args, **kwargs):\n    ...     pass\n\n    \"\"\"\n\n    def decorator(func: Callable):\n        @wraps(decorator)\n        def wrapper(*args, **kwargs):\n            if len(args) == 1 and isfunction(args[0]):\n                return func(**kwargs)(args[0])\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    if maybe_decorator is None:\n        return decorator\n    return decorator(maybe_decorator)\n</code></pre>","location":"utils/decorator/#danling.utils.decorator.flexible_decorator"},{"title":"<code>method_cache(*cache_args, **lru_kwargs)</code>","text":"<p>Decorator to cache the result of an instance method.</p> <p><code>functools.lru_cache</code> uses a strong reference to the instance, which will make the instance immortal and break the garbage collection.</p> <p><code>method_cache</code> uses a weak reference to the instance and works fine.</p> <p>https://rednafi.github.io/reflections/dont-wrap-instance-methods-with-functoolslru_cache-decorator-in-python.html</p>  Source code in <code>danling/utils/decorator.py</code> Python<pre><code>def method_cache(*cache_args, **lru_kwargs):\n    r\"\"\"\n    Decorator to cache the result of an instance method.\n\n    `functools.lru_cache` uses a strong reference to the instance,\n    which will make the instance immortal and break the garbage collection.\n\n    `method_cache` uses a weak reference to the instance and works fine.\n\n    https://rednafi.github.io/reflections/dont-wrap-instance-methods-with-functoolslru_cache-decorator-in-python.html\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(self, *args, **kwargs):\n            self_ref = ref(self)\n\n            @wraps(func)\n            @lru_cache(*cache_args, **lru_kwargs)\n            def cached_method(*args, **kwargs):\n                return func(self_ref(), *args, **kwargs)\n\n            setattr(self, func.__name__, cached_method)\n            return cached_method(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n</code></pre>","location":"utils/decorator/#danling.utils.decorator.method_cache"},{"title":"IO","text":"","location":"utils/io/"},{"title":"<code>is_json_serializable(obj)</code>","text":"<p>Check if <code>obj</code> is JSON serializable.</p>  Source code in <code>danling/utils/io.py</code> Python<pre><code>def is_json_serializable(obj: Any) -&gt; bool:\n    \"\"\"\n    Check if `obj` is JSON serializable.\n    \"\"\"\n    try:\n        json_dumps(obj)\n        return True\n    except (TypeError, OverflowError):\n        return False\n</code></pre>","location":"utils/io/#danling.utils.io.is_json_serializable"},{"title":"<code>load(path, *args, **kwargs)</code>","text":"<p>Load any file with supported extensions.</p>  Source code in <code>danling/utils/io.py</code> Python<pre><code>def load(path: str, *args: List[Any], **kwargs: Dict[str, Any]) -&gt; Any:\n    \"\"\"\n    Load any file with supported extensions.\n    \"\"\"\n    if not os.path.isfile(path):\n        raise ValueError(f\"Trying to load {path} but it is not a file.\")\n    extension = os.path.splitext(path)[-1].lower()[1:]\n    if extension in PYTORCH:\n        if torch_load is None:\n            raise ImportError(f\"Trying to load {path} but torch is not installed.\")\n        return torch_load(path)\n    if extension in NUMPY:\n        if numpy_load is None:\n            raise ImportError(f\"Trying to load {path} but numpy is not installed.\")\n        return numpy_load(path, allow_pickle=True)\n    if extension in CSV:\n        if read_csv is None:\n            raise ImportError(f\"Trying to load {path} but pandas is not installed.\")\n        return read_csv(path, *args, **kwargs)\n    if extension in JSON:\n        with open(path, \"r\") as fp:  # pylint: disable=W1514, C0103\n            return json_load(fp, *args, **kwargs)  # type: ignore\n    if extension in PICKLE:\n        with open(path, \"rb\") as fp:  # pylint: disable=C0103\n            return pickle_load(fp, *args, **kwargs)  # type: ignore\n    raise ValueError(f\"Tying to load {path} with unsupported extension={extension}\")\n</code></pre>","location":"utils/io/#danling.utils.io.load"},{"title":"DanLing","text":"","location":"en/"},{"title":"Introduction","text":"<p>DanLing (\u200b\u4e39\u7075\u200b) is a high-level library to help with running neural networks flexibly and transparently.</p> <p>DanLing is meant to be a scaffold for experienced researchers and engineers who know how to define a training loop, but are bored of writing the same boilerplate code, such as DDP, logging, checkpointing, etc., over and over again.</p> <p>Therefore, DanLing does not feature complex Runner designs with many pre-defined methods and complicated hooks. Instead, the Runner of DanLing just initialise the essential parts for you, and you can do whatever you want, however you want.</p> <p>Although many attributes and properties are pre-defined and are expected to be used in DanLing, you have full control over your code.</p> <p>DanLing also provides some utilities, such as [Registry][danling.registry.registry], [NestedTensor][danling.tensors.nestedtensor], [catch][danling.utils.catch], etc.</p>","location":"en/#introduction"},{"title":"Installation","text":"<p>Install the most recent stable version on pypi:</p> Bash<pre><code>pip install danling\n</code></pre> <p>Install the latest version from source:</p> Bash<pre><code>pip install git+https://github.com/ZhiyuanChen/DanLing\n</code></pre> <p>It works the way it should have worked.</p>","location":"en/#installation"},{"title":"License","text":"<p>DanLing is multi-licensed under the following licenses:</p> <ul> <li>Unlicense</li> <li>GNU GPL 2.0 (or any later version)</li> <li>MIT</li> <li>Apache 2.0</li> <li>BSD 2-Clause</li> <li>BSD 3-Clause</li> </ul> <p>You can choose any (one or more) of them if you use this work.</p> <p><code>SPDX-License-Identifier: Unlicense OR GPL-2.0-or-later OR MIT OR Apache-2.0 OR BSD-2-Clause OR BSD-3-Clause</code></p>","location":"en/#license"},{"title":"DanLing","text":"","location":"en/package/"},{"title":"<code>AverageMeter</code>","text":"<p>Computes and stores the average and current value.</p> <p>Attributes:</p>    Name Type Description     <code>val</code>  <code>int</code>  <p>Current value.</p>   <code>avg</code>  <code>float</code>  <p>Average value.</p>   <code>sum</code>  <code>float</code>  <p>Sum of values.</p>   <code>count</code>  <code>int</code>  <p>Number of values.</p>    <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.update(0.9)\n&gt;&gt;&gt; meter.val\n0.9\n&gt;&gt;&gt; meter.avg\n0.8\n&gt;&gt;&gt; meter.sum\n1.6\n&gt;&gt;&gt; meter.count\n2\n&gt;&gt;&gt; meter.reset()\n&gt;&gt;&gt; meter.val\n0\n&gt;&gt;&gt; meter.avg\n0\n</code></pre>  Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>class AverageMeter:\n    r\"\"\"\n    Computes and stores the average and current value.\n\n    Attributes\n    ----------\n    val: int\n        Current value.\n    avg: float\n        Average value.\n    sum: float\n        Sum of values.\n    count: int\n        Number of values.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; meter = AverageMeter()\n    &gt;&gt;&gt; meter.update(0.7)\n    &gt;&gt;&gt; meter.val\n    0.7\n    &gt;&gt;&gt; meter.avg\n    0.7\n    &gt;&gt;&gt; meter.update(0.9)\n    &gt;&gt;&gt; meter.val\n    0.9\n    &gt;&gt;&gt; meter.avg\n    0.8\n    &gt;&gt;&gt; meter.sum\n    1.6\n    &gt;&gt;&gt; meter.count\n    2\n    &gt;&gt;&gt; meter.reset()\n    &gt;&gt;&gt; meter.val\n    0\n    &gt;&gt;&gt; meter.avg\n    0\n\n    ```\n    \"\"\"\n\n    val: int = 0\n    avg: float = 0\n    sum: int = 0\n    count: int = 0\n\n    def __init__(self) -&gt; None:\n        self.reset()\n\n    def reset(self) -&gt; None:\n        r\"\"\"\n        Resets the meter.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; meter = AverageMeter()\n        &gt;&gt;&gt; meter.update(0.7)\n        &gt;&gt;&gt; meter.val\n        0.7\n        &gt;&gt;&gt; meter.avg\n        0.7\n        &gt;&gt;&gt; meter.reset()\n        &gt;&gt;&gt; meter.val\n        0\n        &gt;&gt;&gt; meter.avg\n        0\n\n        ```\n        \"\"\"\n\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n: int = 1) -&gt; None:\n        r\"\"\"\n        Updates the average and current value in the meter.\n\n        Parameters\n        ----------\n        val: int\n            Value to be added to the average.\n        n: int = 1\n            Number of values to be added.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; meter = AverageMeter()\n        &gt;&gt;&gt; meter.update(0.7)\n        &gt;&gt;&gt; meter.val\n        0.7\n        &gt;&gt;&gt; meter.avg\n        0.7\n        &gt;&gt;&gt; meter.update(0.9)\n        &gt;&gt;&gt; meter.val\n        0.9\n        &gt;&gt;&gt; meter.avg\n        0.8\n        &gt;&gt;&gt; meter.sum\n        1.6\n        &gt;&gt;&gt; meter.count\n        2\n\n        ```\n        \"\"\"\n\n        # pylint: disable=C0103\n\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n</code></pre>","location":"en/package/#danling.AverageMeter"},{"title":"<code>reset()</code>","text":"<p>Resets the meter.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.reset()\n&gt;&gt;&gt; meter.val\n0\n&gt;&gt;&gt; meter.avg\n0\n</code></pre>  Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>def reset(self) -&gt; None:\n    r\"\"\"\n    Resets the meter.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; meter = AverageMeter()\n    &gt;&gt;&gt; meter.update(0.7)\n    &gt;&gt;&gt; meter.val\n    0.7\n    &gt;&gt;&gt; meter.avg\n    0.7\n    &gt;&gt;&gt; meter.reset()\n    &gt;&gt;&gt; meter.val\n    0\n    &gt;&gt;&gt; meter.avg\n    0\n\n    ```\n    \"\"\"\n\n    self.val = 0\n    self.avg = 0\n    self.sum = 0\n    self.count = 0\n</code></pre>","location":"en/package/#danling.metrics.average_meter.AverageMeter.reset"},{"title":"<code>update(val, n=1)</code>","text":"<p>Updates the average and current value in the meter.</p> <p>Parameters:</p>    Name Type Description Default     <code>val</code>   <p>Value to be added to the average.</p>  required    <code>n</code>  <code>int</code>  <p>Number of values to be added.</p>  <code>1</code>     <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.update(0.9)\n&gt;&gt;&gt; meter.val\n0.9\n&gt;&gt;&gt; meter.avg\n0.8\n&gt;&gt;&gt; meter.sum\n1.6\n&gt;&gt;&gt; meter.count\n2\n</code></pre>  Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>def update(self, val, n: int = 1) -&gt; None:\n    r\"\"\"\n    Updates the average and current value in the meter.\n\n    Parameters\n    ----------\n    val: int\n        Value to be added to the average.\n    n: int = 1\n        Number of values to be added.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; meter = AverageMeter()\n    &gt;&gt;&gt; meter.update(0.7)\n    &gt;&gt;&gt; meter.val\n    0.7\n    &gt;&gt;&gt; meter.avg\n    0.7\n    &gt;&gt;&gt; meter.update(0.9)\n    &gt;&gt;&gt; meter.val\n    0.9\n    &gt;&gt;&gt; meter.avg\n    0.8\n    &gt;&gt;&gt; meter.sum\n    1.6\n    &gt;&gt;&gt; meter.count\n    2\n\n    ```\n    \"\"\"\n\n    # pylint: disable=C0103\n\n    self.val = val\n    self.sum += val * n\n    self.count += n\n    self.avg = self.sum / self.count\n</code></pre>","location":"en/package/#danling.metrics.average_meter.AverageMeter.update"},{"title":"<code>MultiHeadAttention</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Allows the model to jointly attend to information from different representation subspaces. See <code>Attention Is All You Need &lt;https://arxiv.org/abs/1706.03762&gt;</code>_ .. math::     \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O where :math:<code>head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)</code>. Args:     embed_dim: total dimension of the model.     num_heads: parallel attention heads.     dropout: a Dropout layer on attn_output_weights. Default: 0.0.     bias: add bias as module parameter. Default: True.     add_bias_kv: add bias to the key and value sequences at dim=0.     add_zero_attn: add a new batch of zeros to the key and                    value sequences at dim=1.     k_dim: total number of features in key. Default: None.     v_dim: total number of features in value. Default: None.     batch_first: If <code>True</code>, then the input and output tensors are provided         as (batch, seq, feature). Default: <code>False</code> (seq, batch, feature). Note that if :attr:<code>k_dim</code> and :attr:<code>v_dim</code> are None, they will be set to :attr:<code>embed_dim</code> such that query, key, and value have the same number of features. Examples::     &gt;&gt;&gt; multihead_attn = dl.model.MultiHeadAttention(embed_dim, num_heads)     &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)</p>  Source code in <code>danling/models/transformer/attention/multihead_attention.py</code> Python<pre><code>class MultiHeadAttention(nn.Module):\n    r\"\"\"Allows the model to jointly attend to information\n    from different representation subspaces.\n    See `Attention Is All You Need &lt;https://arxiv.org/abs/1706.03762&gt;`_\n    .. math::\n        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n    where :math:`head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n    Args:\n        embed_dim: total dimension of the model.\n        num_heads: parallel attention heads.\n        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n        bias: add bias as module parameter. Default: True.\n        add_bias_kv: add bias to the key and value sequences at dim=0.\n        add_zero_attn: add a new batch of zeros to the key and\n                       value sequences at dim=1.\n        k_dim: total number of features in key. Default: None.\n        v_dim: total number of features in value. Default: None.\n        batch_first: If ``True``, then the input and output tensors are provided\n            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n    Note that if :attr:`k_dim` and :attr:`v_dim` are None, they will be set\n    to :attr:`embed_dim` such that query, key, and value have the same\n    number of features.\n    Examples::\n        &gt;&gt;&gt; multihead_attn = dl.model.MultiHeadAttention(embed_dim, num_heads)\n        &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)\n    \"\"\"\n    __constants__ = [\"batch_first\"]\n    bias_k: Optional[Tensor]\n    bias_v: Optional[Tensor]\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        attn_dropout: float = 0.0,\n        scale_factor: float = 1.0,\n        bias: bool = True,\n        add_bias_kv: bool = False,\n        add_zero_attn: bool = False,\n        k_dim: Optional[int] = None,\n        v_dim: Optional[int] = None,\n        batch_first: bool = True,\n        **kwargs: Optional[Dict[str, Any]],\n    ) -&gt; None:\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.k_dim = k_dim if k_dim is not None else self.embed_dim\n        self.v_dim = v_dim if v_dim is not None else self.embed_dim\n        self._qkv_same_embed_dim = self.embed_dim == self.k_dim == self.v_dim\n\n        self.num_heads = num_heads\n        self.scale_factor = scale_factor\n        self.batch_first = batch_first\n        self.head_dim = self.embed_dim // self.num_heads\n        self.scaling = float(self.head_dim * self.scale_factor) ** -0.5\n        if not self.head_dim * self.num_heads == self.embed_dim:\n            raise ValueError(f\"embed_dim {self.embed_dim} not divisible by num_heads {self.num_heads}\")\n\n        self.in_proj = nn.Linear(self.embed_dim, self.embed_dim + self.k_dim + self.v_dim, bias=bias)\n        self.dropout = nn.Dropout(attn_dropout)\n        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=bias)\n\n        if add_bias_kv:\n            self.bias_k = nn.Parameter(torch.empty((1, 1, self.embed_dim)))\n            self.bias_v = nn.Parameter(torch.empty((1, 1, self.embed_dim)))\n        else:\n            self.bias_k = self.bias_v = None\n\n        self.add_zero_attn = add_zero_attn\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        nn.init.xavier_uniform_(self.in_proj.weight)\n        if self.in_proj.bias is not None:\n            nn.init.constant_(self.in_proj.bias, 0.0)\n            nn.init.constant_(self.out_proj.bias, 0.0)\n        if self.bias_k is not None:\n            nn.init.xavier_normal_(self.bias_k)\n        if self.bias_v is not None:\n            nn.init.xavier_normal_(self.bias_v)\n\n    def __setstate__(self, state):\n        # Support loading old MultiHeadAttention checkpoints generated by v1.1.0\n        if \"_qkv_same_embed_dim\" not in state:\n            state[\"_qkv_same_embed_dim\"] = True\n        super(MultiHeadAttention, self).__setstate__(state)\n\n    def forward(\n        self,\n        query: Tensor,\n        key: Tensor,\n        value: Tensor,\n        attn_bias: Optional[Tensor] = None,\n        attn_mask: Optional[Tensor] = None,\n        key_padding_mask: Optional[Tensor] = None,\n        need_weights: bool = False,\n        static_k: Optional[Tensor] = None,\n        static_v: Optional[Tensor] = None,\n    ) -&gt; Tuple[Tensor, Optional[Tensor]]:\n        r\"\"\"\n        Args:\n            query, key, value: map a query and a set of key-value pairs to an output.\n                See \"Attention Is All You Need\" for more details.\n            attn_bias: 2D or 3D mask that add bias to attention output weights. Used for relative positional embedding.\n                A 2D bias will be broadcasted for all the batches while a 3D mask allows to specify a different mask for\n                the entries of each batch.\n            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n                the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n            key_padding_mask: if provided, specified padding elements in the key will\n                be ignored by the attention. When given a binary mask and a value is True,\n                the corresponding value on the attention layer will be ignored. When given\n                a byte mask and a value is non-zero, the corresponding value on the attention\n                layer will be ignored\n            need_weights: output attn_output_weights.\n            static_k, static_v: static key and value used for attention operators.\n        Shapes for inputs:\n            - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n                the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n            - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n                the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n            - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n                the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n            - attn_bias: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n                source sequence length.\n                If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n                length, S is the source sequence length. ``attn_bias`` allows to pass pos embed directly into attention\n                If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will\n                be unchanged. If a BoolTensor is provided, positions with ``True`` is not allowed to attend while ``False``\n                values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight.\n            - attn_mask: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n                source sequence length.\n                If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n                length, S is the source sequence length. ``attn_mask`` ensure that position i is allowed to attend\n                the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n                while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n                is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n                is provided, it will be added to the attention weight.\n            - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n                If a ByteTensor is provided, the non-zero positions will be ignored while the position\n                with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the\n                value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n            - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n                N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n            - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n                N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n        Outputs:\n            - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n                E is the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n            - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n                L is the target sequence length, S is the source sequence length.\n        \"\"\"\n        if self.batch_first:\n            query, key, value = [x.transpose(0, 1) for x in (query, key, value)]\n\n        # set up shape vars\n        target_len, batch_size, embed_dim = query.shape\n        source_len, _, _ = key.shape\n        if not key.shape[:2] == value.shape[:2]:\n            raise ValueError(f\"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}\")\n\n        q, k, v = self.in_projection(query, key, value)\n\n        # prep attention mask\n        if attn_mask is not None:\n            if attn_mask.dtype == torch.uint8:\n                warnings.warn(\n                    \"attn_mask is of type uint8. This type is deprecated. Please use bool or float tensors instead.\"\n                )\n                attn_mask = attn_mask.to(torch.bool)\n            elif not (attn_mask.is_floating_point() or attn_mask.dtype == torch.bool):\n                raise ValueError(f\"attn_mask should have type float or bool, but got {attn_mask.dtype}.\")\n            # ensure attn_mask's dim is 3\n            if attn_mask.dim() == 2:\n                correct_shape = (target_len, source_len)\n                if attn_mask.shape != correct_shape:\n                    raise ValueError(f\"attn_mask should have shape {correct_shape}, but got {attn_mask.shape}.\")\n                attn_mask = attn_mask.unsqueeze(0)\n            elif attn_mask.dim() == 3:\n                correct_shape = (batch_size * self.num_heads, target_len, source_len)  # type: ignore\n                if attn_mask.shape != correct_shape:\n                    raise ValueError(f\"attn_mask should have shape {correct_shape}, but got {attn_mask.shape}.\")\n            else:\n                raise RuntimeError(f\"attn_mask should have dimension 2 or 3, bug got {attn_mask.dim()}.\")\n\n        # prep key padding mask\n        if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n            warnings.warn(\n                \"key_padding_mask is of type uint8. This type is deprecated. Please use bool or float tensors instead.\"\n            )\n            key_padding_mask = key_padding_mask.to(torch.bool)\n\n        # add bias along batch dimension (currently second)\n        if self.bias_k is not None and self.bias_v is not None:\n            assert static_k is None, \"bias cannot be added to static key.\"\n            assert static_v is None, \"bias cannot be added to static value.\"\n            k = torch.cat([k, self.bias_k.repeat(1, batch_size, 1)])\n            v = torch.cat([v, self.bias_v.repeat(1, batch_size, 1)])\n            if attn_mask is not None:\n                attn_mask = F.pad(attn_mask, (0, 1))\n            if key_padding_mask is not None:\n                key_padding_mask = F.pad(key_padding_mask, (0, 1))\n        else:\n            assert self.bias_k is None\n            assert self.bias_v is None\n\n        # reshape q, k, v for multihead attention and make em batch first\n        q = q.reshape(target_len, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n        k = k.reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1) if static_k is None else static_k\n        v = v.reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1) if static_v is None else static_v\n\n        if static_k is not None:\n            correct_shape = (  # type: ignore\n                batch_size * self.num_heads,\n                static_k.shape[1],\n                self.head_dim,\n            )\n            if static_k.shape != correct_shape:\n                raise ValueError(f\"static_k should have shape {correct_shape}, but got {static_k.shape}.\")\n        if static_v is not None:\n            correct_shape = (  # type: ignore\n                batch_size * self.num_heads,\n                static_v.shape[1],\n                self.head_dim,\n            )\n            if static_v.shape != correct_shape:\n                raise ValueError(f\"static_v should have shape {correct_shape}, but got {static_v.shape}.\")\n\n        # add zero attention along batch dimension (now first)\n        if self.add_zero_attn:\n            zero_attn_shape = (batch_size * self.num_heads, 1, self.head_dim)\n            k = torch.cat([k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1)\n            v = torch.cat([v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1)\n            if attn_mask is not None:\n                attn_mask = F.pad(attn_mask, (0, 1))\n            if key_padding_mask is not None:\n                key_padding_mask = F.pad(key_padding_mask, (0, 1))\n\n        # update source sequence length after adjustments\n        source_len = k.shape[1]\n\n        # merge key padding and attention masks\n        if key_padding_mask is not None:\n            if key_padding_mask.shape != (batch_size, source_len):\n                raise ValueError(\n                    f\"key_padding_mask should have shape {(batch_size, source_len)}, but got {key_padding_mask.shape}\"\n                )\n            key_padding_mask = (\n                key_padding_mask.view(batch_size, 1, 1, source_len)\n                .expand(-1, self.num_heads, -1, -1)\n                .reshape(batch_size * self.num_heads, 1, source_len)\n            )\n            if attn_mask is None:\n                attn_mask = key_padding_mask\n            elif attn_mask.dtype == torch.bool:\n                attn_mask = attn_mask.logical_or(key_padding_mask)\n            else:\n                attn_mask = attn_mask.masked_fill(key_padding_mask, float(\"-inf\"))\n\n        # convert mask to float\n        if attn_mask is not None and attn_mask.dtype == torch.bool:\n            new_attn_mask = torch.zeros_like(attn_mask, dtype=torch.float)\n            new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n            attn_mask = new_attn_mask\n\n        # (deep breath) calculate attention and out projection\n        attn_output, attn_output_weights = self.attention(q, k, v, attn_bias, attn_mask)\n        attn_output = attn_output.transpose(0, 1).reshape(target_len, batch_size, embed_dim)\n        attn_output = self.out_projection(attn_output)\n\n        # attn_output_weights is set to torch.empty(0, requires_grad=False) to avoid errors in DDP\n        attn_output_weights = (\n            attn_output_weights.view(batch_size, self.num_heads, target_len, source_len)\n            if need_weights\n            else torch.empty(0, requires_grad=False)\n        )\n\n        if self.batch_first:\n            return attn_output.transpose(0, 1), attn_output_weights\n        else:\n            return attn_output, attn_output_weights\n\n    def in_projection(self, q: Tensor, k: Tensor, v: Tensor) -&gt; Tuple[Tensor, Tensor, Tensor]:\n        r\"\"\"\n        Performs the in-projection step of the attention operation, using packed weights.\n        Output is a triple containing projection tensors for query, key and value.\n        Args:\n            q, k, v: query, key and value tensors to be projected. For self-attention,\n                these are typically the same tensor; for encoder-decoder attention,\n                k and v are typically the same tensor. (We take advantage of these\n                identities for performance if they are present.) Regardless, q, k and v\n                must share a common embedding dimension; otherwise their shapes may vary.\n        Shape:\n            Inputs:\n            - q: :math:`(..., E)` where E is the embedding dimension\n            - k: :math:`(..., E)` where E is the embedding dimension\n            - v: :math:`(..., E)` where E is the embedding dimension\n            Output:\n            - in output list :math:`[q', k', v']`, each output tensor will have the\n                same shape as the corresponding input tensor.\n        \"\"\"\n        if k is v:\n            # self-attention\n            if q is k:\n                return self.in_proj(q).split((self.embed_dim, self.k_dim, self.v_dim), dim=-1)\n            # encoder-decoder attention\n            else:\n                w_q, w_kv = self.in_proj.weight.split([self.embed_dim, self.k_dim + self.v_dim])\n                b_q, b_kv = (\n                    None\n                    if self.in_proj.bias is None\n                    else self.in_proj.bias.split([self.embed_dim, self.k_dim + self.v_dim])\n                )\n                return (F.linear(q, w_q, b_q),) + F.linear(k, w_kv, b_kv).split((self.k_dim, self.v_dim), dim=-1)\n        else:\n            w_q, w_k, w_v = self.in_proj.weight.split([self.embed_dim, self.k_dim, self.v_dim])\n            b_q, b_k, b_v = (\n                None if self.in_proj.bias is None else self.in_proj.bias.split([self.embed_dim, self.k_dim, self.v_dim])\n            )\n            return F.linear(q, w_q, b_q), F.linear(k, w_k, b_k), F.linear(v, w_v, b_v)\n\n    def attention(\n        self,\n        q: Tensor,\n        k: Tensor,\n        v: Tensor,\n        attn_bias: Optional[Tensor] = None,\n        attn_mask: Optional[Tensor] = None,\n    ) -&gt; Tuple[Tensor, Tensor]:\n        r\"\"\"\n        Computes scaled dot product attention on query, key and value tensors, using\n        an optional attention mask if passed, and applying dropout if a probability\n        greater than 0.0 is specified.\n        Returns a tensor pair containing attended values and attention weights.\n        Args:\n            q, k, v: query, key and value tensors. See Shape section for shape details.\n            attn_mask: optional tensor containing mask values to be added to calculated\n                attention. May be 2D or 3D; see Shape section for details.\n            attn_bias: optional tensor containing bias values to be added to calculated\n                attention. Used for relative positional embedding. May be 2D or 3D; see\n                Shape section for details.\n        Shape:\n            - q: :math:`(B, Nt, E)` where B is batch size, Nt is the target sequence length,\n                and E is embedding dimension.\n            - key: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n                and E is embedding dimension.\n            - value: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n                and E is embedding dimension.\n            - attn_bias: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n                shape :math:`(Nt, Ns)`.\n            - attn_mask: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n                shape :math:`(Nt, Ns)`.\n            - Output: attention values have shape :math:`(B, Nt, E)`; attention weights\n                have shape :math:`(B, Nt, Ns)`\n        \"\"\"\n        q *= self.scaling\n        # (B, Nt, E) x (B, E, Ns) -&gt; (B, Nt, Ns)\n        attn = torch.bmm(q, k.transpose(-2, -1))\n        if attn_bias is not None:\n            attn += attn_bias\n        if attn_mask is not None:\n            attn += attn_mask\n        attn = F.softmax(attn, dim=-1)\n        attn = self.dropout(attn)\n        # (B, Nt, Ns) x (B, Ns, E) -&gt; (B, Nt, E)\n        output = torch.bmm(attn, v)\n        return output, attn\n\n    def out_projection(self, attn_output: Tensor) -&gt; Tensor:\n        return self.out_proj(attn_output)\n</code></pre>","location":"en/package/#danling.MultiHeadAttention"},{"title":"<code>attention(q, k, v, attn_bias=None, attn_mask=None)</code>","text":"<p>Computes scaled dot product attention on query, key and value tensors, using an optional attention mask if passed, and applying dropout if a probability greater than 0.0 is specified. Returns a tensor pair containing attended values and attention weights. Args:     q, k, v: query, key and value tensors. See Shape section for shape details.     attn_mask: optional tensor containing mask values to be added to calculated         attention. May be 2D or 3D; see Shape section for details.     attn_bias: optional tensor containing bias values to be added to calculated         attention. Used for relative positional embedding. May be 2D or 3D; see         Shape section for details. Shape:     - q: :math:<code>(B, Nt, E)</code> where B is batch size, Nt is the target sequence length,         and E is embedding dimension.     - key: :math:<code>(B, Ns, E)</code> where B is batch size, Ns is the source sequence length,         and E is embedding dimension.     - value: :math:<code>(B, Ns, E)</code> where B is batch size, Ns is the source sequence length,         and E is embedding dimension.     - attn_bias: either a 3D tensor of shape :math:<code>(B, Nt, Ns)</code> or a 2D tensor of         shape :math:<code>(Nt, Ns)</code>.     - attn_mask: either a 3D tensor of shape :math:<code>(B, Nt, Ns)</code> or a 2D tensor of         shape :math:<code>(Nt, Ns)</code>.     - Output: attention values have shape :math:<code>(B, Nt, E)</code>; attention weights         have shape :math:<code>(B, Nt, Ns)</code></p>  Source code in <code>danling/models/transformer/attention/multihead_attention.py</code> Python<pre><code>def attention(\n    self,\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    attn_bias: Optional[Tensor] = None,\n    attn_mask: Optional[Tensor] = None,\n) -&gt; Tuple[Tensor, Tensor]:\n    r\"\"\"\n    Computes scaled dot product attention on query, key and value tensors, using\n    an optional attention mask if passed, and applying dropout if a probability\n    greater than 0.0 is specified.\n    Returns a tensor pair containing attended values and attention weights.\n    Args:\n        q, k, v: query, key and value tensors. See Shape section for shape details.\n        attn_mask: optional tensor containing mask values to be added to calculated\n            attention. May be 2D or 3D; see Shape section for details.\n        attn_bias: optional tensor containing bias values to be added to calculated\n            attention. Used for relative positional embedding. May be 2D or 3D; see\n            Shape section for details.\n    Shape:\n        - q: :math:`(B, Nt, E)` where B is batch size, Nt is the target sequence length,\n            and E is embedding dimension.\n        - key: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n            and E is embedding dimension.\n        - value: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n            and E is embedding dimension.\n        - attn_bias: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n            shape :math:`(Nt, Ns)`.\n        - attn_mask: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n            shape :math:`(Nt, Ns)`.\n        - Output: attention values have shape :math:`(B, Nt, E)`; attention weights\n            have shape :math:`(B, Nt, Ns)`\n    \"\"\"\n    q *= self.scaling\n    # (B, Nt, E) x (B, E, Ns) -&gt; (B, Nt, Ns)\n    attn = torch.bmm(q, k.transpose(-2, -1))\n    if attn_bias is not None:\n        attn += attn_bias\n    if attn_mask is not None:\n        attn += attn_mask\n    attn = F.softmax(attn, dim=-1)\n    attn = self.dropout(attn)\n    # (B, Nt, Ns) x (B, Ns, E) -&gt; (B, Nt, E)\n    output = torch.bmm(attn, v)\n    return output, attn\n</code></pre>","location":"en/package/#danling.models.transformer.attention.multihead_attention.MultiHeadAttention.attention"},{"title":"<code>forward(query, key, value, attn_bias=None, attn_mask=None, key_padding_mask=None, need_weights=False, static_k=None, static_v=None)</code>","text":"<p>Shapes for inputs:     - query: :math:<code>(L, N, E)</code> where L is the target sequence length, N is the batch size, E is         the embedding dimension. :math:<code>(N, L, E)</code> if <code>batch_first</code> is <code>True</code>.     - key: :math:<code>(S, N, E)</code>, where S is the source sequence length, N is the batch size, E is         the embedding dimension. :math:<code>(N, S, E)</code> if <code>batch_first</code> is <code>True</code>.     - value: :math:<code>(S, N, E)</code> where S is the source sequence length, N is the batch size, E is         the embedding dimension. :math:<code>(N, S, E)</code> if <code>batch_first</code> is <code>True</code>.     - attn_bias: if a 2D mask: :math:<code>(L, S)</code> where L is the target sequence length, S is the         source sequence length.         If a 3D mask: :math:<code>(N\\cdot\\text{num\\_heads}, L, S)</code> where N is the batch size, L is the target sequence         length, S is the source sequence length. <code>attn_bias</code> allows to pass pos embed directly into attention         If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will         be unchanged. If a BoolTensor is provided, positions with <code>True</code> is not allowed to attend while <code>False</code>         values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight.     - attn_mask: if a 2D mask: :math:<code>(L, S)</code> where L is the target sequence length, S is the         source sequence length.         If a 3D mask: :math:<code>(N\\cdot\\text{num\\_heads}, L, S)</code> where N is the batch size, L is the target sequence         length, S is the source sequence length. <code>attn_mask</code> ensure that position i is allowed to attend         the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend         while the zero positions will be unchanged. If a BoolTensor is provided, positions with <code>True</code>         is not allowed to attend while <code>False</code> values will be unchanged. If a FloatTensor         is provided, it will be added to the attention weight.     - key_padding_mask: :math:<code>(N, S)</code> where N is the batch size, S is the source sequence length.         If a ByteTensor is provided, the non-zero positions will be ignored while the position         with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the         value of <code>True</code> will be ignored while the position with the value of <code>False</code> will be unchanged.     - static_k: :math:<code>(N*num_heads, S, E/num_heads)</code>, where S is the source sequence length,         N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.     - static_v: :math:<code>(N*num_heads, S, E/num_heads)</code>, where S is the source sequence length,         N is the batch size, E is the embedding dimension. E/num_heads is the head dimension. Outputs:     - attn_output: :math:<code>(L, N, E)</code> where L is the target sequence length, N is the batch size,         E is the embedding dimension. :math:<code>(N, L, E)</code> if <code>batch_first</code> is <code>True</code>.     - attn_output_weights: :math:<code>(N, L, S)</code> where N is the batch size,         L is the target sequence length, S is the source sequence length.</p>  Source code in <code>danling/models/transformer/attention/multihead_attention.py</code> Python<pre><code>def forward(\n    self,\n    query: Tensor,\n    key: Tensor,\n    value: Tensor,\n    attn_bias: Optional[Tensor] = None,\n    attn_mask: Optional[Tensor] = None,\n    key_padding_mask: Optional[Tensor] = None,\n    need_weights: bool = False,\n    static_k: Optional[Tensor] = None,\n    static_v: Optional[Tensor] = None,\n) -&gt; Tuple[Tensor, Optional[Tensor]]:\n    r\"\"\"\n    Args:\n        query, key, value: map a query and a set of key-value pairs to an output.\n            See \"Attention Is All You Need\" for more details.\n        attn_bias: 2D or 3D mask that add bias to attention output weights. Used for relative positional embedding.\n            A 2D bias will be broadcasted for all the batches while a 3D mask allows to specify a different mask for\n            the entries of each batch.\n        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n        key_padding_mask: if provided, specified padding elements in the key will\n            be ignored by the attention. When given a binary mask and a value is True,\n            the corresponding value on the attention layer will be ignored. When given\n            a byte mask and a value is non-zero, the corresponding value on the attention\n            layer will be ignored\n        need_weights: output attn_output_weights.\n        static_k, static_v: static key and value used for attention operators.\n    Shapes for inputs:\n        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n            the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n            the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n            the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n        - attn_bias: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n            source sequence length.\n            If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n            length, S is the source sequence length. ``attn_bias`` allows to pass pos embed directly into attention\n            If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will\n            be unchanged. If a BoolTensor is provided, positions with ``True`` is not allowed to attend while ``False``\n            values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight.\n        - attn_mask: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n            source sequence length.\n            If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n            length, S is the source sequence length. ``attn_mask`` ensure that position i is allowed to attend\n            the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n            while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n            is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n            is provided, it will be added to the attention weight.\n        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n            If a ByteTensor is provided, the non-zero positions will be ignored while the position\n            with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the\n            value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n            N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n            N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n    Outputs:\n        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n            E is the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n            L is the target sequence length, S is the source sequence length.\n    \"\"\"\n    if self.batch_first:\n        query, key, value = [x.transpose(0, 1) for x in (query, key, value)]\n\n    # set up shape vars\n    target_len, batch_size, embed_dim = query.shape\n    source_len, _, _ = key.shape\n    if not key.shape[:2] == value.shape[:2]:\n        raise ValueError(f\"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}\")\n\n    q, k, v = self.in_projection(query, key, value)\n\n    # prep attention mask\n    if attn_mask is not None:\n        if attn_mask.dtype == torch.uint8:\n            warnings.warn(\n                \"attn_mask is of type uint8. This type is deprecated. Please use bool or float tensors instead.\"\n            )\n            attn_mask = attn_mask.to(torch.bool)\n        elif not (attn_mask.is_floating_point() or attn_mask.dtype == torch.bool):\n            raise ValueError(f\"attn_mask should have type float or bool, but got {attn_mask.dtype}.\")\n        # ensure attn_mask's dim is 3\n        if attn_mask.dim() == 2:\n            correct_shape = (target_len, source_len)\n            if attn_mask.shape != correct_shape:\n                raise ValueError(f\"attn_mask should have shape {correct_shape}, but got {attn_mask.shape}.\")\n            attn_mask = attn_mask.unsqueeze(0)\n        elif attn_mask.dim() == 3:\n            correct_shape = (batch_size * self.num_heads, target_len, source_len)  # type: ignore\n            if attn_mask.shape != correct_shape:\n                raise ValueError(f\"attn_mask should have shape {correct_shape}, but got {attn_mask.shape}.\")\n        else:\n            raise RuntimeError(f\"attn_mask should have dimension 2 or 3, bug got {attn_mask.dim()}.\")\n\n    # prep key padding mask\n    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n        warnings.warn(\n            \"key_padding_mask is of type uint8. This type is deprecated. Please use bool or float tensors instead.\"\n        )\n        key_padding_mask = key_padding_mask.to(torch.bool)\n\n    # add bias along batch dimension (currently second)\n    if self.bias_k is not None and self.bias_v is not None:\n        assert static_k is None, \"bias cannot be added to static key.\"\n        assert static_v is None, \"bias cannot be added to static value.\"\n        k = torch.cat([k, self.bias_k.repeat(1, batch_size, 1)])\n        v = torch.cat([v, self.bias_v.repeat(1, batch_size, 1)])\n        if attn_mask is not None:\n            attn_mask = F.pad(attn_mask, (0, 1))\n        if key_padding_mask is not None:\n            key_padding_mask = F.pad(key_padding_mask, (0, 1))\n    else:\n        assert self.bias_k is None\n        assert self.bias_v is None\n\n    # reshape q, k, v for multihead attention and make em batch first\n    q = q.reshape(target_len, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n    k = k.reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1) if static_k is None else static_k\n    v = v.reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1) if static_v is None else static_v\n\n    if static_k is not None:\n        correct_shape = (  # type: ignore\n            batch_size * self.num_heads,\n            static_k.shape[1],\n            self.head_dim,\n        )\n        if static_k.shape != correct_shape:\n            raise ValueError(f\"static_k should have shape {correct_shape}, but got {static_k.shape}.\")\n    if static_v is not None:\n        correct_shape = (  # type: ignore\n            batch_size * self.num_heads,\n            static_v.shape[1],\n            self.head_dim,\n        )\n        if static_v.shape != correct_shape:\n            raise ValueError(f\"static_v should have shape {correct_shape}, but got {static_v.shape}.\")\n\n    # add zero attention along batch dimension (now first)\n    if self.add_zero_attn:\n        zero_attn_shape = (batch_size * self.num_heads, 1, self.head_dim)\n        k = torch.cat([k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1)\n        v = torch.cat([v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1)\n        if attn_mask is not None:\n            attn_mask = F.pad(attn_mask, (0, 1))\n        if key_padding_mask is not None:\n            key_padding_mask = F.pad(key_padding_mask, (0, 1))\n\n    # update source sequence length after adjustments\n    source_len = k.shape[1]\n\n    # merge key padding and attention masks\n    if key_padding_mask is not None:\n        if key_padding_mask.shape != (batch_size, source_len):\n            raise ValueError(\n                f\"key_padding_mask should have shape {(batch_size, source_len)}, but got {key_padding_mask.shape}\"\n            )\n        key_padding_mask = (\n            key_padding_mask.view(batch_size, 1, 1, source_len)\n            .expand(-1, self.num_heads, -1, -1)\n            .reshape(batch_size * self.num_heads, 1, source_len)\n        )\n        if attn_mask is None:\n            attn_mask = key_padding_mask\n        elif attn_mask.dtype == torch.bool:\n            attn_mask = attn_mask.logical_or(key_padding_mask)\n        else:\n            attn_mask = attn_mask.masked_fill(key_padding_mask, float(\"-inf\"))\n\n    # convert mask to float\n    if attn_mask is not None and attn_mask.dtype == torch.bool:\n        new_attn_mask = torch.zeros_like(attn_mask, dtype=torch.float)\n        new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n        attn_mask = new_attn_mask\n\n    # (deep breath) calculate attention and out projection\n    attn_output, attn_output_weights = self.attention(q, k, v, attn_bias, attn_mask)\n    attn_output = attn_output.transpose(0, 1).reshape(target_len, batch_size, embed_dim)\n    attn_output = self.out_projection(attn_output)\n\n    # attn_output_weights is set to torch.empty(0, requires_grad=False) to avoid errors in DDP\n    attn_output_weights = (\n        attn_output_weights.view(batch_size, self.num_heads, target_len, source_len)\n        if need_weights\n        else torch.empty(0, requires_grad=False)\n    )\n\n    if self.batch_first:\n        return attn_output.transpose(0, 1), attn_output_weights\n    else:\n        return attn_output, attn_output_weights\n</code></pre>","location":"en/package/#danling.models.transformer.attention.multihead_attention.MultiHeadAttention.forward"},{"title":"<code>in_projection(q, k, v)</code>","text":"<p>Performs the in-projection step of the attention operation, using packed weights. Output is a triple containing projection tensors for query, key and value. Args:     q, k, v: query, key and value tensors to be projected. For self-attention,         these are typically the same tensor; for encoder-decoder attention,         k and v are typically the same tensor. (We take advantage of these         identities for performance if they are present.) Regardless, q, k and v         must share a common embedding dimension; otherwise their shapes may vary. Shape:     Inputs:     - q: :math:<code>(..., E)</code> where E is the embedding dimension     - k: :math:<code>(..., E)</code> where E is the embedding dimension     - v: :math:<code>(..., E)</code> where E is the embedding dimension     Output:     - in output list :math:<code>[q', k', v']</code>, each output tensor will have the         same shape as the corresponding input tensor.</p>  Source code in <code>danling/models/transformer/attention/multihead_attention.py</code> Python<pre><code>def in_projection(self, q: Tensor, k: Tensor, v: Tensor) -&gt; Tuple[Tensor, Tensor, Tensor]:\n    r\"\"\"\n    Performs the in-projection step of the attention operation, using packed weights.\n    Output is a triple containing projection tensors for query, key and value.\n    Args:\n        q, k, v: query, key and value tensors to be projected. For self-attention,\n            these are typically the same tensor; for encoder-decoder attention,\n            k and v are typically the same tensor. (We take advantage of these\n            identities for performance if they are present.) Regardless, q, k and v\n            must share a common embedding dimension; otherwise their shapes may vary.\n    Shape:\n        Inputs:\n        - q: :math:`(..., E)` where E is the embedding dimension\n        - k: :math:`(..., E)` where E is the embedding dimension\n        - v: :math:`(..., E)` where E is the embedding dimension\n        Output:\n        - in output list :math:`[q', k', v']`, each output tensor will have the\n            same shape as the corresponding input tensor.\n    \"\"\"\n    if k is v:\n        # self-attention\n        if q is k:\n            return self.in_proj(q).split((self.embed_dim, self.k_dim, self.v_dim), dim=-1)\n        # encoder-decoder attention\n        else:\n            w_q, w_kv = self.in_proj.weight.split([self.embed_dim, self.k_dim + self.v_dim])\n            b_q, b_kv = (\n                None\n                if self.in_proj.bias is None\n                else self.in_proj.bias.split([self.embed_dim, self.k_dim + self.v_dim])\n            )\n            return (F.linear(q, w_q, b_q),) + F.linear(k, w_kv, b_kv).split((self.k_dim, self.v_dim), dim=-1)\n    else:\n        w_q, w_k, w_v = self.in_proj.weight.split([self.embed_dim, self.k_dim, self.v_dim])\n        b_q, b_k, b_v = (\n            None if self.in_proj.bias is None else self.in_proj.bias.split([self.embed_dim, self.k_dim, self.v_dim])\n        )\n        return F.linear(q, w_q, b_q), F.linear(k, w_k, b_k), F.linear(v, w_v, b_v)\n</code></pre>","location":"en/package/#danling.models.transformer.attention.multihead_attention.MultiHeadAttention.in_projection"},{"title":"<code>NestedTensor</code>","text":"<p>Wrap a sequence of tensors into a single tensor with a mask.</p> <p>In sequence to sequence tasks, elements of a batch are usually not of the same length. This made it tricky to use a single tensor to represent a batch of sequences.</p> <p>NestedTensor allows to store a sequence of tensors of different lengths in a single object. It also provides a mask that can be used to retrieve the original sequence of tensors.</p> <p>Attributes:</p>    Name Type Description     <code>storage</code>  <code>Sequence[torch.Tensor]</code>  <p>The sequence of tensors.</p>   <code>batch_first</code>  <code>bool = True</code>  <p>Whether the first dimension of the tensors is the batch dimension.</p> <p>If <code>True</code>, the first dimension is the batch dimension, i.e., <code>B, N, *</code>.</p> <p>If <code>False</code>, the first dimension is the sequence dimension, i.e., <code>N, B, *</code></p>    <p>Parameters:</p>    Name Type Description Default     <code>tensors</code>  <code>Sequence[Tensor]</code>    required    <code>batch_first</code>  <code>bool</code>    <code>True</code>     <p>Raises:</p>    Type Description      <code>ValueError</code>  <p>If <code>tensors</code> is not a sequence.</p> <p>If <code>tensors</code> is empty.</p>","location":"en/package/#danling.NestedTensor"},{"title":"Notes","text":"<p>We have rewritten the <code>__getattr__</code> function to support as much native tensor operations as possible. However, not all operations are tested.</p> <p>Please file an issue if you find any bugs.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.device\ndevice(type='cpu')\n&gt;&gt;&gt; nested_tensor.dtype\ntorch.int64\n&gt;&gt;&gt; nested_tensor.tensor\ntensor([[1, 2, 3],\n        [4, 5, 0]])\n&gt;&gt;&gt; nested_tensor.mask\ntensor([[ True,  True,  True],\n        [ True,  True, False]])\n&gt;&gt;&gt; nested_tensor.to(torch.float).tensor\ntensor([[1., 2., 3.],\n        [4., 5., 0.]])\n&gt;&gt;&gt; nested_tensor.half().tensor\ntensor([[1., 2., 3.],\n        [4., 5., 0.]], dtype=torch.float16)\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>class NestedTensor:\n    r\"\"\"\n    Wrap a sequence of tensors into a single tensor with a mask.\n\n    In sequence to sequence tasks, elements of a batch are usually not of the same length.\n    This made it tricky to use a single tensor to represent a batch of sequences.\n\n    NestedTensor allows to store a sequence of tensors of different lengths in a single object.\n    It also provides a mask that can be used to retrieve the original sequence of tensors.\n\n    Attributes\n    ----------\n    storage: Sequence[torch.Tensor]\n        The sequence of tensors.\n    batch_first: bool = True\n        Whether the first dimension of the tensors is the batch dimension.\n\n        If `True`, the first dimension is the batch dimension, i.e., `B, N, *`.\n\n        If `False`, the first dimension is the sequence dimension, i.e., `N, B, *`\n\n    Parameters\n    ----------\n    tensors: Sequence[torch.Tensor]\n    batch_first: bool = True\n\n    Raises\n    ------\n    ValueError\n        If `tensors` is not a sequence.\n\n        If `tensors` is empty.\n\n    Notes\n    -----\n    We have rewritten the `__getattr__` function to support as much native tensor operations as possible.\n    However, not all operations are tested.\n\n    Please file an issue if you find any bugs.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.shape\n    torch.Size([2, 3])\n    &gt;&gt;&gt; nested_tensor.device\n    device(type='cpu')\n    &gt;&gt;&gt; nested_tensor.dtype\n    torch.int64\n    &gt;&gt;&gt; nested_tensor.tensor\n    tensor([[1, 2, 3],\n            [4, 5, 0]])\n    &gt;&gt;&gt; nested_tensor.mask\n    tensor([[ True,  True,  True],\n            [ True,  True, False]])\n    &gt;&gt;&gt; nested_tensor.to(torch.float).tensor\n    tensor([[1., 2., 3.],\n            [4., 5., 0.]])\n    &gt;&gt;&gt; nested_tensor.half().tensor\n    tensor([[1., 2., 3.],\n            [4., 5., 0.]], dtype=torch.float16)\n\n    ```\n    \"\"\"\n\n    storage: Sequence[Tensor] = []\n    batch_first: bool = True\n\n    def __init__(self, tensors: Sequence[Tensor], batch_first: bool = True) -&gt; None:\n        if not isinstance(tensors, Sequence):\n            raise ValueError(f\"NestedTensor should be initialised with a Sequence, bug got {type(values)}.\")\n        if len(tensors) == 0:\n            raise ValueError(f\"NestedTensor should be initialised with a non-empty sequence.\")\n        if not isinstance(tensors[0], Tensor):\n            tensors = tuple(torch.tensor(tensor) for tensor in tensors)\n        self.storage = tensors\n        self.batch_first = batch_first\n\n    @property\n    def tensor(self) -&gt; Tensor:\n        r\"\"\"\n        Return a single tensor by padding all the tensors.\n\n        Returns\n        -------\n        torch.Tensor\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.tensor\n        tensor([[1, 2, 3],\n                [4, 5, 0]])\n\n        ```\n        \"\"\"\n\n        return self._tensor(tuple(self.storage), self.batch_first)\n\n    @property\n    def mask(self) -&gt; Tensor:\n        r\"\"\"\n        Padding mask of `tensor`.\n\n        Returns\n        -------\n        torch.Tensor\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.mask\n        tensor([[ True,  True,  True],\n                [ True,  True, False]])\n\n        ```\n        \"\"\"\n\n        return self._mask(tuple(self.storage))\n\n    @property\n    def device(self) -&gt; torch.device:\n        r\"\"\"\n        Device of the NestedTensor.\n\n        Returns\n        -------\n        torch.Tensor\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.device\n        device(type='cpu')\n\n        ```\n        \"\"\"\n\n        return self._device(tuple(self.storage))\n\n    @property\n    def shape(self) -&gt; torch.Size:\n        r\"\"\"\n        Alias for `size`.\n\n        Returns\n        -------\n        torch.Size\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.shape\n        torch.Size([2, 3])\n        &gt;&gt;&gt; nested_tensor.storage.append(torch.tensor([6, 7, 8, 9]))\n        &gt;&gt;&gt; nested_tensor.shape\n        torch.Size([3, 4])\n\n        ```\n        \"\"\"\n\n        return self.size()\n\n    def size(self) -&gt; torch.Size:\n        r\"\"\"\n        Shape of the NestedTensor.\n\n        Returns\n        -------\n        torch.Size\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.size()\n        torch.Size([2, 3])\n        &gt;&gt;&gt; nested_tensor.storage[1] = torch.tensor([4, 5, 6, 7])\n        &gt;&gt;&gt; nested_tensor.size()\n        torch.Size([2, 4])\n\n        ```\n        \"\"\"\n\n        return self._size(tuple(self.storage))\n\n    def __getitem__(self, index) -&gt; Tuple[Tensor, Tensor]:\n        ret = self.storage[index]\n        if isinstance(ret, Tensor):\n            return ret, torch.ones_like(ret)  # pylint: disable=E1101\n        return self.tensor, self.mask\n\n    def __getattr__(self, name) -&gt; Any:\n        if not self.storage:\n            raise ValueError(f\"Unable to get {name} from an empty {self.__class__.__name__}\")\n        ret = [getattr(i, name) for i in self.storage]\n        elem = ret[0]\n        if isinstance(elem, Tensor):\n            return NestedTensor(ret)\n        if callable(elem):\n            return _TensorFuncWrapper(ret)\n        if len(set(ret)) == 1:\n            return elem\n        return ret\n\n    def __len__(self) -&gt; int:\n        return len(self.storage)\n\n    def __setstate__(self, storage) -&gt; None:\n        self.storage = storage\n\n    def __getstate__(self) -&gt; Sequence[Tensor]:\n        return self.storage\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _tensor(storage, batch_first) -&gt; Tensor:\n        return pad_sequence(storage, batch_first=batch_first)\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _mask(storage) -&gt; Tensor:\n        lens = torch.tensor([len(t) for t in storage], device=storage[0].device)\n        return torch.arange(max(lens), device=storage[0].device)[None, :] &lt; lens[:, None]  # pylint: disable=E1101\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _device(storage) -&gt; torch.device:\n        return storage[0].device\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _size(storage) -&gt; torch.Size:\n        return torch.Size(  # pylint: disable=E1101\n            [len(storage), max(t.shape[0] for t in storage), *storage[0].shape[1:]]\n        )\n</code></pre>","location":"en/package/#danling.NestedTensor--notes"},{"title":"<code>device()</code>  <code>property</code>","text":"<p>Device of the NestedTensor.</p> <p>Returns:</p>    Type Description      <code>torch.Tensor</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.device\ndevice(type='cpu')\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@property\ndef device(self) -&gt; torch.device:\n    r\"\"\"\n    Device of the NestedTensor.\n\n    Returns\n    -------\n    torch.Tensor\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.device\n    device(type='cpu')\n\n    ```\n    \"\"\"\n\n    return self._device(tuple(self.storage))\n</code></pre>","location":"en/package/#danling.tensors.nested_tensor.NestedTensor.device"},{"title":"<code>mask()</code>  <code>property</code>","text":"<p>Padding mask of <code>tensor</code>.</p> <p>Returns:</p>    Type Description      <code>torch.Tensor</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.mask\ntensor([[ True,  True,  True],\n        [ True,  True, False]])\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@property\ndef mask(self) -&gt; Tensor:\n    r\"\"\"\n    Padding mask of `tensor`.\n\n    Returns\n    -------\n    torch.Tensor\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.mask\n    tensor([[ True,  True,  True],\n            [ True,  True, False]])\n\n    ```\n    \"\"\"\n\n    return self._mask(tuple(self.storage))\n</code></pre>","location":"en/package/#danling.tensors.nested_tensor.NestedTensor.mask"},{"title":"<code>shape()</code>  <code>property</code>","text":"<p>Alias for <code>size</code>.</p> <p>Returns:</p>    Type Description      <code>torch.Size</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.storage.append(torch.tensor([6, 7, 8, 9]))\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([3, 4])\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@property\ndef shape(self) -&gt; torch.Size:\n    r\"\"\"\n    Alias for `size`.\n\n    Returns\n    -------\n    torch.Size\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.shape\n    torch.Size([2, 3])\n    &gt;&gt;&gt; nested_tensor.storage.append(torch.tensor([6, 7, 8, 9]))\n    &gt;&gt;&gt; nested_tensor.shape\n    torch.Size([3, 4])\n\n    ```\n    \"\"\"\n\n    return self.size()\n</code></pre>","location":"en/package/#danling.tensors.nested_tensor.NestedTensor.shape"},{"title":"<code>size()</code>","text":"<p>Shape of the NestedTensor.</p> <p>Returns:</p>    Type Description      <code>torch.Size</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.size()\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.storage[1] = torch.tensor([4, 5, 6, 7])\n&gt;&gt;&gt; nested_tensor.size()\ntorch.Size([2, 4])\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def size(self) -&gt; torch.Size:\n    r\"\"\"\n    Shape of the NestedTensor.\n\n    Returns\n    -------\n    torch.Size\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.size()\n    torch.Size([2, 3])\n    &gt;&gt;&gt; nested_tensor.storage[1] = torch.tensor([4, 5, 6, 7])\n    &gt;&gt;&gt; nested_tensor.size()\n    torch.Size([2, 4])\n\n    ```\n    \"\"\"\n\n    return self._size(tuple(self.storage))\n</code></pre>","location":"en/package/#danling.tensors.nested_tensor.NestedTensor.size"},{"title":"<code>tensor()</code>  <code>property</code>","text":"<p>Return a single tensor by padding all the tensors.</p> <p>Returns:</p>    Type Description      <code>torch.Tensor</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.tensor\ntensor([[1, 2, 3],\n        [4, 5, 0]])\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@property\ndef tensor(self) -&gt; Tensor:\n    r\"\"\"\n    Return a single tensor by padding all the tensors.\n\n    Returns\n    -------\n    torch.Tensor\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.tensor\n    tensor([[1, 2, 3],\n            [4, 5, 0]])\n\n    ```\n    \"\"\"\n\n    return self._tensor(tuple(self.storage), self.batch_first)\n</code></pre>","location":"en/package/#danling.tensors.nested_tensor.NestedTensor.tensor"},{"title":"<code>Registry</code>","text":"<p>         Bases: <code>NestedDict</code></p> <p><code>Registry</code> for components.</p>","location":"en/package/#danling.Registry"},{"title":"Notes","text":"<p><code>Registry</code> inherits from <code>NestedDict</code>.</p> <p>Therefore, <code>Registry</code> comes in a nested structure by nature. You could create a sub-registry by simply calling <code>registry.sub_registry = Registry</code>, and access through <code>registry.sub_registry.register()</code>.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; registry = Registry(\"test\")\n&gt;&gt;&gt; @registry.register\n... @registry.register(\"Module1\")\n... class Module:\n...     def __init__(self, a, b):\n...         self.a = a\n...         self.b = b\n&gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n&gt;&gt;&gt; registry\nRegistry(\n  (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n  (Module): &lt;class 'danling.registry.registry.Module'&gt;\n  (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n)\n&gt;&gt;&gt; registry.lookup(\"Module\")\n&lt;class 'danling.registry.registry.Module'&gt;\n&gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n&gt;&gt;&gt; # registry.register(Module)\n&gt;&gt;&gt; module = registry.build(config[\"module\"])\n&gt;&gt;&gt; type(module)\n&lt;class 'danling.registry.registry.Module'&gt;\n&gt;&gt;&gt; module.a\n1\n&gt;&gt;&gt; module.b\n2\n</code></pre>  Source code in <code>danling/registry/registry.py</code> Python<pre><code>class Registry(NestedDict):\n    \"\"\"\n    `Registry` for components.\n\n    Notes\n    -----\n\n    `Registry` inherits from [`NestedDict`](https://chanfig.danling.org/nested_dict/).\n\n    Therefore, `Registry` comes in a nested structure by nature.\n    You could create a sub-registry by simply calling `registry.sub_registry = Registry`,\n    and access through `registry.sub_registry.register()`.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; registry = Registry(\"test\")\n    &gt;&gt;&gt; @registry.register\n    ... @registry.register(\"Module1\")\n    ... class Module:\n    ...     def __init__(self, a, b):\n    ...         self.a = a\n    ...         self.b = b\n    &gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n    &gt;&gt;&gt; registry\n    Registry(\n      (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n      (Module): &lt;class 'danling.registry.registry.Module'&gt;\n      (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n    )\n    &gt;&gt;&gt; registry.lookup(\"Module\")\n    &lt;class 'danling.registry.registry.Module'&gt;\n    &gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n    &gt;&gt;&gt; # registry.register(Module)\n    &gt;&gt;&gt; module = registry.build(config[\"module\"])\n    &gt;&gt;&gt; type(module)\n    &lt;class 'danling.registry.registry.Module'&gt;\n    &gt;&gt;&gt; module.a\n    1\n    &gt;&gt;&gt; module.b\n    2\n\n    ```\n    \"\"\"\n\n    override: bool = False\n\n    def __init__(self, override: bool = False):\n        super().__init__()\n        self.setattr(\"override\", override)\n\n    def register(self, component: Optional[Callable] = None, name: Optional[str] = None) -&gt; Callable:\n        r\"\"\"\n        Register a new component\n\n        Parameters\n        ----------\n        component: Optional[Callable] = None\n            The component to register.\n        name: Optional[str] = component.__name__\n            The name of the component.\n\n        Returns\n        -------\n        component: Callable\n            The registered component.\n\n        Raises\n        ------\n        ValueError\n            If the component with the same name already exists and `Registry.override=False`.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; registry = Registry(\"test\")\n        &gt;&gt;&gt; @registry.register\n        ... @registry.register(\"Module1\")\n        ... class Module:\n        ...     def __init__(self, a, b):\n        ...         self.a = a\n        ...         self.b = b\n        &gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n        &gt;&gt;&gt; registry\n        Registry(\n          (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n          (Module): &lt;class 'danling.registry.registry.Module'&gt;\n          (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n        )\n\n        ```\n        \"\"\"\n\n        if isinstance(name, str) and name in self and not self.override:\n            raise ValueError(f\"Component with name {name} already exists\")\n\n        # Registry.register()\n        if name is not None:\n            self.set(name, component)\n\n        # @Registry.register()\n        @wraps(self.register)\n        def register(component, name=None):\n            if name is None:\n                name = component.__name__\n            self.set(name, component)\n            return component\n\n        # @Registry.register\n        if callable(component) and name is None:\n            return register(component)\n\n        return lambda x: register(x, component)\n\n    def lookup(self, name: str) -&gt; Any:\n        r\"\"\"\n        Lookup for a component.\n\n        Parameters\n        ----------\n        name: str\n            The name of the component.\n\n        Returns\n        -------\n        value: Any\n\n        Raises\n        ------\n        KeyError\n            If the component is not registered.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; registry = Registry(\"test\")\n        &gt;&gt;&gt; @registry.register\n        ... class Module:\n        ...     def __init__(self, a, b):\n        ...         self.a = a\n        ...         self.b = b\n        &gt;&gt;&gt; registry.lookup(\"Module\")\n        &lt;class 'danling.registry.registry.Module'&gt;\n\n        ```\n        \"\"\"\n\n        return self.get(name)\n\n    def build(self, name: str, *args, **kwargs):\n        r\"\"\"\n        Build a component.\n\n        Parameters\n        ----------\n        name: str\n            The name of the component.\n        *args\n            The arguments to pass to the component.\n        **kwargs\n            The keyword arguments to pass to the component.\n\n        Returns\n        -------\n        component: Callable\n\n        Raises\n        ------\n        KeyError\n            If the component is not registered.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; registry = Registry(\"test\")\n        &gt;&gt;&gt; @registry.register\n        ... class Module:\n        ...     def __init__(self, a, b):\n        ...         self.a = a\n        ...         self.b = b\n        &gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n        &gt;&gt;&gt; # registry.register(Module)\n        &gt;&gt;&gt; module = registry.build(config[\"module\"])\n        &gt;&gt;&gt; type(module)\n        &lt;class 'danling.registry.registry.Module'&gt;\n        &gt;&gt;&gt; module.a\n        1\n        &gt;&gt;&gt; module.b\n        2\n\n        ```\n        \"\"\"\n\n        if isinstance(name, Mapping) and not args and not kwargs:\n            name, kwargs = name.pop(\"name\"), name  # type: ignore\n        return self.get(name)(*args, **kwargs)\n\n    def __wrapped__(self):\n        pass\n</code></pre>","location":"en/package/#danling.Registry--notes"},{"title":"<code>build(name, *args, **kwargs)</code>","text":"<p>Build a component.</p> <p>Parameters:</p>    Name Type Description Default     <code>name</code>  <code>str</code>  <p>The name of the component.</p>  required    <code>*args</code>   <p>The arguments to pass to the component.</p>  <code>()</code>    <code>**kwargs</code>   <p>The keyword arguments to pass to the component.</p>  <code>{}</code>     <p>Returns:</p>    Name Type Description     <code>component</code>  <code>Callable</code>      <p>Raises:</p>    Type Description      <code>KeyError</code>  <p>If the component is not registered.</p>    <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; registry = Registry(\"test\")\n&gt;&gt;&gt; @registry.register\n... class Module:\n...     def __init__(self, a, b):\n...         self.a = a\n...         self.b = b\n&gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n&gt;&gt;&gt; # registry.register(Module)\n&gt;&gt;&gt; module = registry.build(config[\"module\"])\n&gt;&gt;&gt; type(module)\n&lt;class 'danling.registry.registry.Module'&gt;\n&gt;&gt;&gt; module.a\n1\n&gt;&gt;&gt; module.b\n2\n</code></pre>  Source code in <code>danling/registry/registry.py</code> Python<pre><code>def build(self, name: str, *args, **kwargs):\n    r\"\"\"\n    Build a component.\n\n    Parameters\n    ----------\n    name: str\n        The name of the component.\n    *args\n        The arguments to pass to the component.\n    **kwargs\n        The keyword arguments to pass to the component.\n\n    Returns\n    -------\n    component: Callable\n\n    Raises\n    ------\n    KeyError\n        If the component is not registered.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; registry = Registry(\"test\")\n    &gt;&gt;&gt; @registry.register\n    ... class Module:\n    ...     def __init__(self, a, b):\n    ...         self.a = a\n    ...         self.b = b\n    &gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n    &gt;&gt;&gt; # registry.register(Module)\n    &gt;&gt;&gt; module = registry.build(config[\"module\"])\n    &gt;&gt;&gt; type(module)\n    &lt;class 'danling.registry.registry.Module'&gt;\n    &gt;&gt;&gt; module.a\n    1\n    &gt;&gt;&gt; module.b\n    2\n\n    ```\n    \"\"\"\n\n    if isinstance(name, Mapping) and not args and not kwargs:\n        name, kwargs = name.pop(\"name\"), name  # type: ignore\n    return self.get(name)(*args, **kwargs)\n</code></pre>","location":"en/package/#danling.registry.registry.Registry.build"},{"title":"<code>lookup(name)</code>","text":"<p>Lookup for a component.</p> <p>Parameters:</p>    Name Type Description Default     <code>name</code>  <code>str</code>  <p>The name of the component.</p>  required     <p>Returns:</p>    Name Type Description     <code>value</code>  <code>Any</code>      <p>Raises:</p>    Type Description      <code>KeyError</code>  <p>If the component is not registered.</p>    <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; registry = Registry(\"test\")\n&gt;&gt;&gt; @registry.register\n... class Module:\n...     def __init__(self, a, b):\n...         self.a = a\n...         self.b = b\n&gt;&gt;&gt; registry.lookup(\"Module\")\n&lt;class 'danling.registry.registry.Module'&gt;\n</code></pre>  Source code in <code>danling/registry/registry.py</code> Python<pre><code>def lookup(self, name: str) -&gt; Any:\n    r\"\"\"\n    Lookup for a component.\n\n    Parameters\n    ----------\n    name: str\n        The name of the component.\n\n    Returns\n    -------\n    value: Any\n\n    Raises\n    ------\n    KeyError\n        If the component is not registered.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; registry = Registry(\"test\")\n    &gt;&gt;&gt; @registry.register\n    ... class Module:\n    ...     def __init__(self, a, b):\n    ...         self.a = a\n    ...         self.b = b\n    &gt;&gt;&gt; registry.lookup(\"Module\")\n    &lt;class 'danling.registry.registry.Module'&gt;\n\n    ```\n    \"\"\"\n\n    return self.get(name)\n</code></pre>","location":"en/package/#danling.registry.registry.Registry.lookup"},{"title":"<code>register(component=None, name=None)</code>","text":"<p>Register a new component</p> <p>Parameters:</p>    Name Type Description Default     <code>component</code>  <code>Optional[Callable]</code>  <p>The component to register.</p>  <code>None</code>    <code>name</code>  <code>Optional[str]</code>  <p>The name of the component.</p>  <code>None</code>     <p>Returns:</p>    Name Type Description     <code>component</code>  <code>Callable</code>  <p>The registered component.</p>    <p>Raises:</p>    Type Description      <code>ValueError</code>  <p>If the component with the same name already exists and <code>Registry.override=False</code>.</p>    <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; registry = Registry(\"test\")\n&gt;&gt;&gt; @registry.register\n... @registry.register(\"Module1\")\n... class Module:\n...     def __init__(self, a, b):\n...         self.a = a\n...         self.b = b\n&gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n&gt;&gt;&gt; registry\nRegistry(\n  (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n  (Module): &lt;class 'danling.registry.registry.Module'&gt;\n  (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n)\n</code></pre>  Source code in <code>danling/registry/registry.py</code> Python<pre><code>def register(self, component: Optional[Callable] = None, name: Optional[str] = None) -&gt; Callable:\n    r\"\"\"\n    Register a new component\n\n    Parameters\n    ----------\n    component: Optional[Callable] = None\n        The component to register.\n    name: Optional[str] = component.__name__\n        The name of the component.\n\n    Returns\n    -------\n    component: Callable\n        The registered component.\n\n    Raises\n    ------\n    ValueError\n        If the component with the same name already exists and `Registry.override=False`.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; registry = Registry(\"test\")\n    &gt;&gt;&gt; @registry.register\n    ... @registry.register(\"Module1\")\n    ... class Module:\n    ...     def __init__(self, a, b):\n    ...         self.a = a\n    ...         self.b = b\n    &gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n    &gt;&gt;&gt; registry\n    Registry(\n      (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n      (Module): &lt;class 'danling.registry.registry.Module'&gt;\n      (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n    )\n\n    ```\n    \"\"\"\n\n    if isinstance(name, str) and name in self and not self.override:\n        raise ValueError(f\"Component with name {name} already exists\")\n\n    # Registry.register()\n    if name is not None:\n        self.set(name, component)\n\n    # @Registry.register()\n    @wraps(self.register)\n    def register(component, name=None):\n        if name is None:\n            name = component.__name__\n        self.set(name, component)\n        return component\n\n    # @Registry.register\n    if callable(component) and name is None:\n        return register(component)\n\n    return lambda x: register(x, component)\n</code></pre>","location":"en/package/#danling.registry.registry.Registry.register"},{"title":"<code>SelfAttention</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Allows the model to jointly attend to information from different representation subspaces. See <code>Attention Is All You Need &lt;https://arxiv.org/abs/1706.03762&gt;</code>_ .. math::     \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O where :math:<code>head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)</code>. Args:     embed_dim: total dimension of the model.     num_heads: parallel attention heads.     dropout: a Dropout layer on attn_output_weights. Default: 0.0.     bias: add bias as module parameter. Default: True.     add_bias_kv: add bias to the key and value sequences at dim=0.     add_zero_attn: add a new batch of zeros to the key and                    value sequences at dim=1.     k_dim: total number of features in key. Default: None.     v_dim: total number of features in value. Default: None.     batch_first: If <code>False</code>, then the input and output tensors are provided         as (seq, batch, feature). Default: <code>True</code> (batch, seq, feature). Note that if :attr:<code>k_dim</code> and :attr:<code>v_dim</code> are None, they will be set to :attr:<code>embed_dim</code> such that query, key, and value have the same number of features. Examples::     &gt;&gt;&gt; multihead_attn = dl.models.SelfAttention(embed_dim, num_heads)     &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)</p>  Source code in <code>danling/models/transformer/attention/self_attention.py</code> Python<pre><code>class SelfAttention(nn.Module):\n    r\"\"\"Allows the model to jointly attend to information\n    from different representation subspaces.\n    See `Attention Is All You Need &lt;https://arxiv.org/abs/1706.03762&gt;`_\n    .. math::\n        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n    where :math:`head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n    Args:\n        embed_dim: total dimension of the model.\n        num_heads: parallel attention heads.\n        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n        bias: add bias as module parameter. Default: True.\n        add_bias_kv: add bias to the key and value sequences at dim=0.\n        add_zero_attn: add a new batch of zeros to the key and\n                       value sequences at dim=1.\n        k_dim: total number of features in key. Default: None.\n        v_dim: total number of features in value. Default: None.\n        batch_first: If ``False``, then the input and output tensors are provided\n            as (seq, batch, feature). Default: ``True`` (batch, seq, feature).\n    Note that if :attr:`k_dim` and :attr:`v_dim` are None, they will be set\n    to :attr:`embed_dim` such that query, key, and value have the same\n    number of features.\n    Examples::\n        &gt;&gt;&gt; multihead_attn = dl.models.SelfAttention(embed_dim, num_heads)\n        &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)\n    \"\"\"\n    __constants__ = [\"batch_first\"]\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        attn_dropout: float = 0.0,\n        scale_factor: float = 1.0,\n        bias: bool = True,\n        batch_first: bool = True,\n        **kwargs: Optional[Dict[str, Any]],\n    ) -&gt; None:\n        super(SelfAttention, self).__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.scale_factor = scale_factor\n        self.batch_first = batch_first\n        self.head_dim = self.embed_dim // self.num_heads\n        self.scaling = float(self.head_dim * self.scale_factor) ** -0.5\n        if not self.head_dim * self.num_heads == self.embed_dim:\n            raise ValueError(f\"embed_dim {self.embed_dim} not divisible by num_heads {self.num_heads}\")\n\n        self.in_proj = nn.Linear(self.embed_dim, self.embed_dim + self.k_dim + self.v_dim, bias=bias)\n        self.dropout = nn.Dropout(attn_dropout)\n        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=bias)\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        nn.init.xavier_uniform_(self.in_proj.weight)\n        if self.in_proj.bias is not None:\n            nn.init.constant_(self.in_proj.bias, 0.0)\n            nn.init.constant_(self.out_proj.bias, 0.0)\n\n    def forward(\n        self,\n        query: Tensor,\n        key: Tensor,\n        value: Tensor,\n        attn_bias: Optional[Tensor] = None,\n        attn_mask: Optional[Tensor] = None,\n        key_padding_mask: Optional[Tensor] = None,\n        need_weights: bool = False,\n    ) -&gt; Tuple[Tensor, Optional[Tensor]]:\n        r\"\"\"\n        Args:\n            query, key, value: map a query and a set of key-value pairs to an output.\n                See \"Attention Is All You Need\" for more details.\n            attn_bias: 2D or 3D mask that add bias to attention output weights. Used for relative positional embedding.\n                A 2D bias will be broadcasted for all the batches while a 3D mask allows to specify a different mask for\n                the entries of each batch.\n            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n                the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n            key_padding_mask: if provided, specified padding elements in the key will\n                be ignored by the attention. When given a binary mask and a value is True,\n                the corresponding value on the attention layer will be ignored. When given\n                a byte mask and a value is non-zero, the corresponding value on the attention\n                layer will be ignored\n            need_weights: output attn_output_weights.\n        Shapes for inputs:\n            - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n                the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n            - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n                the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n            - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n                the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n            - attn_bias: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n                source sequence length.\n                If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n                length, S is the source sequence length. ``attn_bias`` allows to pass pos embed directly into attention\n                If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will\n                be unchanged. If a BoolTensor is provided, positions with ``True`` is not allowed to attend while ``False``\n                values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight.\n            - attn_mask: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n                source sequence length.\n                If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n                length, S is the source sequence length. ``attn_mask`` ensure that position i is allowed to attend\n                the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n                while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n                is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n                is provided, it will be added to the attention weight.\n            - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n                If a ByteTensor is provided, the non-zero positions will be ignored while the position\n                with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the\n                value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n        Outputs:\n            - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n                E is the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n            - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n                L is the target sequence length, S is the source sequence length.\n        \"\"\"\n        if self.batch_first:\n            query, key, value = [x.transpose(0, 1) for x in (query, key, value)]\n\n        # set up shape vars\n        target_len, batch_size, embed_dim = query.shape\n        source_len, _, _ = key.shape\n        if not key.shape[:2] == value.shape[:2]:\n            raise ValueError(f\"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}\")\n\n        q, k, v = self.in_projection(query, key, value)\n\n        # prep attention mask\n        if attn_mask is not None:\n            if attn_mask.dtype == torch.uint8:\n                warnings.warn(\n                    \"attn_mask is of type uint8. This type is deprecated. Please use bool or float tensors instead.\"\n                )\n                attn_mask = attn_mask.to(torch.bool)\n            elif not (attn_mask.is_floating_point() or attn_mask.dtype == torch.bool):\n                raise ValueError(f\"attn_mask should have type float or bool, but got {attn_mask.dtype}.\")\n            # ensure attn_mask's dim is 3\n            if attn_mask.dim() == 2:\n                correct_shape = (target_len, source_len)\n                if attn_mask.shape != correct_shape:\n                    raise ValueError(f\"attn_mask should have shape {correct_shape}, but got {attn_mask.shape}.\")\n                attn_mask = attn_mask.unsqueeze(0)\n            elif attn_mask.dim() == 3:\n                correct_shape = (batch_size * self.num_heads, target_len, source_len)  # type: ignore\n                if attn_mask.shape != correct_shape:\n                    raise ValueError(f\"attn_mask should have shape {correct_shape}, but got {attn_mask.shape}.\")\n            else:\n                raise RuntimeError(f\"attn_mask should have dimension 2 or 3, bug got {attn_mask.dim()}.\")\n\n        # prep key padding mask\n        if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n            warnings.warn(\n                \"key_padding_mask is of type uint8. This type is deprecated. Please use bool or float tensors instead.\"\n            )\n            key_padding_mask = key_padding_mask.to(torch.bool)\n\n        # reshape q, k, v for multihead attention and make em batch first\n        q = q.reshape(target_len, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n        k = k.reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n        v = v.reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n\n        # merge key padding and attention masks\n        if key_padding_mask is not None:\n            if key_padding_mask.shape != (batch_size, source_len):\n                raise ValueError(\n                    f\"key_padding_mask should have shape {(batch_size, source_len)}, but got {key_padding_mask.shape}\"\n                )\n            key_padding_mask = (\n                key_padding_mask.view(batch_size, 1, 1, source_len)\n                .expand(-1, self.num_heads, -1, -1)\n                .reshape(batch_size * self.num_heads, 1, source_len)\n            )\n            if attn_mask is None:\n                attn_mask = key_padding_mask\n            elif attn_mask.dtype == torch.bool:\n                attn_mask = attn_mask.logical_or(key_padding_mask)\n            else:\n                attn_mask = attn_mask.masked_fill(key_padding_mask, float(\"-inf\"))\n\n        # convert mask to float\n        if attn_mask is not None and attn_mask.dtype == torch.bool:\n            new_attn_mask = torch.zeros_like(attn_mask, dtype=torch.float)\n            new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n            attn_mask = new_attn_mask\n\n        # (deep breath) calculate attention and out projection\n        attn_output, attn_output_weights = self.attention(q, k, v, attn_bias, attn_mask)\n        attn_output = attn_output.transpose(0, 1).reshape(target_len, batch_size, embed_dim)\n        attn_output = self.out_projection(attn_output)\n\n        # attn_output_weights is set to torch.empty(0, requires_grad=False) to avoid errors in DDP\n        attn_output_weights = (\n            attn_output_weights.view(batch_size, self.num_heads, target_len, source_len)\n            if need_weights\n            else torch.empty(0, requires_grad=False)\n        )\n\n        if self.batch_first:\n            return attn_output.transpose(0, 1), attn_output_weights\n        else:\n            return attn_output, attn_output_weights\n\n    def in_projection(self, q: Tensor, k: Tensor, v: Tensor) -&gt; Tuple[Tensor, Tensor, Tensor]:\n        r\"\"\"\n        Performs the in-projection step of the attention operation, using packed weights.\n        Output is a triple containing projection tensors for query, key and value.\n        Args:\n            q, k, v: query, key and value tensors to be projected. For self-attention,\n                these are typically the same tensor; for encoder-decoder attention,\n                k and v are typically the same tensor. (We take advantage of these\n                identities for performance if they are present.) Regardless, q, k and v\n                must share a common embedding dimension; otherwise their shapes may vary.\n        Shape:\n            Inputs:\n            - q: :math:`(..., E)` where E is the embedding dimension\n            - k: :math:`(..., E)` where E is the embedding dimension\n            - v: :math:`(..., E)` where E is the embedding dimension\n            Output:\n            - in output list :math:`[q', k', v']`, each output tensor will have the\n                same shape as the corresponding input tensor.\n        \"\"\"\n        if k is v:\n            # self-attention\n            if q is k:\n                return self.in_proj(q).split((self.embed_dim, self.k_dim, self.v_dim), dim=-1)\n            # encoder-decoder attention\n            else:\n                w_q, w_kv = self.in_proj.weight.split([self.embed_dim, self.k_dim + self.v_dim])\n                b_q, b_kv = (\n                    None\n                    if self.in_proj.bias is None\n                    else self.in_proj.bias.split([self.embed_dim, self.k_dim + self.v_dim])\n                )\n                return (F.linear(q, w_q, b_q),) + F.linear(k, w_kv, b_kv).split((self.k_dim, self.v_dim), dim=-1)\n        else:\n            w_q, w_k, w_v = self.in_proj.weight.split([self.embed_dim, self.k_dim, self.v_dim])\n            b_q, b_k, b_v = (\n                None if self.in_proj.bias is None else self.in_proj.bias.split([self.embed_dim, self.k_dim, self.v_dim])\n            )\n            return F.linear(q, w_q, b_q), F.linear(k, w_k, b_k), F.linear(v, w_v, b_v)\n\n    def attention(\n        self,\n        q: Tensor,\n        k: Tensor,\n        v: Tensor,\n        attn_bias: Optional[Tensor] = None,\n        attn_mask: Optional[Tensor] = None,\n    ) -&gt; Tuple[Tensor, Tensor]:\n        r\"\"\"\n        Computes scaled dot product attention on query, key and value tensors, using\n        an optional attention mask if passed, and applying dropout if a probability\n        greater than 0.0 is specified.\n        Returns a tensor pair containing attended values and attention weights.\n        Args:\n            q, k, v: query, key and value tensors. See Shape section for shape details.\n            attn_mask: optional tensor containing mask values to be added to calculated\n                attention. May be 2D or 3D; see Shape section for details.\n            attn_bias: optional tensor containing bias values to be added to calculated\n                attention. Used for relative positional embedding. May be 2D or 3D; see\n                Shape section for details.\n        Shape:\n            - q: :math:`(B, Nt, E)` where B is batch size, Nt is the target sequence length,\n                and E is embedding dimension.\n            - key: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n                and E is embedding dimension.\n            - value: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n                and E is embedding dimension.\n            - attn_bias: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n                shape :math:`(Nt, Ns)`.\n            - attn_mask: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n                shape :math:`(Nt, Ns)`.\n            - Output: attention values have shape :math:`(B, Nt, E)`; attention weights\n                have shape :math:`(B, Nt, Ns)`\n        \"\"\"\n        q *= self.scaling\n        # (B, Nt, E) x (B, E, Ns) -&gt; (B, Nt, Ns)\n        attn = torch.bmm(q, k.transpose(-2, -1))\n        if attn_bias is not None:\n            attn += attn_bias\n        if attn_mask is not None:\n            attn += attn_mask\n        attn = F.softmax(attn, dim=-1)\n        attn = self.dropout(attn)\n        # (B, Nt, Ns) x (B, Ns, E) -&gt; (B, Nt, E)\n        output = torch.bmm(attn, v)\n        return output, attn\n\n    def out_projection(self, attn_output: Tensor) -&gt; Tensor:\n        return self.out_proj(attn_output)\n</code></pre>","location":"en/package/#danling.SelfAttention"},{"title":"<code>attention(q, k, v, attn_bias=None, attn_mask=None)</code>","text":"<p>Computes scaled dot product attention on query, key and value tensors, using an optional attention mask if passed, and applying dropout if a probability greater than 0.0 is specified. Returns a tensor pair containing attended values and attention weights. Args:     q, k, v: query, key and value tensors. See Shape section for shape details.     attn_mask: optional tensor containing mask values to be added to calculated         attention. May be 2D or 3D; see Shape section for details.     attn_bias: optional tensor containing bias values to be added to calculated         attention. Used for relative positional embedding. May be 2D or 3D; see         Shape section for details. Shape:     - q: :math:<code>(B, Nt, E)</code> where B is batch size, Nt is the target sequence length,         and E is embedding dimension.     - key: :math:<code>(B, Ns, E)</code> where B is batch size, Ns is the source sequence length,         and E is embedding dimension.     - value: :math:<code>(B, Ns, E)</code> where B is batch size, Ns is the source sequence length,         and E is embedding dimension.     - attn_bias: either a 3D tensor of shape :math:<code>(B, Nt, Ns)</code> or a 2D tensor of         shape :math:<code>(Nt, Ns)</code>.     - attn_mask: either a 3D tensor of shape :math:<code>(B, Nt, Ns)</code> or a 2D tensor of         shape :math:<code>(Nt, Ns)</code>.     - Output: attention values have shape :math:<code>(B, Nt, E)</code>; attention weights         have shape :math:<code>(B, Nt, Ns)</code></p>  Source code in <code>danling/models/transformer/attention/self_attention.py</code> Python<pre><code>def attention(\n    self,\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    attn_bias: Optional[Tensor] = None,\n    attn_mask: Optional[Tensor] = None,\n) -&gt; Tuple[Tensor, Tensor]:\n    r\"\"\"\n    Computes scaled dot product attention on query, key and value tensors, using\n    an optional attention mask if passed, and applying dropout if a probability\n    greater than 0.0 is specified.\n    Returns a tensor pair containing attended values and attention weights.\n    Args:\n        q, k, v: query, key and value tensors. See Shape section for shape details.\n        attn_mask: optional tensor containing mask values to be added to calculated\n            attention. May be 2D or 3D; see Shape section for details.\n        attn_bias: optional tensor containing bias values to be added to calculated\n            attention. Used for relative positional embedding. May be 2D or 3D; see\n            Shape section for details.\n    Shape:\n        - q: :math:`(B, Nt, E)` where B is batch size, Nt is the target sequence length,\n            and E is embedding dimension.\n        - key: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n            and E is embedding dimension.\n        - value: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n            and E is embedding dimension.\n        - attn_bias: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n            shape :math:`(Nt, Ns)`.\n        - attn_mask: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n            shape :math:`(Nt, Ns)`.\n        - Output: attention values have shape :math:`(B, Nt, E)`; attention weights\n            have shape :math:`(B, Nt, Ns)`\n    \"\"\"\n    q *= self.scaling\n    # (B, Nt, E) x (B, E, Ns) -&gt; (B, Nt, Ns)\n    attn = torch.bmm(q, k.transpose(-2, -1))\n    if attn_bias is not None:\n        attn += attn_bias\n    if attn_mask is not None:\n        attn += attn_mask\n    attn = F.softmax(attn, dim=-1)\n    attn = self.dropout(attn)\n    # (B, Nt, Ns) x (B, Ns, E) -&gt; (B, Nt, E)\n    output = torch.bmm(attn, v)\n    return output, attn\n</code></pre>","location":"en/package/#danling.models.transformer.attention.self_attention.SelfAttention.attention"},{"title":"<code>forward(query, key, value, attn_bias=None, attn_mask=None, key_padding_mask=None, need_weights=False)</code>","text":"<p>Shapes for inputs:     - query: :math:<code>(L, N, E)</code> where L is the target sequence length, N is the batch size, E is         the embedding dimension. :math:<code>(N, L, E)</code> if <code>batch_first</code> is <code>True</code>.     - key: :math:<code>(S, N, E)</code>, where S is the source sequence length, N is the batch size, E is         the embedding dimension. :math:<code>(N, S, E)</code> if <code>batch_first</code> is <code>True</code>.     - value: :math:<code>(S, N, E)</code> where S is the source sequence length, N is the batch size, E is         the embedding dimension. :math:<code>(N, S, E)</code> if <code>batch_first</code> is <code>True</code>.     - attn_bias: if a 2D mask: :math:<code>(L, S)</code> where L is the target sequence length, S is the         source sequence length.         If a 3D mask: :math:<code>(N\\cdot\\text{num\\_heads}, L, S)</code> where N is the batch size, L is the target sequence         length, S is the source sequence length. <code>attn_bias</code> allows to pass pos embed directly into attention         If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will         be unchanged. If a BoolTensor is provided, positions with <code>True</code> is not allowed to attend while <code>False</code>         values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight.     - attn_mask: if a 2D mask: :math:<code>(L, S)</code> where L is the target sequence length, S is the         source sequence length.         If a 3D mask: :math:<code>(N\\cdot\\text{num\\_heads}, L, S)</code> where N is the batch size, L is the target sequence         length, S is the source sequence length. <code>attn_mask</code> ensure that position i is allowed to attend         the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend         while the zero positions will be unchanged. If a BoolTensor is provided, positions with <code>True</code>         is not allowed to attend while <code>False</code> values will be unchanged. If a FloatTensor         is provided, it will be added to the attention weight.     - key_padding_mask: :math:<code>(N, S)</code> where N is the batch size, S is the source sequence length.         If a ByteTensor is provided, the non-zero positions will be ignored while the position         with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the         value of <code>True</code> will be ignored while the position with the value of <code>False</code> will be unchanged. Outputs:     - attn_output: :math:<code>(L, N, E)</code> where L is the target sequence length, N is the batch size,         E is the embedding dimension. :math:<code>(N, L, E)</code> if <code>batch_first</code> is <code>True</code>.     - attn_output_weights: :math:<code>(N, L, S)</code> where N is the batch size,         L is the target sequence length, S is the source sequence length.</p>  Source code in <code>danling/models/transformer/attention/self_attention.py</code> Python<pre><code>def forward(\n    self,\n    query: Tensor,\n    key: Tensor,\n    value: Tensor,\n    attn_bias: Optional[Tensor] = None,\n    attn_mask: Optional[Tensor] = None,\n    key_padding_mask: Optional[Tensor] = None,\n    need_weights: bool = False,\n) -&gt; Tuple[Tensor, Optional[Tensor]]:\n    r\"\"\"\n    Args:\n        query, key, value: map a query and a set of key-value pairs to an output.\n            See \"Attention Is All You Need\" for more details.\n        attn_bias: 2D or 3D mask that add bias to attention output weights. Used for relative positional embedding.\n            A 2D bias will be broadcasted for all the batches while a 3D mask allows to specify a different mask for\n            the entries of each batch.\n        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n        key_padding_mask: if provided, specified padding elements in the key will\n            be ignored by the attention. When given a binary mask and a value is True,\n            the corresponding value on the attention layer will be ignored. When given\n            a byte mask and a value is non-zero, the corresponding value on the attention\n            layer will be ignored\n        need_weights: output attn_output_weights.\n    Shapes for inputs:\n        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n            the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n            the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n            the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n        - attn_bias: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n            source sequence length.\n            If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n            length, S is the source sequence length. ``attn_bias`` allows to pass pos embed directly into attention\n            If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will\n            be unchanged. If a BoolTensor is provided, positions with ``True`` is not allowed to attend while ``False``\n            values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight.\n        - attn_mask: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n            source sequence length.\n            If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n            length, S is the source sequence length. ``attn_mask`` ensure that position i is allowed to attend\n            the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n            while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n            is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n            is provided, it will be added to the attention weight.\n        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n            If a ByteTensor is provided, the non-zero positions will be ignored while the position\n            with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the\n            value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n    Outputs:\n        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n            E is the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n            L is the target sequence length, S is the source sequence length.\n    \"\"\"\n    if self.batch_first:\n        query, key, value = [x.transpose(0, 1) for x in (query, key, value)]\n\n    # set up shape vars\n    target_len, batch_size, embed_dim = query.shape\n    source_len, _, _ = key.shape\n    if not key.shape[:2] == value.shape[:2]:\n        raise ValueError(f\"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}\")\n\n    q, k, v = self.in_projection(query, key, value)\n\n    # prep attention mask\n    if attn_mask is not None:\n        if attn_mask.dtype == torch.uint8:\n            warnings.warn(\n                \"attn_mask is of type uint8. This type is deprecated. Please use bool or float tensors instead.\"\n            )\n            attn_mask = attn_mask.to(torch.bool)\n        elif not (attn_mask.is_floating_point() or attn_mask.dtype == torch.bool):\n            raise ValueError(f\"attn_mask should have type float or bool, but got {attn_mask.dtype}.\")\n        # ensure attn_mask's dim is 3\n        if attn_mask.dim() == 2:\n            correct_shape = (target_len, source_len)\n            if attn_mask.shape != correct_shape:\n                raise ValueError(f\"attn_mask should have shape {correct_shape}, but got {attn_mask.shape}.\")\n            attn_mask = attn_mask.unsqueeze(0)\n        elif attn_mask.dim() == 3:\n            correct_shape = (batch_size * self.num_heads, target_len, source_len)  # type: ignore\n            if attn_mask.shape != correct_shape:\n                raise ValueError(f\"attn_mask should have shape {correct_shape}, but got {attn_mask.shape}.\")\n        else:\n            raise RuntimeError(f\"attn_mask should have dimension 2 or 3, bug got {attn_mask.dim()}.\")\n\n    # prep key padding mask\n    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n        warnings.warn(\n            \"key_padding_mask is of type uint8. This type is deprecated. Please use bool or float tensors instead.\"\n        )\n        key_padding_mask = key_padding_mask.to(torch.bool)\n\n    # reshape q, k, v for multihead attention and make em batch first\n    q = q.reshape(target_len, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n    k = k.reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n    v = v.reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n\n    # merge key padding and attention masks\n    if key_padding_mask is not None:\n        if key_padding_mask.shape != (batch_size, source_len):\n            raise ValueError(\n                f\"key_padding_mask should have shape {(batch_size, source_len)}, but got {key_padding_mask.shape}\"\n            )\n        key_padding_mask = (\n            key_padding_mask.view(batch_size, 1, 1, source_len)\n            .expand(-1, self.num_heads, -1, -1)\n            .reshape(batch_size * self.num_heads, 1, source_len)\n        )\n        if attn_mask is None:\n            attn_mask = key_padding_mask\n        elif attn_mask.dtype == torch.bool:\n            attn_mask = attn_mask.logical_or(key_padding_mask)\n        else:\n            attn_mask = attn_mask.masked_fill(key_padding_mask, float(\"-inf\"))\n\n    # convert mask to float\n    if attn_mask is not None and attn_mask.dtype == torch.bool:\n        new_attn_mask = torch.zeros_like(attn_mask, dtype=torch.float)\n        new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n        attn_mask = new_attn_mask\n\n    # (deep breath) calculate attention and out projection\n    attn_output, attn_output_weights = self.attention(q, k, v, attn_bias, attn_mask)\n    attn_output = attn_output.transpose(0, 1).reshape(target_len, batch_size, embed_dim)\n    attn_output = self.out_projection(attn_output)\n\n    # attn_output_weights is set to torch.empty(0, requires_grad=False) to avoid errors in DDP\n    attn_output_weights = (\n        attn_output_weights.view(batch_size, self.num_heads, target_len, source_len)\n        if need_weights\n        else torch.empty(0, requires_grad=False)\n    )\n\n    if self.batch_first:\n        return attn_output.transpose(0, 1), attn_output_weights\n    else:\n        return attn_output, attn_output_weights\n</code></pre>","location":"en/package/#danling.models.transformer.attention.self_attention.SelfAttention.forward"},{"title":"<code>in_projection(q, k, v)</code>","text":"<p>Performs the in-projection step of the attention operation, using packed weights. Output is a triple containing projection tensors for query, key and value. Args:     q, k, v: query, key and value tensors to be projected. For self-attention,         these are typically the same tensor; for encoder-decoder attention,         k and v are typically the same tensor. (We take advantage of these         identities for performance if they are present.) Regardless, q, k and v         must share a common embedding dimension; otherwise their shapes may vary. Shape:     Inputs:     - q: :math:<code>(..., E)</code> where E is the embedding dimension     - k: :math:<code>(..., E)</code> where E is the embedding dimension     - v: :math:<code>(..., E)</code> where E is the embedding dimension     Output:     - in output list :math:<code>[q', k', v']</code>, each output tensor will have the         same shape as the corresponding input tensor.</p>  Source code in <code>danling/models/transformer/attention/self_attention.py</code> Python<pre><code>def in_projection(self, q: Tensor, k: Tensor, v: Tensor) -&gt; Tuple[Tensor, Tensor, Tensor]:\n    r\"\"\"\n    Performs the in-projection step of the attention operation, using packed weights.\n    Output is a triple containing projection tensors for query, key and value.\n    Args:\n        q, k, v: query, key and value tensors to be projected. For self-attention,\n            these are typically the same tensor; for encoder-decoder attention,\n            k and v are typically the same tensor. (We take advantage of these\n            identities for performance if they are present.) Regardless, q, k and v\n            must share a common embedding dimension; otherwise their shapes may vary.\n    Shape:\n        Inputs:\n        - q: :math:`(..., E)` where E is the embedding dimension\n        - k: :math:`(..., E)` where E is the embedding dimension\n        - v: :math:`(..., E)` where E is the embedding dimension\n        Output:\n        - in output list :math:`[q', k', v']`, each output tensor will have the\n            same shape as the corresponding input tensor.\n    \"\"\"\n    if k is v:\n        # self-attention\n        if q is k:\n            return self.in_proj(q).split((self.embed_dim, self.k_dim, self.v_dim), dim=-1)\n        # encoder-decoder attention\n        else:\n            w_q, w_kv = self.in_proj.weight.split([self.embed_dim, self.k_dim + self.v_dim])\n            b_q, b_kv = (\n                None\n                if self.in_proj.bias is None\n                else self.in_proj.bias.split([self.embed_dim, self.k_dim + self.v_dim])\n            )\n            return (F.linear(q, w_q, b_q),) + F.linear(k, w_kv, b_kv).split((self.k_dim, self.v_dim), dim=-1)\n    else:\n        w_q, w_k, w_v = self.in_proj.weight.split([self.embed_dim, self.k_dim, self.v_dim])\n        b_q, b_k, b_v = (\n            None if self.in_proj.bias is None else self.in_proj.bias.split([self.embed_dim, self.k_dim, self.v_dim])\n        )\n        return F.linear(q, w_q, b_q), F.linear(k, w_k, b_k), F.linear(v, w_v, b_v)\n</code></pre>","location":"en/package/#danling.models.transformer.attention.self_attention.SelfAttention.in_projection"},{"title":"<code>TorchRunner</code>","text":"<p>         Bases: <code>BaseRunner</code></p> <p>Set up everything for running a job.</p> <p>Attributes:</p>    Name Type Description     <code>accelerator</code>  <code>Accelerator</code>     <code>accelerate</code>  <code>Dict[str, Any] = {}</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>class TorchRunner(BaseRunner):\n    r\"\"\"\n    Set up everything for running a job.\n\n    Attributes\n    ----------\n    accelerator: Accelerator\n    accelerate: Dict[str, Any] = {}\n    \"\"\"\n\n    # pylint: disable=R0902\n\n    accelerator: Accelerator\n    accelerate: Dict[str, Any] = {}\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        self.accelerator = Accelerator(**self.accelerate)\n        super().__init__(*args, **kwargs)\n\n    @on_main_process\n    def init_tensorboard(self, *args, **kwargs) -&gt; None:\n        r\"\"\"\n        Set up Tensoraoard SummaryWriter.\n        \"\"\"\n        from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n        if \"log_dir\" not in kwargs:\n            kwargs[\"log_dir\"] = self.dir\n\n        self.writer = SummaryWriter(*args, **kwargs)\n\n    def set_seed(self, bias: Optional[int] = None) -&gt; None:\n        r\"\"\"\n        Set up random seed.\n\n        Parameters\n        ----------\n        bias: Optional[int] = self.rank\n            Make the seed different for each processes.\n            This avoids same data augmentation are applied on every processes.\n            Set to `False` to disable this feature.\n        \"\"\"\n\n        if self.distributed:\n            # TODO: use broadcast_object instead.\n            self.seed = (\n                self.accelerator.gather(torch.tensor(self.seed).cuda()).unsqueeze(0).flatten()[0]  # pylint: disable=E1101\n            )\n        if bias is None:\n            bias = self.rank\n        seed = self.seed + bias if bias else self.seed\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        np.random.seed(seed)\n        random.seed(seed)\n\n    def set_deterministic(self) -&gt; None:\n        r\"\"\"\n        Set up deterministic.\n        \"\"\"\n\n        cudnn.benchmark = False\n        cudnn.deterministic = True\n        if torch.__version__ &gt;= \"1.8.0\":\n            torch.use_deterministic_algorithms(True)\n\n    def init_distributed(self) -&gt; None:\n        r\"\"\"\n        Set up distributed training.\n\n        Initialise process group and set up DDP variables.\n\n        .. deprecated:: 0.1.0\n            `init_distributed` is deprecated in favor of `Accelerator()`.\n        \"\"\"\n\n        # pylint: disable=W0201\n\n        dist.init_process_group(backend=\"nccl\")\n        self.rank = dist.get_rank()\n        self.world_size = dist.get_world_size()\n        self.local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n        torch.cuda.set_device(self.local_rank)\n        self.is_main_process = self.rank == 0\n        self.is_local_main_process = self.local_rank == 0\n\n    @catch\n    def save(self, obj: Any, f: File, main_process_only: bool = True) -&gt; File:  # pylint: disable=C0103\n        r\"\"\"\n        Save object to a path or file.\n        Returns\n        -------\n        File\n        \"\"\"\n\n        if main_process_only and self.is_main_process or not on_main_process:\n            self.accelerator.save(obj, f)\n        return f\n\n    @staticmethod\n    def load(f: File, *args, **kwargs) -&gt; Any:  # pylint: disable=C0103\n        r\"\"\"\n        Load object from a path or file.\n        Returns\n        -------\n        Any\n        \"\"\"\n\n        return torch.load(f, *args, **kwargs)  # type: ignore\n\n    @property\n    def world_size(self) -&gt; int:\n        r\"\"\"\n        Number of Processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return self.accelerator.num_processes\n\n    @property\n    def rank(self) -&gt; int:\n        r\"\"\"\n        Process index in all processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return self.accelerator.process_index\n\n    @property\n    def local_rank(self) -&gt; int:\n        r\"\"\"\n        Process index in local processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return self.accelerator.local_process_index\n</code></pre>","location":"en/package/#danling.TorchRunner"},{"title":"<code>init_distributed()</code>","text":"<p>Set up distributed training.</p> <p>Initialise process group and set up DDP variables.</p> <p>.. deprecated:: 0.1.0     <code>init_distributed</code> is deprecated in favor of <code>Accelerator()</code>.</p>  Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def init_distributed(self) -&gt; None:\n    r\"\"\"\n    Set up distributed training.\n\n    Initialise process group and set up DDP variables.\n\n    .. deprecated:: 0.1.0\n        `init_distributed` is deprecated in favor of `Accelerator()`.\n    \"\"\"\n\n    # pylint: disable=W0201\n\n    dist.init_process_group(backend=\"nccl\")\n    self.rank = dist.get_rank()\n    self.world_size = dist.get_world_size()\n    self.local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n    torch.cuda.set_device(self.local_rank)\n    self.is_main_process = self.rank == 0\n    self.is_local_main_process = self.local_rank == 0\n</code></pre>","location":"en/package/#danling.runner.torch_runner.TorchRunner.init_distributed"},{"title":"<code>init_tensorboard(*args, **kwargs)</code>","text":"<p>Set up Tensoraoard SummaryWriter.</p>  Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@on_main_process\ndef init_tensorboard(self, *args, **kwargs) -&gt; None:\n    r\"\"\"\n    Set up Tensoraoard SummaryWriter.\n    \"\"\"\n    from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n    if \"log_dir\" not in kwargs:\n        kwargs[\"log_dir\"] = self.dir\n\n    self.writer = SummaryWriter(*args, **kwargs)\n</code></pre>","location":"en/package/#danling.runner.torch_runner.TorchRunner.init_tensorboard"},{"title":"<code>load(f, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Load object from a path or file.</p> <p>Returns:</p>    Type Description      <code>Any</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@staticmethod\ndef load(f: File, *args, **kwargs) -&gt; Any:  # pylint: disable=C0103\n    r\"\"\"\n    Load object from a path or file.\n    Returns\n    -------\n    Any\n    \"\"\"\n\n    return torch.load(f, *args, **kwargs)  # type: ignore\n</code></pre>","location":"en/package/#danling.runner.torch_runner.TorchRunner.load"},{"title":"<code>local_rank()</code>  <code>property</code>","text":"<p>Process index in local processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@property\ndef local_rank(self) -&gt; int:\n    r\"\"\"\n    Process index in local processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return self.accelerator.local_process_index\n</code></pre>","location":"en/package/#danling.runner.torch_runner.TorchRunner.local_rank"},{"title":"<code>rank()</code>  <code>property</code>","text":"<p>Process index in all processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@property\ndef rank(self) -&gt; int:\n    r\"\"\"\n    Process index in all processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return self.accelerator.process_index\n</code></pre>","location":"en/package/#danling.runner.torch_runner.TorchRunner.rank"},{"title":"<code>save(obj, f, main_process_only=True)</code>","text":"<p>Save object to a path or file.</p> <p>Returns:</p>    Type Description      <code>File</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@catch\ndef save(self, obj: Any, f: File, main_process_only: bool = True) -&gt; File:  # pylint: disable=C0103\n    r\"\"\"\n    Save object to a path or file.\n    Returns\n    -------\n    File\n    \"\"\"\n\n    if main_process_only and self.is_main_process or not on_main_process:\n        self.accelerator.save(obj, f)\n    return f\n</code></pre>","location":"en/package/#danling.runner.torch_runner.TorchRunner.save"},{"title":"<code>set_deterministic()</code>","text":"<p>Set up deterministic.</p>  Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def set_deterministic(self) -&gt; None:\n    r\"\"\"\n    Set up deterministic.\n    \"\"\"\n\n    cudnn.benchmark = False\n    cudnn.deterministic = True\n    if torch.__version__ &gt;= \"1.8.0\":\n        torch.use_deterministic_algorithms(True)\n</code></pre>","location":"en/package/#danling.runner.torch_runner.TorchRunner.set_deterministic"},{"title":"<code>set_seed(bias=None)</code>","text":"<p>Set up random seed.</p> <p>Parameters:</p>    Name Type Description Default     <code>bias</code>  <code>Optional[int]</code>  <p>Make the seed different for each processes. This avoids same data augmentation are applied on every processes. Set to <code>False</code> to disable this feature.</p>  <code>None</code>      Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def set_seed(self, bias: Optional[int] = None) -&gt; None:\n    r\"\"\"\n    Set up random seed.\n\n    Parameters\n    ----------\n    bias: Optional[int] = self.rank\n        Make the seed different for each processes.\n        This avoids same data augmentation are applied on every processes.\n        Set to `False` to disable this feature.\n    \"\"\"\n\n    if self.distributed:\n        # TODO: use broadcast_object instead.\n        self.seed = (\n            self.accelerator.gather(torch.tensor(self.seed).cuda()).unsqueeze(0).flatten()[0]  # pylint: disable=E1101\n        )\n    if bias is None:\n        bias = self.rank\n    seed = self.seed + bias if bias else self.seed\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n</code></pre>","location":"en/package/#danling.runner.torch_runner.TorchRunner.set_seed"},{"title":"<code>world_size()</code>  <code>property</code>","text":"<p>Number of Processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@property\ndef world_size(self) -&gt; int:\n    r\"\"\"\n    Number of Processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return self.accelerator.num_processes\n</code></pre>","location":"en/package/#danling.runner.torch_runner.TorchRunner.world_size"},{"title":"<code>TransformerDecoder</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>TransformerDecoder is a stack of N decoder layers Args:     num_layers: the number of sub-decoder-layers in the decoder (required).     layer: the sub-decoder-layer in the decoder (default=TransformerDecoderLayer).     drop_layer: the drop layer rate (default=0.0). Examples::     &gt;&gt;&gt; transformer_decoder = dl.model.TransformerDecoder(num_layers=6)     &gt;&gt;&gt; src = torch.rand(10, 32, 512)     &gt;&gt;&gt; out = transformer_decoder(src)</p>  Source code in <code>danling/models/transformer/decoder.py</code> Python<pre><code>class TransformerDecoder(nn.Module):\n    r\"\"\"TransformerDecoder is a stack of N decoder layers\n    Args:\n        num_layers: the number of sub-decoder-layers in the decoder (required).\n        layer: the sub-decoder-layer in the decoder (default=TransformerDecoderLayer).\n        drop_layer: the drop layer rate (default=0.0).\n    Examples::\n        &gt;&gt;&gt; transformer_decoder = dl.model.TransformerDecoder(num_layers=6)\n        &gt;&gt;&gt; src = torch.rand(10, 32, 512)\n        &gt;&gt;&gt; out = transformer_decoder(src)\n    \"\"\"\n    __constants__ = [\"norm\"]\n\n    def __init__(self, layer: TransformerDecoderLayer, num_layers: int = 6, **kwargs: Optional[Dict[str, Any]]) -&gt; None:\n        super().__init__()\n        self.num_layers = num_layers\n        self.layers = nn.ModuleList([])\n        self.layers.extend([layer for _ in range(self.num_layers)])\n\n    def forward(\n        self,\n        tgt: Tensor,\n        mem: Tensor,\n        tgt_bias: Optional[Tensor] = None,\n        tgt_mask: Optional[Tensor] = None,\n        tgt_key_padding_mask: Optional[Tensor] = None,\n        mem_bias: Optional[Tensor] = None,\n        mem_mask: Optional[Tensor] = None,\n        mem_key_padding_mask: Optional[Tensor] = None,\n        need_weights: bool = False,\n        gradient_checkpoint: bool = False,\n    ) -&gt; Tensor:\n        r\"\"\"Pass the input through the decoder layers in turn.\n        Args:\n            src: the sequence to the decoder (required).\n            attn_mask: the mask for the src sequence (optional).\n            key_padding_mask: the mask for the src keys per batch (optional).\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"\n        output = tgt\n        # attn_weights is set to torch.empty(0, requires_grad=False) to avoid errors in DDP\n        attn_weights = [] if need_weights else torch.empty(0, requires_grad=False)\n\n        for layer in self.layers:\n            if gradient_checkpoint and self.training:\n                layer = partial(checkpoint, layer)\n                need_weights = torch.tensor(need_weights)\n            output, weights = layer(\n                output,\n                mem,\n                tgt_bias,\n                tgt_mask,\n                tgt_key_padding_mask,\n                mem_bias,\n                mem_mask,\n                mem_key_padding_mask,\n                need_weights,\n            )\n            if need_weights:\n                attn_weights.append(weights)  # type: ignore\n\n        if need_weights:\n            attn_weights = torch.stack(attn_weights).cpu().detach()  # type: ignore\n\n        return output, attn_weights\n</code></pre>","location":"en/package/#danling.TransformerDecoder"},{"title":"<code>forward(tgt, mem, tgt_bias=None, tgt_mask=None, tgt_key_padding_mask=None, mem_bias=None, mem_mask=None, mem_key_padding_mask=None, need_weights=False, gradient_checkpoint=False)</code>","text":"<p>Pass the input through the decoder layers in turn. Args:     src: the sequence to the decoder (required).     attn_mask: the mask for the src sequence (optional).     key_padding_mask: the mask for the src keys per batch (optional). Shape:     see the docs in Transformer class.</p>  Source code in <code>danling/models/transformer/decoder.py</code> Python<pre><code>def forward(\n    self,\n    tgt: Tensor,\n    mem: Tensor,\n    tgt_bias: Optional[Tensor] = None,\n    tgt_mask: Optional[Tensor] = None,\n    tgt_key_padding_mask: Optional[Tensor] = None,\n    mem_bias: Optional[Tensor] = None,\n    mem_mask: Optional[Tensor] = None,\n    mem_key_padding_mask: Optional[Tensor] = None,\n    need_weights: bool = False,\n    gradient_checkpoint: bool = False,\n) -&gt; Tensor:\n    r\"\"\"Pass the input through the decoder layers in turn.\n    Args:\n        src: the sequence to the decoder (required).\n        attn_mask: the mask for the src sequence (optional).\n        key_padding_mask: the mask for the src keys per batch (optional).\n    Shape:\n        see the docs in Transformer class.\n    \"\"\"\n    output = tgt\n    # attn_weights is set to torch.empty(0, requires_grad=False) to avoid errors in DDP\n    attn_weights = [] if need_weights else torch.empty(0, requires_grad=False)\n\n    for layer in self.layers:\n        if gradient_checkpoint and self.training:\n            layer = partial(checkpoint, layer)\n            need_weights = torch.tensor(need_weights)\n        output, weights = layer(\n            output,\n            mem,\n            tgt_bias,\n            tgt_mask,\n            tgt_key_padding_mask,\n            mem_bias,\n            mem_mask,\n            mem_key_padding_mask,\n            need_weights,\n        )\n        if need_weights:\n            attn_weights.append(weights)  # type: ignore\n\n    if need_weights:\n        attn_weights = torch.stack(attn_weights).cpu().detach()  # type: ignore\n\n    return output, attn_weights\n</code></pre>","location":"en/package/#danling.models.transformer.decoder.TransformerDecoder.forward"},{"title":"<code>TransformerDecoderLayer</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>TransformerDecoderLayer is made up of self-attn and feedforward network. This standard decoder layer is based on the paper \u201cAttention Is All You Need\u201d. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application. Args:     embed_dim: the number of expected features in the input (required).     num_heads: the number of heads in the multi head attention models (required).     ffn_dim: the dimension of the feedforward network model (default=embed_dim*4).     dropout: the dropout value (default=0.1).     activation: the activation function of intermediate layer, relu or gelu (default=relu).     layer_norm_eps: the eps value in layer normalization components (default=1e-5).     batch_first: If <code>True</code>, then the input and output tensors are provided         as (batch, seq, feature). Default: <code>False</code>.     norm_first: if <code>True</code>, layer norm is done prior to attention and feedforward         operations, respectivaly. Otherwise it\u2019s done after. Default: <code>False</code> (after). Examples::     &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(embed_dim=512, num_heads=8)     &gt;&gt;&gt; src = torch.rand(10, 32, 512)     &gt;&gt;&gt; out = decoder_layer(src) Alternatively, when <code>batch_first</code> is <code>True</code>:     &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(embed_dim=512, num_heads=8, batch_first=True)     &gt;&gt;&gt; src = torch.rand(32, 10, 512)     &gt;&gt;&gt; out = decoder_layer(src)</p>  Source code in <code>danling/models/transformer/decoder.py</code> Python<pre><code>class TransformerDecoderLayer(nn.Module):\n    r\"\"\"TransformerDecoderLayer is made up of self-attn and feedforward network.\n    This standard decoder layer is based on the paper \"Attention Is All You Need\".\n    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n    in a different way during application.\n    Args:\n        embed_dim: the number of expected features in the input (required).\n        num_heads: the number of heads in the multi head attention models (required).\n        ffn_dim: the dimension of the feedforward network model (default=embed_dim*4).\n        dropout: the dropout value (default=0.1).\n        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n        batch_first: If ``True``, then the input and output tensors are provided\n            as (batch, seq, feature). Default: ``False``.\n        norm_first: if ``True``, layer norm is done prior to attention and feedforward\n            operations, respectivaly. Otherwise it's done after. Default: ``False`` (after).\n    Examples::\n        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(embed_dim=512, num_heads=8)\n        &gt;&gt;&gt; src = torch.rand(10, 32, 512)\n        &gt;&gt;&gt; out = decoder_layer(src)\n    Alternatively, when ``batch_first`` is ``True``:\n        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(embed_dim=512, num_heads=8, batch_first=True)\n        &gt;&gt;&gt; src = torch.rand(32, 10, 512)\n        &gt;&gt;&gt; out = decoder_layer(src)\n    \"\"\"\n    __constants__ = [\"batch_first\", \"norm_first\"]\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        ffn_dim: Optional[int] = None,\n        dropout: float = 0.1,\n        attn_dropout: float = 0.1,\n        ffn_dropout: float = 0.1,\n        activation: str = \"GELU\",\n        layer_norm_eps: float = 1e-5,\n        scale_factor: float = 1.0,\n        bias: bool = True,\n        add_bias_kv: bool = False,\n        add_zero_attn: bool = False,\n        batch_first: bool = True,\n        norm_first: bool = False,\n        Attention: nn.Module = MultiHeadAttention,\n        FeedForwardNetwork: nn.Module = FullyConnectedNetwork,\n        **kwargs: Optional[Dict[str, Any]]\n    ) -&gt; None:\n        super().__init__()\n        if ffn_dim is None:\n            ffn_dim = embed_dim * 4\n        self.norm_first = norm_first\n        self.self_attn = Attention(\n            embed_dim,\n            num_heads,\n            attn_dropout=attn_dropout,\n            scale_factor=scale_factor,\n            bias=bias,\n            add_bias_kv=add_bias_kv,\n            add_zero_attn=add_zero_attn,\n            batch_first=batch_first,\n            **kwargs\n        )\n        self.cross_attn = Attention(\n            embed_dim,\n            num_heads,\n            attn_dropout=attn_dropout,\n            scale_factor=scale_factor,\n            bias=bias,\n            add_bias_kv=add_bias_kv,\n            add_zero_attn=add_zero_attn,\n            batch_first=batch_first,\n            **kwargs\n        )\n        self.norm1 = nn.LayerNorm(embed_dim, eps=layer_norm_eps)\n        self.ffn = FeedForwardNetwork(embed_dim, ffn_dim, activation, ffn_dropout, **kwargs)\n        self.norm2 = nn.LayerNorm(embed_dim, eps=layer_norm_eps)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(\n        self,\n        tgt: Tensor,\n        mem: Tensor,\n        tgt_bias: Optional[Tensor] = None,\n        tgt_mask: Optional[Tensor] = None,\n        tgt_key_padding_mask: Optional[Tensor] = None,\n        mem_bias: Optional[Tensor] = None,\n        mem_mask: Optional[Tensor] = None,\n        mem_key_padding_mask: Optional[Tensor] = None,\n        need_weights: bool = False,\n    ) -&gt; Tensor:\n        r\"\"\"Pass the input through the decoder layer.\n        Args:\n            src: the sequence to the decoder layer (required).\n            attn_mask: the mask for the src sequence (optional).\n            key_padding_mask: the mask for the src keys per batch (optional).\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"\n\n        if self.norm_first:\n            tgt = self.norm1(tgt)\n\n        self_attn, weights = self.self_attn(\n            tgt,\n            tgt,\n            tgt,\n            attn_bias=tgt_bias,\n            attn_mask=tgt_mask,\n            key_padding_mask=tgt_key_padding_mask,\n            need_weights=need_weights,\n        )\n        self_attn = tgt + self.dropout(self_attn)\n        cross_attn, weights = self.cross_attn(\n            self_attn,\n            mem,\n            mem,\n            attn_bias=mem_bias,\n            attn_mask=mem_mask,\n            key_padding_mask=mem_key_padding_mask,\n            need_weights=need_weights,\n        )\n        cross_attn = self_attn + self.dropout(cross_attn)\n        attn = self.norm1(cross_attn) if not self.norm_first else self.norm2(cross_attn)\n\n        ffn = self.ffn(attn)\n        ffn = attn + self.dropout(ffn)\n\n        if not self.norm_first:\n            ffn = self.norm2(ffn)\n\n        return ffn, weights\n</code></pre>","location":"en/package/#danling.TransformerDecoderLayer"},{"title":"<code>forward(tgt, mem, tgt_bias=None, tgt_mask=None, tgt_key_padding_mask=None, mem_bias=None, mem_mask=None, mem_key_padding_mask=None, need_weights=False)</code>","text":"<p>Pass the input through the decoder layer. Args:     src: the sequence to the decoder layer (required).     attn_mask: the mask for the src sequence (optional).     key_padding_mask: the mask for the src keys per batch (optional). Shape:     see the docs in Transformer class.</p>  Source code in <code>danling/models/transformer/decoder.py</code> Python<pre><code>def forward(\n    self,\n    tgt: Tensor,\n    mem: Tensor,\n    tgt_bias: Optional[Tensor] = None,\n    tgt_mask: Optional[Tensor] = None,\n    tgt_key_padding_mask: Optional[Tensor] = None,\n    mem_bias: Optional[Tensor] = None,\n    mem_mask: Optional[Tensor] = None,\n    mem_key_padding_mask: Optional[Tensor] = None,\n    need_weights: bool = False,\n) -&gt; Tensor:\n    r\"\"\"Pass the input through the decoder layer.\n    Args:\n        src: the sequence to the decoder layer (required).\n        attn_mask: the mask for the src sequence (optional).\n        key_padding_mask: the mask for the src keys per batch (optional).\n    Shape:\n        see the docs in Transformer class.\n    \"\"\"\n\n    if self.norm_first:\n        tgt = self.norm1(tgt)\n\n    self_attn, weights = self.self_attn(\n        tgt,\n        tgt,\n        tgt,\n        attn_bias=tgt_bias,\n        attn_mask=tgt_mask,\n        key_padding_mask=tgt_key_padding_mask,\n        need_weights=need_weights,\n    )\n    self_attn = tgt + self.dropout(self_attn)\n    cross_attn, weights = self.cross_attn(\n        self_attn,\n        mem,\n        mem,\n        attn_bias=mem_bias,\n        attn_mask=mem_mask,\n        key_padding_mask=mem_key_padding_mask,\n        need_weights=need_weights,\n    )\n    cross_attn = self_attn + self.dropout(cross_attn)\n    attn = self.norm1(cross_attn) if not self.norm_first else self.norm2(cross_attn)\n\n    ffn = self.ffn(attn)\n    ffn = attn + self.dropout(ffn)\n\n    if not self.norm_first:\n        ffn = self.norm2(ffn)\n\n    return ffn, weights\n</code></pre>","location":"en/package/#danling.models.transformer.decoder.TransformerDecoderLayer.forward"},{"title":"<code>TransformerEncoder</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>TransformerEncoder is a stack of N encoder layers Args:     num_layers: the number of sub-encoder-layers in the encoder (required).     layer: the sub-encoder-layer in the encoder (default=TransformerEncoderLayer).     drop_layer: the drop layer rate (default=0.0). Examples::     &gt;&gt;&gt; transformer_encoder = dl.model.TransformerEncoder(num_layers=6)     &gt;&gt;&gt; src = torch.rand(10, 32, 512)     &gt;&gt;&gt; out = transformer_encoder(src)</p>  Source code in <code>danling/models/transformer/encoder.py</code> Python<pre><code>class TransformerEncoder(nn.Module):\n    r\"\"\"TransformerEncoder is a stack of N encoder layers\n    Args:\n        num_layers: the number of sub-encoder-layers in the encoder (required).\n        layer: the sub-encoder-layer in the encoder (default=TransformerEncoderLayer).\n        drop_layer: the drop layer rate (default=0.0).\n    Examples::\n        &gt;&gt;&gt; transformer_encoder = dl.model.TransformerEncoder(num_layers=6)\n        &gt;&gt;&gt; src = torch.rand(10, 32, 512)\n        &gt;&gt;&gt; out = transformer_encoder(src)\n    \"\"\"\n    __constants__ = [\"norm\"]\n\n    def __init__(self, layer: TransformerEncoderLayer, num_layers: int = 6, **kwargs: Optional[Dict[str, Any]]) -&gt; None:\n        super().__init__()\n        self.num_layers = num_layers\n        self.layers = nn.ModuleList([])\n        self.layers.extend([layer for _ in range(self.num_layers)])\n\n    def forward(\n        self,\n        src: Tensor,\n        attn_bias: Optional[Tensor] = None,\n        attn_mask: Optional[Tensor] = None,\n        key_padding_mask: Optional[Tensor] = None,\n        need_weights: bool = False,\n        gradient_checkpoint: bool = False,\n    ) -&gt; Tensor:\n        r\"\"\"Pass the input through the encoder layers in turn.\n        Args:\n            src: the sequence to the encoder (required).\n            attn_mask: the mask for the src sequence (optional).\n            key_padding_mask: the mask for the src keys per batch (optional).\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"\n        output = src\n        # attn_weights is set to torch.empty(0, requires_grad=False) to avoid errors in DDP\n        attn_weights = [] if need_weights else torch.empty(0, requires_grad=False)\n\n        for layer in self.layers:\n            if gradient_checkpoint and self.training:\n                layer = partial(checkpoint, layer)\n                need_weights = torch.tensor(need_weights)\n            output, weights = layer(output, attn_bias, attn_mask, key_padding_mask, need_weights)\n            if need_weights:\n                attn_weights.append(weights)  # type: ignore\n\n        if need_weights:\n            attn_weights = torch.stack(attn_weights).cpu().detach()  # type: ignore\n\n        return output, attn_weights\n</code></pre>","location":"en/package/#danling.TransformerEncoder"},{"title":"<code>forward(src, attn_bias=None, attn_mask=None, key_padding_mask=None, need_weights=False, gradient_checkpoint=False)</code>","text":"<p>Pass the input through the encoder layers in turn. Args:     src: the sequence to the encoder (required).     attn_mask: the mask for the src sequence (optional).     key_padding_mask: the mask for the src keys per batch (optional). Shape:     see the docs in Transformer class.</p>  Source code in <code>danling/models/transformer/encoder.py</code> Python<pre><code>def forward(\n    self,\n    src: Tensor,\n    attn_bias: Optional[Tensor] = None,\n    attn_mask: Optional[Tensor] = None,\n    key_padding_mask: Optional[Tensor] = None,\n    need_weights: bool = False,\n    gradient_checkpoint: bool = False,\n) -&gt; Tensor:\n    r\"\"\"Pass the input through the encoder layers in turn.\n    Args:\n        src: the sequence to the encoder (required).\n        attn_mask: the mask for the src sequence (optional).\n        key_padding_mask: the mask for the src keys per batch (optional).\n    Shape:\n        see the docs in Transformer class.\n    \"\"\"\n    output = src\n    # attn_weights is set to torch.empty(0, requires_grad=False) to avoid errors in DDP\n    attn_weights = [] if need_weights else torch.empty(0, requires_grad=False)\n\n    for layer in self.layers:\n        if gradient_checkpoint and self.training:\n            layer = partial(checkpoint, layer)\n            need_weights = torch.tensor(need_weights)\n        output, weights = layer(output, attn_bias, attn_mask, key_padding_mask, need_weights)\n        if need_weights:\n            attn_weights.append(weights)  # type: ignore\n\n    if need_weights:\n        attn_weights = torch.stack(attn_weights).cpu().detach()  # type: ignore\n\n    return output, attn_weights\n</code></pre>","location":"en/package/#danling.models.transformer.encoder.TransformerEncoder.forward"},{"title":"<code>TransformerEncoderLayer</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>TransformerEncoderLayer is made up of self-attn and feedforward network. This standard encoder layer is based on the paper \u201cAttention Is All You Need\u201d. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application. Args:     embed_dim: the number of expected features in the input (required).     num_heads: the number of heads in the multi head attention models (required).     ffn_dim: the dimension of the feedforward network model (default=embed_dim*4).     dropout: the dropout value (default=0.1).     activation: the activation function of intermediate layer, relu or gelu (default=relu).     layer_norm_eps: the eps value in layer normalization components (default=1e-5).     batch_first: If <code>True</code>, then the input and output tensors are provided         as (batch, seq, feature). Default: <code>False</code>.     norm_first: if <code>True</code>, layer norm is done prior to attention and feedforward         operations, respectivaly. Otherwise it\u2019s done after. Default: <code>False</code> (after). Examples::     &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(embed_dim=512, num_heads=8)     &gt;&gt;&gt; src = torch.rand(10, 32, 512)     &gt;&gt;&gt; out = encoder_layer(src) Alternatively, when <code>batch_first</code> is <code>True</code>:     &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(embed_dim=512, num_heads=8, batch_first=True)     &gt;&gt;&gt; src = torch.rand(32, 10, 512)     &gt;&gt;&gt; out = encoder_layer(src)</p>  Source code in <code>danling/models/transformer/encoder.py</code> Python<pre><code>class TransformerEncoderLayer(nn.Module):\n    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n    in a different way during application.\n    Args:\n        embed_dim: the number of expected features in the input (required).\n        num_heads: the number of heads in the multi head attention models (required).\n        ffn_dim: the dimension of the feedforward network model (default=embed_dim*4).\n        dropout: the dropout value (default=0.1).\n        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n        batch_first: If ``True``, then the input and output tensors are provided\n            as (batch, seq, feature). Default: ``False``.\n        norm_first: if ``True``, layer norm is done prior to attention and feedforward\n            operations, respectivaly. Otherwise it's done after. Default: ``False`` (after).\n    Examples::\n        &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(embed_dim=512, num_heads=8)\n        &gt;&gt;&gt; src = torch.rand(10, 32, 512)\n        &gt;&gt;&gt; out = encoder_layer(src)\n    Alternatively, when ``batch_first`` is ``True``:\n        &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(embed_dim=512, num_heads=8, batch_first=True)\n        &gt;&gt;&gt; src = torch.rand(32, 10, 512)\n        &gt;&gt;&gt; out = encoder_layer(src)\n    \"\"\"\n    __constants__ = [\"batch_first\", \"norm_first\"]\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        ffn_dim: Optional[int] = None,\n        dropout: float = 0.1,\n        attn_dropout: float = 0.1,\n        ffn_dropout: float = 0.1,\n        activation: str = \"GELU\",\n        layer_norm_eps: float = 1e-5,\n        scale_factor: float = 1.0,\n        bias: bool = True,\n        add_bias_kv: bool = False,\n        add_zero_attn: bool = False,\n        batch_first: bool = True,\n        norm_first: bool = False,\n        Attention: nn.Module = MultiHeadAttention,\n        FeedForwardNetwork: nn.Module = FullyConnectedNetwork,\n        **kwargs: Optional[Dict[str, Any]]\n    ) -&gt; None:\n        super().__init__()\n        if ffn_dim is None:\n            ffn_dim = embed_dim * 4\n        self.norm_first = norm_first\n        self.attn = Attention(\n            embed_dim,\n            num_heads,\n            attn_dropout=attn_dropout,\n            scale_factor=scale_factor,\n            bias=bias,\n            add_bias_kv=add_bias_kv,\n            add_zero_attn=add_zero_attn,\n            batch_first=batch_first,\n            **kwargs\n        )\n        self.norm1 = nn.LayerNorm(embed_dim, eps=layer_norm_eps)\n        self.ffn = FeedForwardNetwork(embed_dim, ffn_dim, activation, ffn_dropout, **kwargs)\n        self.norm2 = nn.LayerNorm(embed_dim, eps=layer_norm_eps)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(\n        self,\n        src: Tensor,\n        attn_bias: Optional[Tensor] = None,\n        attn_mask: Optional[Tensor] = None,\n        key_padding_mask: Optional[Tensor] = None,\n        need_weights: bool = False,\n    ) -&gt; Tensor:\n        r\"\"\"Pass the input through the encoder layer.\n        Args:\n            src: the sequence to the encoder layer (required).\n            attn_mask: the mask for the src sequence (optional).\n            key_padding_mask: the mask for the src keys per batch (optional).\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"\n\n        if self.norm_first:\n            src = self.norm1(src)\n\n        attn, weights = self.attn(\n            src,\n            src,\n            src,\n            attn_bias=attn_bias,\n            attn_mask=attn_mask,\n            key_padding_mask=key_padding_mask,\n            need_weights=need_weights,\n        )\n        attn = src + self.dropout(attn)\n        attn = self.norm1(attn) if not self.norm_first else self.norm2(attn)\n\n        ffn = self.ffn(attn)\n        ffn = attn + self.dropout(ffn)\n\n        if not self.norm_first:\n            ffn = self.norm2(ffn)\n\n        return ffn, weights\n</code></pre>","location":"en/package/#danling.TransformerEncoderLayer"},{"title":"<code>forward(src, attn_bias=None, attn_mask=None, key_padding_mask=None, need_weights=False)</code>","text":"<p>Pass the input through the encoder layer. Args:     src: the sequence to the encoder layer (required).     attn_mask: the mask for the src sequence (optional).     key_padding_mask: the mask for the src keys per batch (optional). Shape:     see the docs in Transformer class.</p>  Source code in <code>danling/models/transformer/encoder.py</code> Python<pre><code>def forward(\n    self,\n    src: Tensor,\n    attn_bias: Optional[Tensor] = None,\n    attn_mask: Optional[Tensor] = None,\n    key_padding_mask: Optional[Tensor] = None,\n    need_weights: bool = False,\n) -&gt; Tensor:\n    r\"\"\"Pass the input through the encoder layer.\n    Args:\n        src: the sequence to the encoder layer (required).\n        attn_mask: the mask for the src sequence (optional).\n        key_padding_mask: the mask for the src keys per batch (optional).\n    Shape:\n        see the docs in Transformer class.\n    \"\"\"\n\n    if self.norm_first:\n        src = self.norm1(src)\n\n    attn, weights = self.attn(\n        src,\n        src,\n        src,\n        attn_bias=attn_bias,\n        attn_mask=attn_mask,\n        key_padding_mask=key_padding_mask,\n        need_weights=need_weights,\n    )\n    attn = src + self.dropout(attn)\n    attn = self.norm1(attn) if not self.norm_first else self.norm2(attn)\n\n    ffn = self.ffn(attn)\n    ffn = attn + self.dropout(ffn)\n\n    if not self.norm_first:\n        ffn = self.norm2(ffn)\n\n    return ffn, weights\n</code></pre>","location":"en/package/#danling.models.transformer.encoder.TransformerEncoderLayer.forward"},{"title":"<code>UnitedPositionEmbedding</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>United Position Embedding See <code>Rethinking Positional Encoding in Language Pre-training &lt;https://arxiv.org/abs/2006.15595&gt;</code>_ .. math::     \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O where :math:<code>head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)</code>. Args:     embed_dim: total dimension of the model.     num_heads: parallel attention heads.     dropout: a Dropout layer on attn_output_weights. Default: 0.0.     bias: add bias as module parameter. Default: True.     add_bias_kv: add bias to the key and value sequences at dim=0.     add_zero_attn: add a new batch of zeros to the key and                    value sequences at dim=1.     k_dim: total number of features in key. Default: None.     v_dim: total number of features in value. Default: None.     batch_first: If <code>True</code>, then the input and output tensors are provided         as (batch, seq, feature). Default: <code>False</code> (seq, batch, feature). Note that if :attr:<code>k_dim</code> and :attr:<code>v_dim</code> are None, they will be set to :attr:<code>embed_dim</code> such that query, key, and value have the same number of features. Examples::     &gt;&gt;&gt; multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)     &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)</p>  Source code in <code>danling/models/transformer/pos_embed/pos_embed.py</code> Python<pre><code>class UnitedPositionEmbedding(nn.Module):\n    r\"\"\"United Position Embedding\n    See `Rethinking Positional Encoding in Language Pre-training &lt;https://arxiv.org/abs/2006.15595&gt;`_\n    .. math::\n        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n    where :math:`head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n    Args:\n        embed_dim: total dimension of the model.\n        num_heads: parallel attention heads.\n        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n        bias: add bias as module parameter. Default: True.\n        add_bias_kv: add bias to the key and value sequences at dim=0.\n        add_zero_attn: add a new batch of zeros to the key and\n                       value sequences at dim=1.\n        k_dim: total number of features in key. Default: None.\n        v_dim: total number of features in value. Default: None.\n        batch_first: If ``True``, then the input and output tensors are provided\n            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n    Note that if :attr:`k_dim` and :attr:`v_dim` are None, they will be set\n    to :attr:`embed_dim` such that query, key, and value have the same\n    number of features.\n    Examples::\n        &gt;&gt;&gt; multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n        &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        seq_len_max: int,\n        rel_pos_embed: bool = False,\n        rel_pos_embed_buckets: int = 32,\n        rel_pos_embed_max: int = 128,\n        pos_embed_dropout: float = 0.0,\n        pos_scale_factor: int = 1,\n        has_cls_token: bool = True,\n    ) -&gt; None:\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.seq_len_max = seq_len_max\n        self.has_cls_token = has_cls_token\n        self.dropout = nn.Dropout(pos_embed_dropout)\n        if self.has_cls_token:\n            # make room for [CLS]-to-others and others-to-[CLS]\n            self.seq_len_max += 2\n        self.abs_pos_embed = nn.Parameter(torch.randn(self.seq_len_max, self.embed_dim))\n        self.ln = nn.LayerNorm(self.embed_dim)\n        self.in_proj = nn.Linear(self.embed_dim, self.embed_dim * 2)\n        self.scaling = (embed_dim / num_heads * pos_scale_factor) ** -0.5\n\n        self.rel_pos_embed = None\n        if rel_pos_embed:\n            assert rel_pos_embed_buckets % 2 == 0\n            self.rel_pos_embed_buckets = rel_pos_embed_buckets\n            self.rel_pos_embed_max = rel_pos_embed_max\n            self.rel_pos_embed = nn.Embedding(self.rel_pos_embed_buckets + 1, self.num_heads)\n            self.rel_pos_embed_bucket = relative_position_bucket(\n                seq_len_max=self.seq_len_max,\n                num_buckets=self.rel_pos_embed_buckets,\n                max_distance=self.rel_pos_embed_max,\n            )\n\n    def forward(self, src: Tensor, cls_token_index: Optional[Tensor] = None) -&gt; Tensor:\n        B, N, C = src.shape\n        # 0 is for others-to-[CLS] 1 is for [CLS]-to-others\n        # Assume the input is ordered. If your input token is permuted, you may need to update this accordingly\n        if self.has_cls_token:\n            # only plus 1 here since because [CLS] already plused 1\n            N += 1\n        weight = self.ln(self.abs_pos_embed[:N, :])\n        q, k = self.in_proj(weight).reshape(N, 2, self.num_heads, C // self.num_heads).permute(1, 2, 0, 3)\n        q = q * self.scaling\n        pos_embed = torch.bmm(q, k.transpose(1, 2))\n        if self.has_cls_token:\n            # p_0 \\dot p_0 is [CLS]-to-others\n            cls_2_others = pos_embed[:, 0, 0]\n            # p_1 \\dot p_1 is others-to-[CLS]\n            others_2_cls = pos_embed[:, 1, 1]\n            # offset\n            pos_embed = pos_embed[:, 1:, 1:]\n            # if [CLS] is not the first token\n            if cls_token_index is not None:\n                pos_embed = pos_embed.repeat(B, 1, 1, 1)\n                pos_embed[torch.arange(B), :, cls_token_index, :] = cls_2_others.expand(B, -1).unsqueeze(-1)\n                pos_embed[torch.arange(B), :, :, cls_token_index] = others_2_cls.expand(B, -1).unsqueeze(-1)\n            else:\n                pos_embed[:, 0, :] = cls_2_others.unsqueeze(-1)\n                pos_embed[:, :, 0] = others_2_cls.unsqueeze(-1)\n            N -= 1\n        rel_pos_embed = torch.zeros_like(pos_embed)\n        if self.rel_pos_embed:\n            rel_pos_embed_bucket = self.rel_pos_embed_bucket[:N, :N]\n            if self.has_cls_token:\n                if cls_token_index is not None:\n                    rel_pos_embed_bucket = rel_pos_embed_bucket.repeat(B, 1, 1)\n                    rel_pos_embed_bucket[torch.arange(B), cls_token_index, :] = self.rel_pos_embed_buckets // 2\n                    rel_pos_embed_bucket[torch.arange(B), :, cls_token_index] = self.rel_pos_embed_buckets\n                    rel_pos_embed = self.rel_pos_embed(rel_pos_embed_bucket).permute(0, 3, 1, 2)\n                else:\n                    rel_pos_embed_bucket[0, :] = self.rel_pos_embed_buckets // 2\n                    rel_pos_embed_bucket[:, 0] = self.rel_pos_embed_buckets\n                    rel_pos_embed = self.rel_pos_embed(rel_pos_embed_bucket).permute(2, 0, 1)\n            else:\n                rel_pos_embed = self.rel_pos_embed(rel_pos_embed_bucket).permute(2, 0, 1)\n            pos_embed += rel_pos_embed\n\n        pos_embed = (\n            pos_embed.view(-1, *pos_embed.shape[2:]) if cls_token_index is not None else pos_embed.repeat(B, 1, 1)\n        )\n        return self.dropout(pos_embed)\n</code></pre>","location":"en/package/#danling.UnitedPositionEmbedding"},{"title":"<code>catch(error=Exception, exclude=None, print_args=False)</code>","text":"<p>Decorator to catch <code>error</code> except for <code>exclude</code>. Detailed traceback will be printed to <code>stdout</code>.</p> <p><code>catch</code> is extremely useful for unfatal errors. For example, <code>Runner</code> by de</p> <p>Parameters:</p>    Name Type Description Default     <code>error</code>  <code>Exception</code>    <code>Exception</code>    <code>exclude</code>  <code>Optional[Exception]</code>    <code>None</code>    <code>print_args</code>  <code>bool</code>  <p>Whether to print the arguments passed to the function.</p>  <code>False</code>      Source code in <code>danling/utils/decorator.py</code> Python<pre><code>@flexible_decorator\ndef catch(error: Exception = Exception, exclude: Optional[Exception] = None, print_args: bool = False):\n    \"\"\"\n    Decorator to catch `error` except for `exclude`.\n    Detailed traceback will be printed to `stdout`.\n\n    `catch` is extremely useful for unfatal errors.\n    For example, `Runner` by de\n\n    Parameters\n    ----------\n    error: Exception\n    exclude: Optional[Exception] = None\n    print_args: bool = False\n        Whether to print the arguments passed to the function.\n    \"\"\"\n\n    def decorator(func, error: Exception = Exception, exclude: Optional[Exception] = None, print_args: bool = False):\n        @wraps(func)\n        def wrapper(*args, **kwargs):  # pylint: disable=R1710\n            try:\n                return func(*args, **kwargs)\n            except error as exc:  # pylint: disable=W0703\n                if exclude is not None and isinstance(exc, exclude):\n                    raise exc\n                message = format_exc()\n                message += f\"\\nencoutered when calling {func}\"\n                if print_args:\n                    message += (f\"with args {args} and kwargs {kwargs}\",)\n                print(message, file=stderr, force=True)\n\n        return wrapper\n\n    return lambda func: decorator(func, error, exclude, print_args)\n</code></pre>","location":"en/package/#danling.catch"},{"title":"<code>load(path, *args, **kwargs)</code>","text":"<p>Load any file with supported extensions.</p>  Source code in <code>danling/utils/io.py</code> Python<pre><code>def load(path: str, *args: List[Any], **kwargs: Dict[str, Any]) -&gt; Any:\n    \"\"\"\n    Load any file with supported extensions.\n    \"\"\"\n    if not os.path.isfile(path):\n        raise ValueError(f\"Trying to load {path} but it is not a file.\")\n    extension = os.path.splitext(path)[-1].lower()[1:]\n    if extension in PYTORCH:\n        if torch_load is None:\n            raise ImportError(f\"Trying to load {path} but torch is not installed.\")\n        return torch_load(path)\n    if extension in NUMPY:\n        if numpy_load is None:\n            raise ImportError(f\"Trying to load {path} but numpy is not installed.\")\n        return numpy_load(path, allow_pickle=True)\n    if extension in CSV:\n        if read_csv is None:\n            raise ImportError(f\"Trying to load {path} but pandas is not installed.\")\n        return read_csv(path, *args, **kwargs)\n    if extension in JSON:\n        with open(path, \"r\") as fp:  # pylint: disable=W1514, C0103\n            return json_load(fp, *args, **kwargs)  # type: ignore\n    if extension in PICKLE:\n        with open(path, \"rb\") as fp:  # pylint: disable=C0103\n            return pickle_load(fp, *args, **kwargs)  # type: ignore\n    raise ValueError(f\"Tying to load {path} with unsupported extension={extension}\")\n</code></pre>","location":"en/package/#danling.load"},{"title":"Registry","text":"<p>         Bases: <code>NestedDict</code></p> <p><code>Registry</code> for components.</p>","location":"en/registry/"},{"title":"Notes","text":"<p><code>Registry</code> inherits from <code>NestedDict</code>.</p> <p>Therefore, <code>Registry</code> comes in a nested structure by nature. You could create a sub-registry by simply calling <code>registry.sub_registry = Registry</code>, and access through <code>registry.sub_registry.register()</code>.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; registry = Registry(\"test\")\n&gt;&gt;&gt; @registry.register\n... @registry.register(\"Module1\")\n... class Module:\n...     def __init__(self, a, b):\n...         self.a = a\n...         self.b = b\n&gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n&gt;&gt;&gt; registry\nRegistry(\n  (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n  (Module): &lt;class 'danling.registry.registry.Module'&gt;\n  (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n)\n&gt;&gt;&gt; registry.lookup(\"Module\")\n&lt;class 'danling.registry.registry.Module'&gt;\n&gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n&gt;&gt;&gt; # registry.register(Module)\n&gt;&gt;&gt; module = registry.build(config[\"module\"])\n&gt;&gt;&gt; type(module)\n&lt;class 'danling.registry.registry.Module'&gt;\n&gt;&gt;&gt; module.a\n1\n&gt;&gt;&gt; module.b\n2\n</code></pre>  Source code in <code>danling/registry/registry.py</code> Python<pre><code>class Registry(NestedDict):\n    \"\"\"\n    `Registry` for components.\n\n    Notes\n    -----\n\n    `Registry` inherits from [`NestedDict`](https://chanfig.danling.org/nested_dict/).\n\n    Therefore, `Registry` comes in a nested structure by nature.\n    You could create a sub-registry by simply calling `registry.sub_registry = Registry`,\n    and access through `registry.sub_registry.register()`.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; registry = Registry(\"test\")\n    &gt;&gt;&gt; @registry.register\n    ... @registry.register(\"Module1\")\n    ... class Module:\n    ...     def __init__(self, a, b):\n    ...         self.a = a\n    ...         self.b = b\n    &gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n    &gt;&gt;&gt; registry\n    Registry(\n      (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n      (Module): &lt;class 'danling.registry.registry.Module'&gt;\n      (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n    )\n    &gt;&gt;&gt; registry.lookup(\"Module\")\n    &lt;class 'danling.registry.registry.Module'&gt;\n    &gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n    &gt;&gt;&gt; # registry.register(Module)\n    &gt;&gt;&gt; module = registry.build(config[\"module\"])\n    &gt;&gt;&gt; type(module)\n    &lt;class 'danling.registry.registry.Module'&gt;\n    &gt;&gt;&gt; module.a\n    1\n    &gt;&gt;&gt; module.b\n    2\n\n    ```\n    \"\"\"\n\n    override: bool = False\n\n    def __init__(self, override: bool = False):\n        super().__init__()\n        self.setattr(\"override\", override)\n\n    def register(self, component: Optional[Callable] = None, name: Optional[str] = None) -&gt; Callable:\n        r\"\"\"\n        Register a new component\n\n        Parameters\n        ----------\n        component: Optional[Callable] = None\n            The component to register.\n        name: Optional[str] = component.__name__\n            The name of the component.\n\n        Returns\n        -------\n        component: Callable\n            The registered component.\n\n        Raises\n        ------\n        ValueError\n            If the component with the same name already exists and `Registry.override=False`.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; registry = Registry(\"test\")\n        &gt;&gt;&gt; @registry.register\n        ... @registry.register(\"Module1\")\n        ... class Module:\n        ...     def __init__(self, a, b):\n        ...         self.a = a\n        ...         self.b = b\n        &gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n        &gt;&gt;&gt; registry\n        Registry(\n          (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n          (Module): &lt;class 'danling.registry.registry.Module'&gt;\n          (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n        )\n\n        ```\n        \"\"\"\n\n        if isinstance(name, str) and name in self and not self.override:\n            raise ValueError(f\"Component with name {name} already exists\")\n\n        # Registry.register()\n        if name is not None:\n            self.set(name, component)\n\n        # @Registry.register()\n        @wraps(self.register)\n        def register(component, name=None):\n            if name is None:\n                name = component.__name__\n            self.set(name, component)\n            return component\n\n        # @Registry.register\n        if callable(component) and name is None:\n            return register(component)\n\n        return lambda x: register(x, component)\n\n    def lookup(self, name: str) -&gt; Any:\n        r\"\"\"\n        Lookup for a component.\n\n        Parameters\n        ----------\n        name: str\n            The name of the component.\n\n        Returns\n        -------\n        value: Any\n\n        Raises\n        ------\n        KeyError\n            If the component is not registered.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; registry = Registry(\"test\")\n        &gt;&gt;&gt; @registry.register\n        ... class Module:\n        ...     def __init__(self, a, b):\n        ...         self.a = a\n        ...         self.b = b\n        &gt;&gt;&gt; registry.lookup(\"Module\")\n        &lt;class 'danling.registry.registry.Module'&gt;\n\n        ```\n        \"\"\"\n\n        return self.get(name)\n\n    def build(self, name: str, *args, **kwargs):\n        r\"\"\"\n        Build a component.\n\n        Parameters\n        ----------\n        name: str\n            The name of the component.\n        *args\n            The arguments to pass to the component.\n        **kwargs\n            The keyword arguments to pass to the component.\n\n        Returns\n        -------\n        component: Callable\n\n        Raises\n        ------\n        KeyError\n            If the component is not registered.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; registry = Registry(\"test\")\n        &gt;&gt;&gt; @registry.register\n        ... class Module:\n        ...     def __init__(self, a, b):\n        ...         self.a = a\n        ...         self.b = b\n        &gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n        &gt;&gt;&gt; # registry.register(Module)\n        &gt;&gt;&gt; module = registry.build(config[\"module\"])\n        &gt;&gt;&gt; type(module)\n        &lt;class 'danling.registry.registry.Module'&gt;\n        &gt;&gt;&gt; module.a\n        1\n        &gt;&gt;&gt; module.b\n        2\n\n        ```\n        \"\"\"\n\n        if isinstance(name, Mapping) and not args and not kwargs:\n            name, kwargs = name.pop(\"name\"), name  # type: ignore\n        return self.get(name)(*args, **kwargs)\n\n    def __wrapped__(self):\n        pass\n</code></pre>","location":"en/registry/#danling.registry.Registry--notes"},{"title":"<code>register(component=None, name=None)</code>","text":"<p>Register a new component</p> <p>Parameters:</p>    Name Type Description Default     <code>component</code>  <code>Optional[Callable]</code>  <p>The component to register.</p>  <code>None</code>    <code>name</code>  <code>Optional[str]</code>  <p>The name of the component.</p>  <code>None</code>     <p>Returns:</p>    Name Type Description     <code>component</code>  <code>Callable</code>  <p>The registered component.</p>    <p>Raises:</p>    Type Description      <code>ValueError</code>  <p>If the component with the same name already exists and <code>Registry.override=False</code>.</p>    <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; registry = Registry(\"test\")\n&gt;&gt;&gt; @registry.register\n... @registry.register(\"Module1\")\n... class Module:\n...     def __init__(self, a, b):\n...         self.a = a\n...         self.b = b\n&gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n&gt;&gt;&gt; registry\nRegistry(\n  (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n  (Module): &lt;class 'danling.registry.registry.Module'&gt;\n  (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n)\n</code></pre>  Source code in <code>danling/registry/registry.py</code> Python<pre><code>def register(self, component: Optional[Callable] = None, name: Optional[str] = None) -&gt; Callable:\n    r\"\"\"\n    Register a new component\n\n    Parameters\n    ----------\n    component: Optional[Callable] = None\n        The component to register.\n    name: Optional[str] = component.__name__\n        The name of the component.\n\n    Returns\n    -------\n    component: Callable\n        The registered component.\n\n    Raises\n    ------\n    ValueError\n        If the component with the same name already exists and `Registry.override=False`.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; registry = Registry(\"test\")\n    &gt;&gt;&gt; @registry.register\n    ... @registry.register(\"Module1\")\n    ... class Module:\n    ...     def __init__(self, a, b):\n    ...         self.a = a\n    ...         self.b = b\n    &gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n    &gt;&gt;&gt; registry\n    Registry(\n      (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n      (Module): &lt;class 'danling.registry.registry.Module'&gt;\n      (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n    )\n\n    ```\n    \"\"\"\n\n    if isinstance(name, str) and name in self and not self.override:\n        raise ValueError(f\"Component with name {name} already exists\")\n\n    # Registry.register()\n    if name is not None:\n        self.set(name, component)\n\n    # @Registry.register()\n    @wraps(self.register)\n    def register(component, name=None):\n        if name is None:\n            name = component.__name__\n        self.set(name, component)\n        return component\n\n    # @Registry.register\n    if callable(component) and name is None:\n        return register(component)\n\n    return lambda x: register(x, component)\n</code></pre>","location":"en/registry/#danling.registry.registry.Registry.register"},{"title":"<code>lookup(name)</code>","text":"<p>Lookup for a component.</p> <p>Parameters:</p>    Name Type Description Default     <code>name</code>  <code>str</code>  <p>The name of the component.</p>  required     <p>Returns:</p>    Name Type Description     <code>value</code>  <code>Any</code>      <p>Raises:</p>    Type Description      <code>KeyError</code>  <p>If the component is not registered.</p>    <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; registry = Registry(\"test\")\n&gt;&gt;&gt; @registry.register\n... class Module:\n...     def __init__(self, a, b):\n...         self.a = a\n...         self.b = b\n&gt;&gt;&gt; registry.lookup(\"Module\")\n&lt;class 'danling.registry.registry.Module'&gt;\n</code></pre>  Source code in <code>danling/registry/registry.py</code> Python<pre><code>def lookup(self, name: str) -&gt; Any:\n    r\"\"\"\n    Lookup for a component.\n\n    Parameters\n    ----------\n    name: str\n        The name of the component.\n\n    Returns\n    -------\n    value: Any\n\n    Raises\n    ------\n    KeyError\n        If the component is not registered.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; registry = Registry(\"test\")\n    &gt;&gt;&gt; @registry.register\n    ... class Module:\n    ...     def __init__(self, a, b):\n    ...         self.a = a\n    ...         self.b = b\n    &gt;&gt;&gt; registry.lookup(\"Module\")\n    &lt;class 'danling.registry.registry.Module'&gt;\n\n    ```\n    \"\"\"\n\n    return self.get(name)\n</code></pre>","location":"en/registry/#danling.registry.registry.Registry.lookup"},{"title":"<code>build(name, *args, **kwargs)</code>","text":"<p>Build a component.</p> <p>Parameters:</p>    Name Type Description Default     <code>name</code>  <code>str</code>  <p>The name of the component.</p>  required    <code>*args</code>   <p>The arguments to pass to the component.</p>  <code>()</code>    <code>**kwargs</code>   <p>The keyword arguments to pass to the component.</p>  <code>{}</code>     <p>Returns:</p>    Name Type Description     <code>component</code>  <code>Callable</code>      <p>Raises:</p>    Type Description      <code>KeyError</code>  <p>If the component is not registered.</p>    <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; registry = Registry(\"test\")\n&gt;&gt;&gt; @registry.register\n... class Module:\n...     def __init__(self, a, b):\n...         self.a = a\n...         self.b = b\n&gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n&gt;&gt;&gt; # registry.register(Module)\n&gt;&gt;&gt; module = registry.build(config[\"module\"])\n&gt;&gt;&gt; type(module)\n&lt;class 'danling.registry.registry.Module'&gt;\n&gt;&gt;&gt; module.a\n1\n&gt;&gt;&gt; module.b\n2\n</code></pre>  Source code in <code>danling/registry/registry.py</code> Python<pre><code>def build(self, name: str, *args, **kwargs):\n    r\"\"\"\n    Build a component.\n\n    Parameters\n    ----------\n    name: str\n        The name of the component.\n    *args\n        The arguments to pass to the component.\n    **kwargs\n        The keyword arguments to pass to the component.\n\n    Returns\n    -------\n    component: Callable\n\n    Raises\n    ------\n    KeyError\n        If the component is not registered.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; registry = Registry(\"test\")\n    &gt;&gt;&gt; @registry.register\n    ... class Module:\n    ...     def __init__(self, a, b):\n    ...         self.a = a\n    ...         self.b = b\n    &gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n    &gt;&gt;&gt; # registry.register(Module)\n    &gt;&gt;&gt; module = registry.build(config[\"module\"])\n    &gt;&gt;&gt; type(module)\n    &lt;class 'danling.registry.registry.Module'&gt;\n    &gt;&gt;&gt; module.a\n    1\n    &gt;&gt;&gt; module.b\n    2\n\n    ```\n    \"\"\"\n\n    if isinstance(name, Mapping) and not args and not kwargs:\n        name, kwargs = name.pop(\"name\"), name  # type: ignore\n    return self.get(name)(*args, **kwargs)\n</code></pre>","location":"en/registry/#danling.registry.registry.Registry.build"},{"title":"DanLing","text":"","location":"en/blog/"},{"title":"AverageMeter","text":"<p>Computes and stores the average and current value.</p> <p>Attributes:</p>    Name Type Description     <code>val</code>  <code>int</code>  <p>Current value.</p>   <code>avg</code>  <code>float</code>  <p>Average value.</p>   <code>sum</code>  <code>float</code>  <p>Sum of values.</p>   <code>count</code>  <code>int</code>  <p>Number of values.</p>    <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.update(0.9)\n&gt;&gt;&gt; meter.val\n0.9\n&gt;&gt;&gt; meter.avg\n0.8\n&gt;&gt;&gt; meter.sum\n1.6\n&gt;&gt;&gt; meter.count\n2\n&gt;&gt;&gt; meter.reset()\n&gt;&gt;&gt; meter.val\n0\n&gt;&gt;&gt; meter.avg\n0\n</code></pre>  Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>class AverageMeter:\n    r\"\"\"\n    Computes and stores the average and current value.\n\n    Attributes\n    ----------\n    val: int\n        Current value.\n    avg: float\n        Average value.\n    sum: float\n        Sum of values.\n    count: int\n        Number of values.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; meter = AverageMeter()\n    &gt;&gt;&gt; meter.update(0.7)\n    &gt;&gt;&gt; meter.val\n    0.7\n    &gt;&gt;&gt; meter.avg\n    0.7\n    &gt;&gt;&gt; meter.update(0.9)\n    &gt;&gt;&gt; meter.val\n    0.9\n    &gt;&gt;&gt; meter.avg\n    0.8\n    &gt;&gt;&gt; meter.sum\n    1.6\n    &gt;&gt;&gt; meter.count\n    2\n    &gt;&gt;&gt; meter.reset()\n    &gt;&gt;&gt; meter.val\n    0\n    &gt;&gt;&gt; meter.avg\n    0\n\n    ```\n    \"\"\"\n\n    val: int = 0\n    avg: float = 0\n    sum: int = 0\n    count: int = 0\n\n    def __init__(self) -&gt; None:\n        self.reset()\n\n    def reset(self) -&gt; None:\n        r\"\"\"\n        Resets the meter.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; meter = AverageMeter()\n        &gt;&gt;&gt; meter.update(0.7)\n        &gt;&gt;&gt; meter.val\n        0.7\n        &gt;&gt;&gt; meter.avg\n        0.7\n        &gt;&gt;&gt; meter.reset()\n        &gt;&gt;&gt; meter.val\n        0\n        &gt;&gt;&gt; meter.avg\n        0\n\n        ```\n        \"\"\"\n\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n: int = 1) -&gt; None:\n        r\"\"\"\n        Updates the average and current value in the meter.\n\n        Parameters\n        ----------\n        val: int\n            Value to be added to the average.\n        n: int = 1\n            Number of values to be added.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; meter = AverageMeter()\n        &gt;&gt;&gt; meter.update(0.7)\n        &gt;&gt;&gt; meter.val\n        0.7\n        &gt;&gt;&gt; meter.avg\n        0.7\n        &gt;&gt;&gt; meter.update(0.9)\n        &gt;&gt;&gt; meter.val\n        0.9\n        &gt;&gt;&gt; meter.avg\n        0.8\n        &gt;&gt;&gt; meter.sum\n        1.6\n        &gt;&gt;&gt; meter.count\n        2\n\n        ```\n        \"\"\"\n\n        # pylint: disable=C0103\n\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n</code></pre>","location":"en/metrics/average_meter/"},{"title":"<code>reset()</code>","text":"<p>Resets the meter.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.reset()\n&gt;&gt;&gt; meter.val\n0\n&gt;&gt;&gt; meter.avg\n0\n</code></pre>  Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>def reset(self) -&gt; None:\n    r\"\"\"\n    Resets the meter.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; meter = AverageMeter()\n    &gt;&gt;&gt; meter.update(0.7)\n    &gt;&gt;&gt; meter.val\n    0.7\n    &gt;&gt;&gt; meter.avg\n    0.7\n    &gt;&gt;&gt; meter.reset()\n    &gt;&gt;&gt; meter.val\n    0\n    &gt;&gt;&gt; meter.avg\n    0\n\n    ```\n    \"\"\"\n\n    self.val = 0\n    self.avg = 0\n    self.sum = 0\n    self.count = 0\n</code></pre>","location":"en/metrics/average_meter/#danling.metrics.average_meter.AverageMeter.reset"},{"title":"<code>update(val, n=1)</code>","text":"<p>Updates the average and current value in the meter.</p> <p>Parameters:</p>    Name Type Description Default     <code>val</code>   <p>Value to be added to the average.</p>  required    <code>n</code>  <code>int</code>  <p>Number of values to be added.</p>  <code>1</code>     <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.update(0.9)\n&gt;&gt;&gt; meter.val\n0.9\n&gt;&gt;&gt; meter.avg\n0.8\n&gt;&gt;&gt; meter.sum\n1.6\n&gt;&gt;&gt; meter.count\n2\n</code></pre>  Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>def update(self, val, n: int = 1) -&gt; None:\n    r\"\"\"\n    Updates the average and current value in the meter.\n\n    Parameters\n    ----------\n    val: int\n        Value to be added to the average.\n    n: int = 1\n        Number of values to be added.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; meter = AverageMeter()\n    &gt;&gt;&gt; meter.update(0.7)\n    &gt;&gt;&gt; meter.val\n    0.7\n    &gt;&gt;&gt; meter.avg\n    0.7\n    &gt;&gt;&gt; meter.update(0.9)\n    &gt;&gt;&gt; meter.val\n    0.9\n    &gt;&gt;&gt; meter.avg\n    0.8\n    &gt;&gt;&gt; meter.sum\n    1.6\n    &gt;&gt;&gt; meter.count\n    2\n\n    ```\n    \"\"\"\n\n    # pylint: disable=C0103\n\n    self.val = val\n    self.sum += val * n\n    self.count += n\n    self.avg = self.sum / self.count\n</code></pre>","location":"en/metrics/average_meter/#danling.metrics.average_meter.AverageMeter.update"},{"title":"Runner","text":"<p>The Runner of DanLing sets up the basic environment for running neural networks.</p>","location":"en/runner/"},{"title":"Components","text":"<p>For readability and maintainability, there are three levels of Runner:</p>","location":"en/runner/#components"},{"title":"[<code>RunnerBase</code>][danling.runner.bases.RunnerBase]","text":"<p>[<code>RunnerBase</code>][danling.runner.bases.RunnerBase] gives you a basic instinct on what attributes and properties are provided by the Runner.</p> <p>It works in an AbstractBaseClass manner and should neither be used directly nor be inherited from.</p>","location":"en/runner/#runnerbasedanlingrunnerbasesrunnerbase"},{"title":"[<code>BaseRunner</code>][danling.runner.BaseRunner]","text":"<p>[<code>BaseRunner</code>][danling.runner.BaseRunner] contains core methods of general basic functionality, such as <code>init_logging</code>, <code>append_result</code>, <code>print_result</code>.</p>","location":"en/runner/#baserunnerdanlingrunnerbaserunner"},{"title":"[<code>Runner</code>][danling.runner.TorchRunner].","text":"<p><code>Runner</code> should only contain platform-specific features. Currently, only [<code>TorchRunner</code>][danling.runner.TorchRunner] is supported.</p>","location":"en/runner/#runnerdanlingrunnertorchrunner"},{"title":"Philosophy","text":"<p>DanLing Runner is designed for a 3.5-level experiments management system: Project, Group, Experiment, and, Run.</p>","location":"en/runner/#philosophy"},{"title":"Project","text":"<p>Project corresponds to your project.</p> <p>Generally speaking, there should be only one project for each repository.</p>","location":"en/runner/#project"},{"title":"Group","text":"<p>A group groups multiple experiments with similar characteristics.</p> <p>For example, if you run multiple experiments on learning rate, you may want to group them into a group.</p> <p>Note that Group is a virtual level (which is why it only counts 0.5) and does not corresponds to anything.</p>","location":"en/runner/#group"},{"title":"Experiment","text":"<p>Experiment is the basic unit of experiments.</p> <p>Experiment usually corresponds to a commit, which means the code should be consistent across the experiment.</p>","location":"en/runner/#experiment"},{"title":"Run","text":"<p>Run is the basic unit of running.</p> <p>Run corresponds to a single run of an experiment, each run may have different hyperparameters.</p>","location":"en/runner/#run"},{"title":"Attributes &amp; Properties","text":"","location":"en/runner/#attributes-properties"},{"title":"General","text":"<ul> <li>id</li> <li>name</li> <li>seed</li> <li>deterministic</li> </ul>","location":"en/runner/#general"},{"title":"Progress","text":"<ul> <li>iters</li> <li>steps</li> <li>epochs</li> <li>iter_end</li> <li>step_end</li> <li>epoch_end</li> <li>progress</li> </ul> <p>Note that generally you should only use one of <code>iter</code>, <code>step</code>, <code>epoch</code> to indicate the progress.</p>","location":"en/runner/#progress"},{"title":"Model","text":"<ul> <li>model</li> <li>criterion</li> <li>optimizer</li> <li>scheduler</li> </ul>","location":"en/runner/#model"},{"title":"Data","text":"<ul> <li>datasets</li> <li>datasamplers</li> <li>dataloaders</li> <li>batch_size</li> <li>batch_size_equivalent</li> </ul> <p><code>datasets</code>, <code>datasamplers</code>, <code>dataloaders</code> should be a dict with the same keys. Their keys should be <code>split</code> (e.g. <code>train</code>, <code>val</code>, <code>test</code>).</p>","location":"en/runner/#data"},{"title":"Results","text":"<ul> <li>results</li> <li>latest_result</li> <li>best_result</li> <li>scores</li> <li>latest_score</li> <li>best_score</li> <li>index_set</li> <li>index</li> <li>is_best</li> </ul> <p><code>results</code> should be a list of <code>result</code>. <code>result</code> should be a dict with the same <code>split</code> as keys, like <code>dataloaders</code>. A typical <code>result</code> might look like this: Python<pre><code>{\n    \"train\": {\n        \"loss\": 0.1,\n        \"accuracy\": 0.9,\n    },\n    \"val\": {\n        \"loss\": 0.2,\n        \"accuracy\": 0.8,\n    },\n    \"test\": {\n        \"loss\": 0.3,\n        \"accuracy\": 0.7,\n    },\n}\n</code></pre></p> <p><code>scores</code> are usually a list of <code>float</code>, and are dynamically extracted from <code>results</code> by <code>index_set</code> and <code>index</code>. If <code>index_set = \"val\"</code>, <code>index = \"accuracy\"</code>, then <code>scores = 0.9</code>.</p>","location":"en/runner/#results"},{"title":"DDP","text":"<ul> <li>world_size</li> <li>rank</li> <li>local_rank</li> <li>distributed</li> <li>is_main_process</li> <li>is_local_main_process</li> </ul>","location":"en/runner/#ddp"},{"title":"IO","text":"<ul> <li>experiments_root</li> <li>dir</li> <li>checkpoint_dir</li> <li>log_path</li> <li>checkpoint_dir_name</li> </ul> <p><code>experiments_root</code> is the root directory of all experiments, and should be consistent across the Project.</p> <p><code>dir</code> is the directory of a certain Run.</p> <p>There is no attributes/properties for Group and Experiment.</p> <p><code>checkpoint_dir_name</code> is relative to <code>dir</code>, and is passed to generate <code>checkpoint_dir</code> (<code>checkpoint_dir = os.path.join(dir, checkpoint_dir_name)</code>). In practice, <code>checkpoint_dir_name</code> is rarely called.</p>","location":"en/runner/#io"},{"title":"logging","text":"<ul> <li>log</li> <li>logger</li> <li>tensorboard</li> <li>writer</li> </ul>","location":"en/runner/#logging"},{"title":"BaseRunner","text":"<p>         Bases: <code>RunnerBase</code></p> <p>Set up everything for running a job.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>class BaseRunner(RunnerBase):\n    r\"\"\"\n    Set up everything for running a job.\n    \"\"\"\n\n    # pylint: disable=R0902\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        if self.seed is not None:\n            self.set_seed()\n        if self.deterministic:\n            self.set_deterministic()\n        if self.log:\n            self.init_logging()\n        self.init_print()\n        if self.tensorboard:\n            self.init_tensorboard()\n\n        atexit.register(self.print_result)\n\n    @on_main_process\n    def init_logging(self) -&gt; None:\n        r\"\"\"\n        Set up logging.\n        \"\"\"\n\n        # Why is setting up proper logging so !@?#! ugly?\n        logging.config.dictConfig(\n            {\n                \"version\": 1,\n                \"disable_existing_loggers\": False,\n                \"formatters\": {\n                    \"standard\": {\"format\": \"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"},\n                },\n                \"handlers\": {\n                    \"stdout\": {\n                        \"level\": \"INFO\",\n                        \"formatter\": \"standard\",\n                        \"class\": \"logging.StreamHandler\",\n                        \"stream\": \"ext://sys.stdout\",\n                    },\n                    \"logfile\": {\n                        \"level\": \"DEBUG\",\n                        \"formatter\": \"standard\",\n                        \"class\": \"logging.FileHandler\",\n                        \"filename\": self.log_path,\n                        \"mode\": \"a\",\n                    },\n                },\n                \"loggers\": {\n                    \"\": {\n                        \"handlers\": [\"stdout\", \"logfile\"],\n                        \"level\": \"DEBUG\",\n                        \"propagate\": True,\n                    },\n                },\n            }\n        )\n        logging.captureWarnings(True)\n        self.logger = logging.getLogger(\"runner\")\n        self.logger.flush = lambda: [h.flush() for h in self.logger.handlers]  # type: ignore\n\n    def init_print(self, process: int = 0) -&gt; None:\n        r\"\"\"\n        Set up print.\n\n        Only print on a specific process or when force is indicated.\n\n        Parameters\n        ----------\n        process: int, optional\n            The process to print on.\n\n        Notes\n        -----\n        If `self.log = True`, the default `print` function will be override by `logging.info`.\n        \"\"\"\n\n        logger = logging.getLogger(\"print\")\n        logger.flush = lambda: [h.flush for h in logger.handlers]  # type: ignore\n        import builtins as __builtin__  # pylint: disable=C0415\n\n        builtin_print = __builtin__.print\n\n        @catch\n        def print(*args, force=False, end=\"\\n\", file=None, flush=False, **kwargs):  # pylint: disable=W0622\n            if self.rank == process or force:\n                if self.log:\n                    logger.info(*args, **kwargs)\n                else:\n                    builtin_print(*args, end=end, file=file, flush=flush, **kwargs)\n\n        __builtin__.print = print\n\n    def set_deterministic(self) -&gt; None:\n        r\"\"\"\n        Set up deterministic.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def set_seed(self, bias: Optional[int] = None) -&gt; None:\n        r\"\"\"\n        Set up random seed.\n\n        Parameters\n        ----------\n        bias: Optional[int] = self.rank\n            Make the seed different for each processes.\n            This avoids same data augmentation are applied on every processes.\n            Set to `False` to disable this feature.\n        \"\"\"\n\n        if bias is None:\n            bias = self.rank\n        seed = self.seed + bias if bias else self.seed\n        np.random.seed(seed)\n        random.seed(seed)\n\n    def scale_lr(\n        self,\n        lr_scale_factor: Optional[float] = None,\n        batch_size_base: Optional[int] = None,\n    ) -&gt; None:\n        r\"\"\"\n        Scale learning rate according to [linear scaling rule](https://arxiv.org/abs/1706.02677).\n        \"\"\"\n\n        # pylint: disable=W0201\n\n        if lr_scale_factor is None:\n            if batch_size_base is None:\n                if batch_size_base := getattr(self, \"batch_size_base\", None) is None:\n                    raise ValueError(\"batch_size_base must be specified to auto scale lr\")\n            lr_scale_factor = self.batch_size_equivalent / batch_size_base\n        self.lr_scale_factor = lr_scale_factor\n        self.lr = self.lr * self.lr_scale_factor  # type: float  # pylint: disable=C0103\n        self.lr_final = self.lr_final * self.lr_scale_factor  # type: float\n\n    def step(self, zero_grad: bool = True) -&gt; None:\n        r\"\"\"\n        Step optimizer and scheduler.\n\n        This method also increment the `self.steps` attribute.\n\n        Parameters\n        ----------\n        zero_grad: bool, optional\n            Whether to zero the gradients.\n        \"\"\"\n\n        if self.optimizer is not None:\n            self.optimizer.step()\n            if zero_grad:\n                self.optimizer.zero_grad()\n        if self.scheduler is not None:\n            self.scheduler.step()\n        self.steps += 1\n        # TODO: Support `drop_last = False`\n        self.iters += self.batch_size_equivalent\n\n    def state_dict(self, cls: Callable = dict) -&gt; Mapping:\n        r\"\"\"\n        Return dict of all attributes for checkpoint.\n        \"\"\"\n\n        if self.model is None:\n            raise ValueError(\"Model must be defined when calling state_dict\")\n        model = self.accelerator.unwrap_model(self.model)\n        return cls(\n            runner=self.dict(),\n            model=model.state_dict(),\n            optimizer=self.optimizer.state_dict() if self.optimizer else None,\n            scheduler=self.scheduler.state_dict() if self.scheduler else None,\n        )\n\n    @catch\n    @on_main_process\n    def save_checkpoint(self) -&gt; None:\n        r\"\"\"\n        Save checkpoint to `runner.checkpoint_dir`.\n\n        The checkpoint will be saved to `runner.checkpoint_dir/latest.pth`.\n\n        If `save_freq` is specified and `self.epochs + 1` is a multiple of `save_freq`,\n        the checkpoint will also be copied to `runner.checkpoint_dir/epoch-{self.epochs}.pth`.\n\n        If `self.is_best` is `True`, the checkpoint will also be copied to `runner.checkpoint_dir/best.pth`.\n        \"\"\"\n\n        latest_path = os.path.join(self.checkpoint_dir, \"latest.pth\")\n        self.save(self.state_dict(), latest_path)\n        if hasattr(self, \"save_freq\") and (self.epochs + 1) % self.save_freq == 0:\n            save_path = os.path.join(self.checkpoint_dir, f\"epoch-{self.epochs}.pth\")\n            shutil.copy(latest_path, save_path)\n        if self.is_best:\n            best_path = os.path.join(self.checkpoint_dir, \"best.pth\")\n            shutil.copy(latest_path, best_path)\n\n    def load_checkpoint(  # pylint: disable=W1113\n        self, checkpoint: Optional[Union[Mapping, str]] = None, override_config: bool = True, *args, **kwargs\n    ) -&gt; None:\n        \"\"\"\n        Load info from checkpoint.\n\n        Parameters\n        ----------\n        checkpoint: Optional[Union[Mapping, str]] = latest_checkpoint\n            Checkpoint (or its path) to load.\n        override_config: bool = True\n            If True, override runner config with checkpoint config.\n        *args, **kwargs\n            Additional arguments to pass to `runner.load`.\n\n        Raises\n        ------\n        FileNotFoundError\n            If `checkpoint` does not exists.\n\n        See also\n        --------\n        from_checkpoint: Build runner from checkpoint.\n        \"\"\"\n\n        if checkpoint is None:\n            checkpoint = os.path.join(self.checkpoint_dir, \"latest.pth\")\n        # TODO: Support loading checkpoints in other format\n        if isinstance(checkpoint, str):\n            if not os.path.exists(checkpoint):\n                raise FileNotFoundError(f\"checkpoint is set to {checkpoint} but does not exist.\")\n            state_dict = self.load(checkpoint, *args, **kwargs)\n        # TODO: Wrap state_dict in a dataclass\n        if override_config:\n            self.__dict__.update(state_dict[\"runner\"])  # type: ignore\n        if self.model is not None and \"model\" in state_dict:  # type: ignore\n            self.model.load_state_dict(state_dict[\"model\"])  # type: ignore\n        if self.optimizer is not None and \"optimizer\" in state_dict:  # type: ignore\n            self.optimizer.load_state_dict(state_dict[\"optimizer\"])  # type: ignore\n        if self.scheduler is not None and \"scheduler\" in state_dict:  # type: ignore\n            self.scheduler.load_state_dict(state_dict[\"scheduler\"])  # type: ignore\n        self.checkpoint = checkpoint  # pylint: disable=W0201\n\n    @classmethod\n    def from_checkpoint(cls, checkpoint: Union[Mapping, str], *args, **kwargs) -&gt; BaseRunner:\n        r\"\"\"\n        Build BaseRunner from checkpoint.\n\n        Parameters\n        ----------\n        checkpoint: Optional[Union[Mapping, str]] = latest_checkpoint\n            Checkpoint (or its path) to load.\n        *args, **kwargs\n            Additional arguments to pass to `runner.load`.\n\n        Returns\n        -------\n        BaseRunner\n        \"\"\"\n\n        if isinstance(checkpoint, str):\n            checkpoint = cls.load(checkpoint, *args, **kwargs)\n        runner = cls(**checkpoint[\"runner\"])  # type: ignore\n        runner.load_checkpoint(checkpoint, override_config=False)\n        return runner\n\n    def append_result(self, result) -&gt; None:\n        r\"\"\"\n        Append result to `self.results`.\n\n        Warnings\n        --------\n        `self.results` is heavily relied upon for computing metrics.\n\n        Failed to use this method may lead to unexpected behavior.\n        \"\"\"\n\n        self.results.append(result)\n\n    def print_result(self) -&gt; None:\n        r\"\"\"\n        Print latest and best result.\n        \"\"\"\n\n        print(f\"latest result: {self.latest_result}\")\n        print(f\"best result: {self.best_result}\")\n\n    @catch\n    @on_main_process\n    def save_result(self) -&gt; None:\n        r\"\"\"\n        Save result to `runner.dir`.\n\n        This method will save latest and best result to\n        `runner.dir/latest.json` and `runner.dir/best.json` respectively.\n        \"\"\"\n\n        ret = {\"id\": self.id, \"name\": self.name}\n        result = self.latest_result  # type: ignore\n        if isinstance(result, FlatDict):\n            result = result.dict()  # type: ignore\n        # This is slower but ensure id is the first key\n        ret.update(result)  # type: ignore\n        latest_path = os.path.join(self.dir, \"latest.json\")\n        with FlatDict.open(latest_path, \"w\") as f:  # pylint: disable=C0103\n            json.dump(ret, f, indent=4)\n        if self.is_best:\n            best_path = os.path.join(self.dir, \"best.json\")\n            shutil.copy(latest_path, best_path)\n</code></pre>","location":"en/runner/base_runner/"},{"title":"<code>step(zero_grad=True)</code>","text":"<p>Step optimizer and scheduler.</p> <p>This method also increment the <code>self.steps</code> attribute.</p> <p>Parameters:</p>    Name Type Description Default     <code>zero_grad</code>  <code>bool</code>  <p>Whether to zero the gradients.</p>  <code>True</code>      Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def step(self, zero_grad: bool = True) -&gt; None:\n    r\"\"\"\n    Step optimizer and scheduler.\n\n    This method also increment the `self.steps` attribute.\n\n    Parameters\n    ----------\n    zero_grad: bool, optional\n        Whether to zero the gradients.\n    \"\"\"\n\n    if self.optimizer is not None:\n        self.optimizer.step()\n        if zero_grad:\n            self.optimizer.zero_grad()\n    if self.scheduler is not None:\n        self.scheduler.step()\n    self.steps += 1\n    # TODO: Support `drop_last = False`\n    self.iters += self.batch_size_equivalent\n</code></pre>","location":"en/runner/base_runner/#danling.runner.base_runner.BaseRunner.step"},{"title":"<code>append_result(result)</code>","text":"<p>Append result to <code>self.results</code>.</p>","location":"en/runner/base_runner/#danling.runner.base_runner.BaseRunner.append_result"},{"title":"Warnings","text":"<p><code>self.results</code> is heavily relied upon for computing metrics.</p> <p>Failed to use this method may lead to unexpected behavior.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def append_result(self, result) -&gt; None:\n    r\"\"\"\n    Append result to `self.results`.\n\n    Warnings\n    --------\n    `self.results` is heavily relied upon for computing metrics.\n\n    Failed to use this method may lead to unexpected behavior.\n    \"\"\"\n\n    self.results.append(result)\n</code></pre>","location":"en/runner/base_runner/#danling.runner.base_runner.BaseRunner.append_result--warnings"},{"title":"<code>print_result()</code>","text":"<p>Print latest and best result.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def print_result(self) -&gt; None:\n    r\"\"\"\n    Print latest and best result.\n    \"\"\"\n\n    print(f\"latest result: {self.latest_result}\")\n    print(f\"best result: {self.best_result}\")\n</code></pre>","location":"en/runner/base_runner/#danling.runner.base_runner.BaseRunner.print_result"},{"title":"<code>save_result()</code>","text":"<p>Save result to <code>runner.dir</code>.</p> <p>This method will save latest and best result to <code>runner.dir/latest.json</code> and <code>runner.dir/best.json</code> respectively.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\n@on_main_process\ndef save_result(self) -&gt; None:\n    r\"\"\"\n    Save result to `runner.dir`.\n\n    This method will save latest and best result to\n    `runner.dir/latest.json` and `runner.dir/best.json` respectively.\n    \"\"\"\n\n    ret = {\"id\": self.id, \"name\": self.name}\n    result = self.latest_result  # type: ignore\n    if isinstance(result, FlatDict):\n        result = result.dict()  # type: ignore\n    # This is slower but ensure id is the first key\n    ret.update(result)  # type: ignore\n    latest_path = os.path.join(self.dir, \"latest.json\")\n    with FlatDict.open(latest_path, \"w\") as f:  # pylint: disable=C0103\n        json.dump(ret, f, indent=4)\n    if self.is_best:\n        best_path = os.path.join(self.dir, \"best.json\")\n        shutil.copy(latest_path, best_path)\n</code></pre>","location":"en/runner/base_runner/#danling.runner.base_runner.BaseRunner.save_result"},{"title":"<code>state_dict(cls=dict)</code>","text":"<p>Return dict of all attributes for checkpoint.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def state_dict(self, cls: Callable = dict) -&gt; Mapping:\n    r\"\"\"\n    Return dict of all attributes for checkpoint.\n    \"\"\"\n\n    if self.model is None:\n        raise ValueError(\"Model must be defined when calling state_dict\")\n    model = self.accelerator.unwrap_model(self.model)\n    return cls(\n        runner=self.dict(),\n        model=model.state_dict(),\n        optimizer=self.optimizer.state_dict() if self.optimizer else None,\n        scheduler=self.scheduler.state_dict() if self.scheduler else None,\n    )\n</code></pre>","location":"en/runner/base_runner/#danling.runner.base_runner.BaseRunner.state_dict"},{"title":"<code>save_checkpoint()</code>","text":"<p>Save checkpoint to <code>runner.checkpoint_dir</code>.</p> <p>The checkpoint will be saved to <code>runner.checkpoint_dir/latest.pth</code>.</p> <p>If <code>save_freq</code> is specified and <code>self.epochs + 1</code> is a multiple of <code>save_freq</code>, the checkpoint will also be copied to <code>runner.checkpoint_dir/epoch-{self.epochs}.pth</code>.</p> <p>If <code>self.is_best</code> is <code>True</code>, the checkpoint will also be copied to <code>runner.checkpoint_dir/best.pth</code>.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\n@on_main_process\ndef save_checkpoint(self) -&gt; None:\n    r\"\"\"\n    Save checkpoint to `runner.checkpoint_dir`.\n\n    The checkpoint will be saved to `runner.checkpoint_dir/latest.pth`.\n\n    If `save_freq` is specified and `self.epochs + 1` is a multiple of `save_freq`,\n    the checkpoint will also be copied to `runner.checkpoint_dir/epoch-{self.epochs}.pth`.\n\n    If `self.is_best` is `True`, the checkpoint will also be copied to `runner.checkpoint_dir/best.pth`.\n    \"\"\"\n\n    latest_path = os.path.join(self.checkpoint_dir, \"latest.pth\")\n    self.save(self.state_dict(), latest_path)\n    if hasattr(self, \"save_freq\") and (self.epochs + 1) % self.save_freq == 0:\n        save_path = os.path.join(self.checkpoint_dir, f\"epoch-{self.epochs}.pth\")\n        shutil.copy(latest_path, save_path)\n    if self.is_best:\n        best_path = os.path.join(self.checkpoint_dir, \"best.pth\")\n        shutil.copy(latest_path, best_path)\n</code></pre>","location":"en/runner/base_runner/#danling.runner.base_runner.BaseRunner.save_checkpoint"},{"title":"<code>load_checkpoint(checkpoint=None, override_config=True, *args, **kwargs)</code>","text":"<p>Load info from checkpoint.</p> <p>Parameters:</p>    Name Type Description Default     <code>checkpoint</code>  <code>Optional[Union[Mapping, str]]</code>  <p>Checkpoint (or its path) to load.</p>  <code>None</code>    <code>override_config</code>  <code>bool</code>  <p>If True, override runner config with checkpoint config.</p>  <code>True</code>    <code>*args</code>   <p>Additional arguments to pass to <code>runner.load</code>.</p>  <code>()</code>    <code>**kwargs</code>   <p>Additional arguments to pass to <code>runner.load</code>.</p>  <code>()</code>     <p>Raises:</p>    Type Description      <code>FileNotFoundError</code>  <p>If <code>checkpoint</code> does not exists.</p>","location":"en/runner/base_runner/#danling.runner.base_runner.BaseRunner.load_checkpoint"},{"title":"See also","text":"<p>from_checkpoint: Build runner from checkpoint.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def load_checkpoint(  # pylint: disable=W1113\n    self, checkpoint: Optional[Union[Mapping, str]] = None, override_config: bool = True, *args, **kwargs\n) -&gt; None:\n    \"\"\"\n    Load info from checkpoint.\n\n    Parameters\n    ----------\n    checkpoint: Optional[Union[Mapping, str]] = latest_checkpoint\n        Checkpoint (or its path) to load.\n    override_config: bool = True\n        If True, override runner config with checkpoint config.\n    *args, **kwargs\n        Additional arguments to pass to `runner.load`.\n\n    Raises\n    ------\n    FileNotFoundError\n        If `checkpoint` does not exists.\n\n    See also\n    --------\n    from_checkpoint: Build runner from checkpoint.\n    \"\"\"\n\n    if checkpoint is None:\n        checkpoint = os.path.join(self.checkpoint_dir, \"latest.pth\")\n    # TODO: Support loading checkpoints in other format\n    if isinstance(checkpoint, str):\n        if not os.path.exists(checkpoint):\n            raise FileNotFoundError(f\"checkpoint is set to {checkpoint} but does not exist.\")\n        state_dict = self.load(checkpoint, *args, **kwargs)\n    # TODO: Wrap state_dict in a dataclass\n    if override_config:\n        self.__dict__.update(state_dict[\"runner\"])  # type: ignore\n    if self.model is not None and \"model\" in state_dict:  # type: ignore\n        self.model.load_state_dict(state_dict[\"model\"])  # type: ignore\n    if self.optimizer is not None and \"optimizer\" in state_dict:  # type: ignore\n        self.optimizer.load_state_dict(state_dict[\"optimizer\"])  # type: ignore\n    if self.scheduler is not None and \"scheduler\" in state_dict:  # type: ignore\n        self.scheduler.load_state_dict(state_dict[\"scheduler\"])  # type: ignore\n    self.checkpoint = checkpoint  # pylint: disable=W0201\n</code></pre>","location":"en/runner/base_runner/#danling.runner.base_runner.BaseRunner.load_checkpoint--see-also"},{"title":"<code>from_checkpoint(checkpoint, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Build BaseRunner from checkpoint.</p> <p>Parameters:</p>    Name Type Description Default     <code>checkpoint</code>  <code>Union[Mapping, str]</code>  <p>Checkpoint (or its path) to load.</p>  required    <code>*args</code>   <p>Additional arguments to pass to <code>runner.load</code>.</p>  <code>()</code>    <code>**kwargs</code>   <p>Additional arguments to pass to <code>runner.load</code>.</p>  <code>()</code>     <p>Returns:</p>    Type Description      <code>BaseRunner</code>       Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@classmethod\ndef from_checkpoint(cls, checkpoint: Union[Mapping, str], *args, **kwargs) -&gt; BaseRunner:\n    r\"\"\"\n    Build BaseRunner from checkpoint.\n\n    Parameters\n    ----------\n    checkpoint: Optional[Union[Mapping, str]] = latest_checkpoint\n        Checkpoint (or its path) to load.\n    *args, **kwargs\n        Additional arguments to pass to `runner.load`.\n\n    Returns\n    -------\n    BaseRunner\n    \"\"\"\n\n    if isinstance(checkpoint, str):\n        checkpoint = cls.load(checkpoint, *args, **kwargs)\n    runner = cls(**checkpoint[\"runner\"])  # type: ignore\n    runner.load_checkpoint(checkpoint, override_config=False)\n    return runner\n</code></pre>","location":"en/runner/base_runner/#danling.runner.base_runner.BaseRunner.from_checkpoint"},{"title":"<code>init_logging()</code>","text":"<p>Set up logging.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@on_main_process\ndef init_logging(self) -&gt; None:\n    r\"\"\"\n    Set up logging.\n    \"\"\"\n\n    # Why is setting up proper logging so !@?#! ugly?\n    logging.config.dictConfig(\n        {\n            \"version\": 1,\n            \"disable_existing_loggers\": False,\n            \"formatters\": {\n                \"standard\": {\"format\": \"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"},\n            },\n            \"handlers\": {\n                \"stdout\": {\n                    \"level\": \"INFO\",\n                    \"formatter\": \"standard\",\n                    \"class\": \"logging.StreamHandler\",\n                    \"stream\": \"ext://sys.stdout\",\n                },\n                \"logfile\": {\n                    \"level\": \"DEBUG\",\n                    \"formatter\": \"standard\",\n                    \"class\": \"logging.FileHandler\",\n                    \"filename\": self.log_path,\n                    \"mode\": \"a\",\n                },\n            },\n            \"loggers\": {\n                \"\": {\n                    \"handlers\": [\"stdout\", \"logfile\"],\n                    \"level\": \"DEBUG\",\n                    \"propagate\": True,\n                },\n            },\n        }\n    )\n    logging.captureWarnings(True)\n    self.logger = logging.getLogger(\"runner\")\n    self.logger.flush = lambda: [h.flush() for h in self.logger.handlers]  # type: ignore\n</code></pre>","location":"en/runner/base_runner/#danling.runner.base_runner.BaseRunner.init_logging"},{"title":"<code>init_print(process=0)</code>","text":"<p>Set up print.</p> <p>Only print on a specific process or when force is indicated.</p> <p>Parameters:</p>    Name Type Description Default     <code>process</code>  <code>int</code>  <p>The process to print on.</p>  <code>0</code>","location":"en/runner/base_runner/#danling.runner.base_runner.BaseRunner.init_print"},{"title":"Notes","text":"<p>If <code>self.log = True</code>, the default <code>print</code> function will be override by <code>logging.info</code>.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def init_print(self, process: int = 0) -&gt; None:\n    r\"\"\"\n    Set up print.\n\n    Only print on a specific process or when force is indicated.\n\n    Parameters\n    ----------\n    process: int, optional\n        The process to print on.\n\n    Notes\n    -----\n    If `self.log = True`, the default `print` function will be override by `logging.info`.\n    \"\"\"\n\n    logger = logging.getLogger(\"print\")\n    logger.flush = lambda: [h.flush for h in logger.handlers]  # type: ignore\n    import builtins as __builtin__  # pylint: disable=C0415\n\n    builtin_print = __builtin__.print\n\n    @catch\n    def print(*args, force=False, end=\"\\n\", file=None, flush=False, **kwargs):  # pylint: disable=W0622\n        if self.rank == process or force:\n            if self.log:\n                logger.info(*args, **kwargs)\n            else:\n                builtin_print(*args, end=end, file=file, flush=flush, **kwargs)\n\n    __builtin__.print = print\n</code></pre>","location":"en/runner/base_runner/#danling.runner.base_runner.BaseRunner.init_print--notes"},{"title":"<code>set_deterministic()</code>","text":"<p>Set up deterministic.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def set_deterministic(self) -&gt; None:\n    r\"\"\"\n    Set up deterministic.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>","location":"en/runner/base_runner/#danling.runner.base_runner.BaseRunner.set_deterministic"},{"title":"<code>set_seed(bias=None)</code>","text":"<p>Set up random seed.</p> <p>Parameters:</p>    Name Type Description Default     <code>bias</code>  <code>Optional[int]</code>  <p>Make the seed different for each processes. This avoids same data augmentation are applied on every processes. Set to <code>False</code> to disable this feature.</p>  <code>None</code>      Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def set_seed(self, bias: Optional[int] = None) -&gt; None:\n    r\"\"\"\n    Set up random seed.\n\n    Parameters\n    ----------\n    bias: Optional[int] = self.rank\n        Make the seed different for each processes.\n        This avoids same data augmentation are applied on every processes.\n        Set to `False` to disable this feature.\n    \"\"\"\n\n    if bias is None:\n        bias = self.rank\n    seed = self.seed + bias if bias else self.seed\n    np.random.seed(seed)\n    random.seed(seed)\n</code></pre>","location":"en/runner/base_runner/#danling.runner.base_runner.BaseRunner.set_seed"},{"title":"<code>scale_lr(lr_scale_factor=None, batch_size_base=None)</code>","text":"<p>Scale learning rate according to linear scaling rule.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def scale_lr(\n    self,\n    lr_scale_factor: Optional[float] = None,\n    batch_size_base: Optional[int] = None,\n) -&gt; None:\n    r\"\"\"\n    Scale learning rate according to [linear scaling rule](https://arxiv.org/abs/1706.02677).\n    \"\"\"\n\n    # pylint: disable=W0201\n\n    if lr_scale_factor is None:\n        if batch_size_base is None:\n            if batch_size_base := getattr(self, \"batch_size_base\", None) is None:\n                raise ValueError(\"batch_size_base must be specified to auto scale lr\")\n        lr_scale_factor = self.batch_size_equivalent / batch_size_base\n    self.lr_scale_factor = lr_scale_factor\n    self.lr = self.lr * self.lr_scale_factor  # type: float  # pylint: disable=C0103\n    self.lr_final = self.lr_final * self.lr_scale_factor  # type: float\n</code></pre>","location":"en/runner/base_runner/#danling.runner.base_runner.BaseRunner.scale_lr"},{"title":"RunnerBase","text":"<p>Base class for all runners.</p> <p><code>RunnerBase</code> is designed as a \u201cdataclass\u201d.</p> <p>It defines all basic attributes and relevant properties such as <code>scores</code>, <code>progress</code>, etc.</p> <p><code>RunnerBase</code> also defines basic IO operations such as <code>save</code>, <code>load</code>, <code>json</code>, <code>yaml</code>, etc.</p> <p>Attributes:</p>    Name Type Description     <code>id</code>  <code>str = f\"{self.name}-{self.seed}\"</code>     <code>name</code>  <code>str = \"danling\"</code>     <code>seed</code>  <code>int = randint(0, 2**32 - 1)</code>     <code>deterministic</code>  <code>bool = False</code>  <p>Ensure deterministic operations.</p>   <code>iters</code>  <code>int = 0</code>  <p>Current running iters. Iters refers to the number of data samples processed. Iters equals to steps when batch size is 1.</p>   <code>steps</code>  <code>int = 0</code>  <p>Current running steps. Steps refers to the number of <code>step</code> calls.</p>   <code>epochs</code>  <code>int = 0</code>  <p>Current running epochs. Epochs refers to the number of complete passes over the datasets.</p>   <code>iter_end</code>  <code>int</code>  <p>End running iters. Note that <code>step_end</code> not initialised since this variable may not apply to some Runners.</p>   <code>step_end</code>  <code>int</code>  <p>End running steps. Note that <code>step_end</code> not initialised since this variable may not apply to some Runners.</p>   <code>epoch_end</code>  <code>int</code>  <p>End running epochs. Note that <code>epoch_end</code> not initialised since this variable may not apply to some Runners.</p>   <code>model</code>  <code>Optional = None</code>     <code>criterion</code>  <code>Optional = None</code>     <code>optimizer</code>  <code>Optional = None</code>     <code>scheduler</code>  <code>Optional = None</code>     <code>datasets</code>  <code>FlatDict</code>  <p>All datasets, should be in the form of <code>{subset: dataset}</code>.</p>   <code>datasamplers</code>  <code>FlatDict</code>  <p>All datasamplers, should be in the form of <code>{subset: datasampler}</code>.</p>   <code>dataloaders</code>  <code>FlatDict</code>  <p>All dataloaders, should be in the form of <code>{subset: dataloader}</code>.</p>   <code>batch_size</code>  <code>int = 1</code>     <code>results</code>  <code>List[NestedDict] = []</code>  <p>All results, should be in the form of <code>[{subset: {index: score}}]</code>.</p>   <code>index_set</code>  <code>str = 'val'</code>  <p>The subset to calculate the core score.</p>   <code>index</code>  <code>str = 'loss'</code>  <p>The index to calculate the core score.</p>   <code>experiments_root</code>  <code>str = \"experiments\"</code>  <p>The root directory for all experiments.</p>   <code>checkpoint_dir_name</code>  <code>str = \"checkpoints\"</code>  <p>The name of the directory under <code>runner.dir</code> to save checkpoints.</p>   <code>log</code>  <code>bool = True</code>  <p>Whether to log the results.</p>   <code>logger</code>  <code>Optional[logging.Logger] = None</code>     <code>tensorboard</code>  <code>bool = False</code>  <p>Whether to use tensorboard.</p>   <code>writer</code>  <code>Optional[SummaryWriter] = None</code>","location":"en/runner/bases/"},{"title":"Notes","text":"<p>The <code>RunnerBase</code> class is not intended to be used directly, nor to be directly inherit from.</p> <p>This is because <code>RunnerBase</code> is designed as a \u201cdataclass\u201d, and is meant for demonstrating all attributes and properties only.</p>","location":"en/runner/bases/#danling.runner.bases.RunnerBase--notes"},{"title":"See Also","text":"<p>[<code>BaseRunner</code>][danling.base_runner.BaseRunner]: The base runner class.</p>  Source code in <code>danling/runner/bases.py</code> Python<pre><code>class RunnerBase:\n    r\"\"\"\n    Base class for all runners.\n\n    `RunnerBase` is designed as a \"dataclass\".\n\n    It defines all basic attributes and relevant properties such as `scores`, `progress`, etc.\n\n    `RunnerBase` also defines basic IO operations such as `save`, `load`, `json`, `yaml`, etc.\n\n    Attributes\n    ----------\n    id: str = f\"{self.name}-{self.seed}\"\n    name: str = \"danling\"\n    seed: int = randint(0, 2**32 - 1)\n    deterministic: bool = False\n        Ensure [deterministic](https://pytorch.org/docs/stable/notes/randomness.html) operations.\n    iters: int = 0\n        Current running iters.\n        Iters refers to the number of data samples processed.\n        Iters equals to steps when batch size is 1.\n    steps: int = 0\n        Current running steps.\n        Steps refers to the number of `step` calls.\n    epochs: int = 0\n        Current running epochs.\n        Epochs refers to the number of complete passes over the datasets.\n    iter_end: int\n        End running iters.\n        Note that `step_end` not initialised since this variable may not apply to some Runners.\n    step_end: int\n        End running steps.\n        Note that `step_end` not initialised since this variable may not apply to some Runners.\n    epoch_end: int\n        End running epochs.\n        Note that `epoch_end` not initialised since this variable may not apply to some Runners.\n    model: Optional = None\n    criterion: Optional = None\n    optimizer: Optional = None\n    scheduler: Optional = None\n    datasets: FlatDict\n        All datasets, should be in the form of ``{subset: dataset}``.\n    datasamplers: FlatDict\n        All datasamplers, should be in the form of ``{subset: datasampler}``.\n    dataloaders: FlatDict\n        All dataloaders, should be in the form of ``{subset: dataloader}``.\n    batch_size: int = 1\n    results: List[NestedDict] = []\n        All results, should be in the form of ``[{subset: {index: score}}]``.\n    index_set: str = 'val'\n        The subset to calculate the core score.\n    index: str = 'loss'\n        The index to calculate the core score.\n    experiments_root: str = \"experiments\"\n        The root directory for all experiments.\n    checkpoint_dir_name: str = \"checkpoints\"\n        The name of the directory under `runner.dir` to save checkpoints.\n    log: bool = True\n        Whether to log the results.\n    logger: Optional[logging.Logger] = None\n    tensorboard: bool = False\n        Whether to use tensorboard.\n    writer: Optional[SummaryWriter] = None\n\n    Notes\n    -----\n    The `RunnerBase` class is not intended to be used directly, nor to be directly inherit from.\n\n    This is because `RunnerBase` is designed as a \"dataclass\",\n    and is meant for demonstrating all attributes and properties only.\n\n    See Also\n    --------\n    [`BaseRunner`][danling.base_runner.BaseRunner]: The base runner class.\n    \"\"\"\n\n    # pylint: disable=R0902, R0904\n\n    id: str = \"\"\n    name: str = \"DanLing\"\n\n    seed: int\n    deterministic: bool\n\n    iters: int\n    steps: int\n    epochs: int\n    # iter_begin: int  # Deprecated\n    # step_begin: int  # Deprecated\n    # epoch_begin: int  # Deprecated\n    iter_end: int\n    step_end: int\n    epoch_end: int\n\n    model: Optional = None\n    criterion: Optional = None\n    optimizer: Optional = None\n    scheduler: Optional = None\n\n    datasets: FlatDict\n    datasamplers: FlatDict\n    dataloaders: FlatDict\n\n    batch_size: int\n\n    results: List[NestedDict] = []\n    index_set: Optional[str]\n    index: str\n\n    experiments_root: str = \"experiments\"\n    checkpoint_dir_name: str = \"checkpoints\"\n    log: bool\n    logger: Optional[logging.Logger] = None\n    tensorboard: bool\n    writer: Optional = None\n\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        # Init attributes that should be kept in checkpoint inside `__init__`.\n        # Note that attributes should be init before redefine `self.__dict__`.\n        self.deterministic = False\n        self.iters = 0\n        self.steps = 0\n        self.epochs = 0\n        self.batch_size = 1\n        self.seed = randint(0, 2**32 - 1)\n        self.datasets = FlatDict()\n        self.datasamplers = FlatDict()\n        self.dataloaders = FlatDict()\n        self.index_set = None\n        self.index = \"loss\"\n        if len(args) == 1 and isinstance(args[0], FlatDict) and not kwargs:\n            args, kwargs = (), args[0]\n        self.__dict__ = NestedDict(**self.__dict__)\n        self.__dict__.update(args)\n        self.__dict__.update(kwargs)\n        if not self.id:\n            self.id = f\"{self.name}-{self.seed}\"  # pylint: disable=C0103\n\n    @property\n    def progress(self) -&gt; float:\n        r\"\"\"\n        Training Progress.\n\n        Returns\n        -------\n        float\n\n        Raises\n        ------\n        RuntimeError\n            If no terminal is defined.\n        \"\"\"\n\n        if hasattr(self, \"iter_end\"):\n            return self.iters / self.iter_end\n        if hasattr(self, \"step_end\"):\n            return self.steps / self.step_end\n        if hasattr(self, \"epoch_end\"):\n            return self.epochs / self.epoch_end\n        raise RuntimeError(\"DanLing cannot determine progress since no terminal is defined.\")\n\n    @property\n    def batch_size_equivalent(self) -&gt; int:\n        r\"\"\"\n        Actual batch size.\n\n        `batch_size` * `world_size` * `accum_steps`\n        \"\"\"\n\n        return self.batch_size * self.world_size * getattr(self, \"accum_steps\", 1)\n\n    @property\n    def latest_result(self) -&gt; Optional[NestedDict]:\n        r\"\"\"\n        Latest result.\n\n        Returns\n        -------\n        Optional[NestedDict]\n        \"\"\"\n\n        return self.results[-1] if self.results else None\n\n    @property\n    def best_result(self) -&gt; Optional[NestedDict]:\n        r\"\"\"\n        Best result.\n\n        Returns\n        -------\n        Optional[NestedDict]\n        \"\"\"\n\n        return self.results[-1 - self.scores[::-1].index(self.best_score)] if self.results else None  # type: ignore\n\n    @property\n    def scores(self) -&gt; List[float]:\n        r\"\"\"\n        All scores.\n\n        Scores are extracted from results by `index_set` and `runner.index`,\n        following `[r[index_set][self.index] for r in self.results]`.\n\n        By default, `index_set` points to `self.index_set` and is set to `val`,\n        if `self.index_set` is not set, it will be the last key of the last result.\n\n        Scores are considered as the index of the performance of the model.\n        It is useful to determine the best model and the best hyper-parameters.\n\n        Returns\n        -------\n        List[float]\n        \"\"\"\n\n        if not self.results:\n            return []\n        index_set = self.index_set or next(reversed(self.results[-1]))\n        return [r[index_set][self.index] for r in self.results]\n\n    @property\n    def latest_score(self) -&gt; Optional[float]:\n        r\"\"\"\n        Latest score.\n\n        Returns\n        -------\n        Optional[float]\n        \"\"\"\n\n        return self.scores[-1] if self.results else None\n\n    @property\n    def best_score(self) -&gt; Optional[float]:\n        r\"\"\"\n        Best score.\n\n        Returns\n        -------\n        Optional[float]\n        \"\"\"\n\n        return self.best_fn(self.scores) if self.results else None\n\n    @staticmethod\n    def best_fn(scores: Sequence[float], fn: Callable = max) -&gt; float:  # pylint: disable=C0103\n        r\"\"\"\n        Function to determine the best score from a list of scores.\n\n        Subclass can override this method to accommodate needs, such as `min(scores)`.\n\n        Parameters\n        ----------\n        scores: Sequence[float]\n            List of scores.\n        fn: Callable = max\n            Function to determine the best score from a list of scores.\n\n        Returns\n        -------\n        best_score: float\n            The best score from a list of scores.\n        \"\"\"\n\n        return fn(scores)\n\n    @property\n    def is_best(self) -&gt; bool:\n        r\"\"\"\n        If current epoch is the best epoch.\n\n        Returns\n        -------\n        bool\n        \"\"\"\n\n        return abs(self.latest_score - self.best_score) &lt; 1e-7\n        # return self.latest_score == self.best_score\n\n    @property\n    def world_size(self) -&gt; int:\n        r\"\"\"\n        Number of Processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return 1\n\n    @property\n    def rank(self) -&gt; int:\n        r\"\"\"\n        Process index in all processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return 0\n\n    @property\n    def local_rank(self) -&gt; int:\n        r\"\"\"\n        Process index in local processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return 0\n\n    @property\n    def distributed(self) -&gt; bool:\n        r\"\"\"\n        If runner is running in distributed mode.\n        \"\"\"\n\n        return self.world_size &gt; 1\n\n    @property\n    def is_main_process(self) -&gt; bool:\n        r\"\"\"\n        If current process is the main process.\n\n        Returns\n        -------\n        bool\n        \"\"\"\n\n        return self.rank == 0\n\n    @property\n    def is_local_main_process(self) -&gt; bool:\n        r\"\"\"\n        If current process is the main process in local.\n\n        Returns\n        -------\n        bool\n        \"\"\"\n\n        return self.local_rank == 0\n\n    @property\n    @ensure_dir\n    def dir(self) -&gt; str:\n        r\"\"\"\n        Directory of the experiment.\n\n        Returns\n        -------\n        str\n        \"\"\"\n\n        return os.path.join(self.experiments_root, self.id)\n\n    @property\n    def log_path(self) -&gt; str:\n        r\"\"\"\n        Path of log file.\n\n        Returns\n        -------\n        str\n        \"\"\"\n\n        return os.path.join(self.dir, \"run.log\")\n\n    @property\n    @ensure_dir\n    def checkpoint_dir(self) -&gt; str:\n        r\"\"\"\n        Directory of checkpoints.\n\n        Returns\n        -------\n        str\n        \"\"\"\n\n        return os.path.join(self.dir, self.checkpoint_dir_name)\n\n    @catch\n    def save(self, obj: Any, f: File, main_process_only: bool = True) -&gt; File:  # pylint: disable=C0103\n        r\"\"\"\n        Save object to a path or file.\n\n        Returns\n        -------\n        File\n        \"\"\"\n\n        raise NotImplementedError\n\n    @staticmethod\n    def load(f: File, *args, **kwargs) -&gt; Any:  # pylint: disable=C0103\n        r\"\"\"\n        Load object from a path or file.\n\n        Returns\n        -------\n        Any\n        \"\"\"\n\n        raise NotImplementedError\n\n    def dict(self, cls: Callable = dict, only_json_serializable: bool = True) -&gt; Mapping:\n        r\"\"\"\n        Convert config to Mapping.\n\n        Note that all non-json-serializable objects will be removed.\n\n        Parameters\n        ----------\n        cls : Callable = dict\n            Class to convert to.\n        only_json_serializable : bool = True\n            If only json serializable objects should be kept.\n\n        Returns\n        -------\n        Mapping\n        \"\"\"\n\n        # pylint: disable=C0103\n\n        ret = cls()\n        for k, v in self.__dict__.items():\n            if isinstance(v, FlatDict):\n                v = v.dict(cls)\n            if not only_json_serializable or is_json_serializable(v):\n                ret[k] = v\n        return ret\n\n    @catch\n    def json(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=W1113\n        r\"\"\"\n        Dump Runner to json file.\n        \"\"\"\n\n        if main_process_only and self.is_main_process or not main_process_only:\n            with FlatDict.open(file, mode=\"w\") as fp:  # pylint: disable=C0103\n                fp.write(self.jsons(*args, **kwargs))\n\n    @classmethod\n    def from_json(cls, file: File, *args, **kwargs) -&gt; RunnerBase:\n        r\"\"\"\n        Construct Runner from json file.\n\n        This function calls `self.from_jsons()` to construct object from json string.\n        You may overwrite `from_jsons` in case something is not json serializable.\n\n        Returns\n        -------\n        RunnerBase\n        \"\"\"\n\n        with FlatDict.open(file) as fp:  # pylint: disable=C0103\n            return cls.from_jsons(fp.read(), *args, **kwargs)\n\n    def jsons(self, *args, **kwargs) -&gt; str:\n        r\"\"\"\n        Dump Runner to json string.\n\n        Returns\n        -------\n        json: str\n        \"\"\"\n\n        if \"cls\" not in kwargs:\n            kwargs[\"cls\"] = JsonEncoder\n        return json_dumps(self.dict(), *args, **kwargs)\n\n    @classmethod\n    def from_jsons(cls, string: str, *args, **kwargs) -&gt; RunnerBase:\n        r\"\"\"\n        Construct Runner from json string.\n\n        Returns\n        -------\n        RunnerBase\n        \"\"\"\n\n        return cls(**Config.from_jsons(string, *args, **kwargs))\n\n    @catch\n    def yaml(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=W1113\n        r\"\"\"\n        Dump Runner to yaml file.\n        \"\"\"\n\n        if main_process_only and self.is_main_process or not main_process_only:\n            with FlatDict.open(file, mode=\"w\") as fp:  # pylint: disable=C0103\n                self.yamls(fp, *args, **kwargs)\n\n    @classmethod\n    def from_yaml(cls, file: File, *args, **kwargs) -&gt; RunnerBase:\n        r\"\"\"\n        Construct Runner from yaml file.\n\n        This function calls `self.from_yamls()` to construct object from yaml string.\n        You may overwrite `from_yamls` in case something is not yaml serializable.\n\n        Returns\n        -------\n        RunnerBase\n        \"\"\"\n\n        with FlatDict.open(file) as fp:  # pylint: disable=C0103\n            return cls.from_yamls(fp.read(), *args, **kwargs)\n\n    def yamls(self, *args, **kwargs) -&gt; str:\n        r\"\"\"\n        Dump Runner to yaml string.\n\n        Returns\n        -------\n        yaml: str\n        \"\"\"\n\n        if \"Dumper\" not in kwargs:\n            kwargs[\"Dumper\"] = YamlDumper\n        return yaml_dump(self.dict(), *args, **kwargs)  # type: ignore\n\n    @classmethod\n    def from_yamls(cls, string: str, *args, **kwargs) -&gt; RunnerBase:\n        r\"\"\"\n        Construct Runner from yaml string.\n\n        Returns\n        -------\n        RunnerBase\n        \"\"\"\n\n        return cls(**Config.from_yamls(string, *args, **kwargs))\n\n    def __getattr__(self, name) -&gt; Any:\n        if \"accelerator\" not in self:\n            raise RuntimeError(f\"{self.__class__.__name__} is not properly initialised\")\n        if hasattr(self.accelerator, name):\n            return getattr(self.accelerator, name)\n        raise AttributeError(f\"{self.__class__.__name__} does not contain {name}\")\n\n    def __contains__(self, name) -&gt; bool:\n        return name in self.__dict__\n\n    def __repr__(self):\n        lines = []\n        for key, value in self.__dict__.items():\n            value_str = repr(value)\n            value_str = self._add_indent(value_str)\n            lines.append(\"(\" + key + \"): \" + value_str)\n\n        main_str = self.__class__.__name__ + \"(\"\n        if lines:\n            main_str += \"\\n  \" + \"\\n  \".join(lines) + \"\\n\"\n\n        main_str += \")\"\n        return main_str\n\n    def _add_indent(self, text):\n        lines = text.split(\"\\n\")\n        # don't do anything for single-line stuff\n        if len(lines) == 1:\n            return text\n        first = lines.pop(0)\n        # add 2 spaces to each line but the first\n        lines = [(2 * \" \") + line for line in lines]\n        lines = \"\\n\".join(lines)\n        lines = first + \"\\n\" + lines\n        return lines\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase--see-also"},{"title":"<code>progress()</code>  <code>property</code>","text":"<p>Training Progress.</p> <p>Returns:</p>    Type Description      <code>float</code>      <p>Raises:</p>    Type Description      <code>RuntimeError</code>  <p>If no terminal is defined.</p>     Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef progress(self) -&gt; float:\n    r\"\"\"\n    Training Progress.\n\n    Returns\n    -------\n    float\n\n    Raises\n    ------\n    RuntimeError\n        If no terminal is defined.\n    \"\"\"\n\n    if hasattr(self, \"iter_end\"):\n        return self.iters / self.iter_end\n    if hasattr(self, \"step_end\"):\n        return self.steps / self.step_end\n    if hasattr(self, \"epoch_end\"):\n        return self.epochs / self.epoch_end\n    raise RuntimeError(\"DanLing cannot determine progress since no terminal is defined.\")\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.progress"},{"title":"<code>batch_size_equivalent()</code>  <code>property</code>","text":"<p>Actual batch size.</p> <p><code>batch_size</code> * <code>world_size</code> * <code>accum_steps</code></p>  Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef batch_size_equivalent(self) -&gt; int:\n    r\"\"\"\n    Actual batch size.\n\n    `batch_size` * `world_size` * `accum_steps`\n    \"\"\"\n\n    return self.batch_size * self.world_size * getattr(self, \"accum_steps\", 1)\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.batch_size_equivalent"},{"title":"<code>best_fn(scores, fn=max)</code>  <code>staticmethod</code>","text":"<p>Function to determine the best score from a list of scores.</p> <p>Subclass can override this method to accommodate needs, such as <code>min(scores)</code>.</p> <p>Parameters:</p>    Name Type Description Default     <code>scores</code>  <code>Sequence[float]</code>  <p>List of scores.</p>  required    <code>fn</code>  <code>Callable</code>  <p>Function to determine the best score from a list of scores.</p>  <code>max</code>     <p>Returns:</p>    Name Type Description     <code>best_score</code>  <code>float</code>  <p>The best score from a list of scores.</p>     Source code in <code>danling/runner/bases.py</code> Python<pre><code>@staticmethod\ndef best_fn(scores: Sequence[float], fn: Callable = max) -&gt; float:  # pylint: disable=C0103\n    r\"\"\"\n    Function to determine the best score from a list of scores.\n\n    Subclass can override this method to accommodate needs, such as `min(scores)`.\n\n    Parameters\n    ----------\n    scores: Sequence[float]\n        List of scores.\n    fn: Callable = max\n        Function to determine the best score from a list of scores.\n\n    Returns\n    -------\n    best_score: float\n        The best score from a list of scores.\n    \"\"\"\n\n    return fn(scores)\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.best_fn"},{"title":"<code>latest_result()</code>  <code>property</code>","text":"<p>Latest result.</p> <p>Returns:</p>    Type Description      <code>Optional[NestedDict]</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef latest_result(self) -&gt; Optional[NestedDict]:\n    r\"\"\"\n    Latest result.\n\n    Returns\n    -------\n    Optional[NestedDict]\n    \"\"\"\n\n    return self.results[-1] if self.results else None\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.latest_result"},{"title":"<code>best_result()</code>  <code>property</code>","text":"<p>Best result.</p> <p>Returns:</p>    Type Description      <code>Optional[NestedDict]</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef best_result(self) -&gt; Optional[NestedDict]:\n    r\"\"\"\n    Best result.\n\n    Returns\n    -------\n    Optional[NestedDict]\n    \"\"\"\n\n    return self.results[-1 - self.scores[::-1].index(self.best_score)] if self.results else None  # type: ignore\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.best_result"},{"title":"<code>scores()</code>  <code>property</code>","text":"<p>All scores.</p> <p>Scores are extracted from results by <code>index_set</code> and <code>runner.index</code>, following <code>[r[index_set][self.index] for r in self.results]</code>.</p> <p>By default, <code>index_set</code> points to <code>self.index_set</code> and is set to <code>val</code>, if <code>self.index_set</code> is not set, it will be the last key of the last result.</p> <p>Scores are considered as the index of the performance of the model. It is useful to determine the best model and the best hyper-parameters.</p> <p>Returns:</p>    Type Description      <code>List[float]</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef scores(self) -&gt; List[float]:\n    r\"\"\"\n    All scores.\n\n    Scores are extracted from results by `index_set` and `runner.index`,\n    following `[r[index_set][self.index] for r in self.results]`.\n\n    By default, `index_set` points to `self.index_set` and is set to `val`,\n    if `self.index_set` is not set, it will be the last key of the last result.\n\n    Scores are considered as the index of the performance of the model.\n    It is useful to determine the best model and the best hyper-parameters.\n\n    Returns\n    -------\n    List[float]\n    \"\"\"\n\n    if not self.results:\n        return []\n    index_set = self.index_set or next(reversed(self.results[-1]))\n    return [r[index_set][self.index] for r in self.results]\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.scores"},{"title":"<code>latest_score()</code>  <code>property</code>","text":"<p>Latest score.</p> <p>Returns:</p>    Type Description      <code>Optional[float]</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef latest_score(self) -&gt; Optional[float]:\n    r\"\"\"\n    Latest score.\n\n    Returns\n    -------\n    Optional[float]\n    \"\"\"\n\n    return self.scores[-1] if self.results else None\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.latest_score"},{"title":"<code>best_score()</code>  <code>property</code>","text":"<p>Best score.</p> <p>Returns:</p>    Type Description      <code>Optional[float]</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef best_score(self) -&gt; Optional[float]:\n    r\"\"\"\n    Best score.\n\n    Returns\n    -------\n    Optional[float]\n    \"\"\"\n\n    return self.best_fn(self.scores) if self.results else None\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.best_score"},{"title":"<code>is_best()</code>  <code>property</code>","text":"<p>If current epoch is the best epoch.</p> <p>Returns:</p>    Type Description      <code>bool</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef is_best(self) -&gt; bool:\n    r\"\"\"\n    If current epoch is the best epoch.\n\n    Returns\n    -------\n    bool\n    \"\"\"\n\n    return abs(self.latest_score - self.best_score) &lt; 1e-7\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.is_best"},{"title":"<code>world_size()</code>  <code>property</code>","text":"<p>Number of Processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef world_size(self) -&gt; int:\n    r\"\"\"\n    Number of Processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return 1\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.world_size"},{"title":"<code>rank()</code>  <code>property</code>","text":"<p>Process index in all processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef rank(self) -&gt; int:\n    r\"\"\"\n    Process index in all processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return 0\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.rank"},{"title":"<code>local_rank()</code>  <code>property</code>","text":"<p>Process index in local processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef local_rank(self) -&gt; int:\n    r\"\"\"\n    Process index in local processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return 0\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.local_rank"},{"title":"<code>distributed()</code>  <code>property</code>","text":"<p>If runner is running in distributed mode.</p>  Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef distributed(self) -&gt; bool:\n    r\"\"\"\n    If runner is running in distributed mode.\n    \"\"\"\n\n    return self.world_size &gt; 1\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.distributed"},{"title":"<code>is_main_process()</code>  <code>property</code>","text":"<p>If current process is the main process.</p> <p>Returns:</p>    Type Description      <code>bool</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef is_main_process(self) -&gt; bool:\n    r\"\"\"\n    If current process is the main process.\n\n    Returns\n    -------\n    bool\n    \"\"\"\n\n    return self.rank == 0\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.is_main_process"},{"title":"<code>is_local_main_process()</code>  <code>property</code>","text":"<p>If current process is the main process in local.</p> <p>Returns:</p>    Type Description      <code>bool</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef is_local_main_process(self) -&gt; bool:\n    r\"\"\"\n    If current process is the main process in local.\n\n    Returns\n    -------\n    bool\n    \"\"\"\n\n    return self.local_rank == 0\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.is_local_main_process"},{"title":"<code>dir()</code>  <code>property</code>","text":"<p>Directory of the experiment.</p> <p>Returns:</p>    Type Description      <code>str</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\n@ensure_dir\ndef dir(self) -&gt; str:\n    r\"\"\"\n    Directory of the experiment.\n\n    Returns\n    -------\n    str\n    \"\"\"\n\n    return os.path.join(self.experiments_root, self.id)\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.dir"},{"title":"<code>log_path()</code>  <code>property</code>","text":"<p>Path of log file.</p> <p>Returns:</p>    Type Description      <code>str</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef log_path(self) -&gt; str:\n    r\"\"\"\n    Path of log file.\n\n    Returns\n    -------\n    str\n    \"\"\"\n\n    return os.path.join(self.dir, \"run.log\")\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.log_path"},{"title":"<code>checkpoint_dir()</code>  <code>property</code>","text":"<p>Directory of checkpoints.</p> <p>Returns:</p>    Type Description      <code>str</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\n@ensure_dir\ndef checkpoint_dir(self) -&gt; str:\n    r\"\"\"\n    Directory of checkpoints.\n\n    Returns\n    -------\n    str\n    \"\"\"\n\n    return os.path.join(self.dir, self.checkpoint_dir_name)\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.checkpoint_dir"},{"title":"<code>save(obj, f, main_process_only=True)</code>","text":"<p>Save object to a path or file.</p> <p>Returns:</p>    Type Description      <code>File</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@catch\ndef save(self, obj: Any, f: File, main_process_only: bool = True) -&gt; File:  # pylint: disable=C0103\n    r\"\"\"\n    Save object to a path or file.\n\n    Returns\n    -------\n    File\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.save"},{"title":"<code>load(f, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Load object from a path or file.</p> <p>Returns:</p>    Type Description      <code>Any</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@staticmethod\ndef load(f: File, *args, **kwargs) -&gt; Any:  # pylint: disable=C0103\n    r\"\"\"\n    Load object from a path or file.\n\n    Returns\n    -------\n    Any\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.load"},{"title":"<code>dict(cls=dict, only_json_serializable=True)</code>","text":"<p>Convert config to Mapping.</p> <p>Note that all non-json-serializable objects will be removed.</p> <p>Parameters:</p>    Name Type Description Default     <code>cls</code>  <code>Callable</code>  <p>Class to convert to.</p>  <code>dict</code>    <code>only_json_serializable</code>  <code>bool</code>  <p>If only json serializable objects should be kept.</p>  <code>True</code>     <p>Returns:</p>    Type Description      <code>Mapping</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>def dict(self, cls: Callable = dict, only_json_serializable: bool = True) -&gt; Mapping:\n    r\"\"\"\n    Convert config to Mapping.\n\n    Note that all non-json-serializable objects will be removed.\n\n    Parameters\n    ----------\n    cls : Callable = dict\n        Class to convert to.\n    only_json_serializable : bool = True\n        If only json serializable objects should be kept.\n\n    Returns\n    -------\n    Mapping\n    \"\"\"\n\n    # pylint: disable=C0103\n\n    ret = cls()\n    for k, v in self.__dict__.items():\n        if isinstance(v, FlatDict):\n            v = v.dict(cls)\n        if not only_json_serializable or is_json_serializable(v):\n            ret[k] = v\n    return ret\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.dict"},{"title":"<code>json(file, main_process_only=True, *args, **kwargs)</code>","text":"<p>Dump Runner to json file.</p>  Source code in <code>danling/runner/bases.py</code> Python<pre><code>@catch\ndef json(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=W1113\n    r\"\"\"\n    Dump Runner to json file.\n    \"\"\"\n\n    if main_process_only and self.is_main_process or not main_process_only:\n        with FlatDict.open(file, mode=\"w\") as fp:  # pylint: disable=C0103\n            fp.write(self.jsons(*args, **kwargs))\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.json"},{"title":"<code>from_json(file, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from json file.</p> <p>This function calls <code>self.from_jsons()</code> to construct object from json string. You may overwrite <code>from_jsons</code> in case something is not json serializable.</p> <p>Returns:</p>    Type Description      <code>RunnerBase</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@classmethod\ndef from_json(cls, file: File, *args, **kwargs) -&gt; RunnerBase:\n    r\"\"\"\n    Construct Runner from json file.\n\n    This function calls `self.from_jsons()` to construct object from json string.\n    You may overwrite `from_jsons` in case something is not json serializable.\n\n    Returns\n    -------\n    RunnerBase\n    \"\"\"\n\n    with FlatDict.open(file) as fp:  # pylint: disable=C0103\n        return cls.from_jsons(fp.read(), *args, **kwargs)\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.from_json"},{"title":"<code>jsons(*args, **kwargs)</code>","text":"<p>Dump Runner to json string.</p> <p>Returns:</p>    Name Type Description     <code>json</code>  <code>str</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>def jsons(self, *args, **kwargs) -&gt; str:\n    r\"\"\"\n    Dump Runner to json string.\n\n    Returns\n    -------\n    json: str\n    \"\"\"\n\n    if \"cls\" not in kwargs:\n        kwargs[\"cls\"] = JsonEncoder\n    return json_dumps(self.dict(), *args, **kwargs)\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.jsons"},{"title":"<code>from_jsons(string, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from json string.</p> <p>Returns:</p>    Type Description      <code>RunnerBase</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@classmethod\ndef from_jsons(cls, string: str, *args, **kwargs) -&gt; RunnerBase:\n    r\"\"\"\n    Construct Runner from json string.\n\n    Returns\n    -------\n    RunnerBase\n    \"\"\"\n\n    return cls(**Config.from_jsons(string, *args, **kwargs))\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.from_jsons"},{"title":"<code>yaml(file, main_process_only=True, *args, **kwargs)</code>","text":"<p>Dump Runner to yaml file.</p>  Source code in <code>danling/runner/bases.py</code> Python<pre><code>@catch\ndef yaml(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=W1113\n    r\"\"\"\n    Dump Runner to yaml file.\n    \"\"\"\n\n    if main_process_only and self.is_main_process or not main_process_only:\n        with FlatDict.open(file, mode=\"w\") as fp:  # pylint: disable=C0103\n            self.yamls(fp, *args, **kwargs)\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.yaml"},{"title":"<code>from_yaml(file, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from yaml file.</p> <p>This function calls <code>self.from_yamls()</code> to construct object from yaml string. You may overwrite <code>from_yamls</code> in case something is not yaml serializable.</p> <p>Returns:</p>    Type Description      <code>RunnerBase</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@classmethod\ndef from_yaml(cls, file: File, *args, **kwargs) -&gt; RunnerBase:\n    r\"\"\"\n    Construct Runner from yaml file.\n\n    This function calls `self.from_yamls()` to construct object from yaml string.\n    You may overwrite `from_yamls` in case something is not yaml serializable.\n\n    Returns\n    -------\n    RunnerBase\n    \"\"\"\n\n    with FlatDict.open(file) as fp:  # pylint: disable=C0103\n        return cls.from_yamls(fp.read(), *args, **kwargs)\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.from_yaml"},{"title":"<code>yamls(*args, **kwargs)</code>","text":"<p>Dump Runner to yaml string.</p> <p>Returns:</p>    Name Type Description     <code>yaml</code>  <code>str</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>def yamls(self, *args, **kwargs) -&gt; str:\n    r\"\"\"\n    Dump Runner to yaml string.\n\n    Returns\n    -------\n    yaml: str\n    \"\"\"\n\n    if \"Dumper\" not in kwargs:\n        kwargs[\"Dumper\"] = YamlDumper\n    return yaml_dump(self.dict(), *args, **kwargs)  # type: ignore\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.yamls"},{"title":"<code>from_yamls(string, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from yaml string.</p> <p>Returns:</p>    Type Description      <code>RunnerBase</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@classmethod\ndef from_yamls(cls, string: str, *args, **kwargs) -&gt; RunnerBase:\n    r\"\"\"\n    Construct Runner from yaml string.\n\n    Returns\n    -------\n    RunnerBase\n    \"\"\"\n\n    return cls(**Config.from_yamls(string, *args, **kwargs))\n</code></pre>","location":"en/runner/bases/#danling.runner.bases.RunnerBase.from_yamls"},{"title":"TorchRunner","text":"<p>         Bases: <code>BaseRunner</code></p> <p>Set up everything for running a job.</p> <p>Attributes:</p>    Name Type Description     <code>accelerator</code>  <code>Accelerator</code>     <code>accelerate</code>  <code>Dict[str, Any] = {}</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>class TorchRunner(BaseRunner):\n    r\"\"\"\n    Set up everything for running a job.\n\n    Attributes\n    ----------\n    accelerator: Accelerator\n    accelerate: Dict[str, Any] = {}\n    \"\"\"\n\n    # pylint: disable=R0902\n\n    accelerator: Accelerator\n    accelerate: Dict[str, Any] = {}\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        self.accelerator = Accelerator(**self.accelerate)\n        super().__init__(*args, **kwargs)\n\n    @on_main_process\n    def init_tensorboard(self, *args, **kwargs) -&gt; None:\n        r\"\"\"\n        Set up Tensoraoard SummaryWriter.\n        \"\"\"\n        from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n        if \"log_dir\" not in kwargs:\n            kwargs[\"log_dir\"] = self.dir\n\n        self.writer = SummaryWriter(*args, **kwargs)\n\n    def set_seed(self, bias: Optional[int] = None) -&gt; None:\n        r\"\"\"\n        Set up random seed.\n\n        Parameters\n        ----------\n        bias: Optional[int] = self.rank\n            Make the seed different for each processes.\n            This avoids same data augmentation are applied on every processes.\n            Set to `False` to disable this feature.\n        \"\"\"\n\n        if self.distributed:\n            # TODO: use broadcast_object instead.\n            self.seed = (\n                self.accelerator.gather(torch.tensor(self.seed).cuda()).unsqueeze(0).flatten()[0]  # pylint: disable=E1101\n            )\n        if bias is None:\n            bias = self.rank\n        seed = self.seed + bias if bias else self.seed\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        np.random.seed(seed)\n        random.seed(seed)\n\n    def set_deterministic(self) -&gt; None:\n        r\"\"\"\n        Set up deterministic.\n        \"\"\"\n\n        cudnn.benchmark = False\n        cudnn.deterministic = True\n        if torch.__version__ &gt;= \"1.8.0\":\n            torch.use_deterministic_algorithms(True)\n\n    def init_distributed(self) -&gt; None:\n        r\"\"\"\n        Set up distributed training.\n\n        Initialise process group and set up DDP variables.\n\n        .. deprecated:: 0.1.0\n            `init_distributed` is deprecated in favor of `Accelerator()`.\n        \"\"\"\n\n        # pylint: disable=W0201\n\n        dist.init_process_group(backend=\"nccl\")\n        self.rank = dist.get_rank()\n        self.world_size = dist.get_world_size()\n        self.local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n        torch.cuda.set_device(self.local_rank)\n        self.is_main_process = self.rank == 0\n        self.is_local_main_process = self.local_rank == 0\n\n    @catch\n    def save(self, obj: Any, f: File, main_process_only: bool = True) -&gt; File:  # pylint: disable=C0103\n        r\"\"\"\n        Save object to a path or file.\n        Returns\n        -------\n        File\n        \"\"\"\n\n        if main_process_only and self.is_main_process or not on_main_process:\n            self.accelerator.save(obj, f)\n        return f\n\n    @staticmethod\n    def load(f: File, *args, **kwargs) -&gt; Any:  # pylint: disable=C0103\n        r\"\"\"\n        Load object from a path or file.\n        Returns\n        -------\n        Any\n        \"\"\"\n\n        return torch.load(f, *args, **kwargs)  # type: ignore\n\n    @property\n    def world_size(self) -&gt; int:\n        r\"\"\"\n        Number of Processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return self.accelerator.num_processes\n\n    @property\n    def rank(self) -&gt; int:\n        r\"\"\"\n        Process index in all processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return self.accelerator.process_index\n\n    @property\n    def local_rank(self) -&gt; int:\n        r\"\"\"\n        Process index in local processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return self.accelerator.local_process_index\n</code></pre>","location":"en/runner/torch_runner/"},{"title":"<code>init_tensorboard(*args, **kwargs)</code>","text":"<p>Set up Tensoraoard SummaryWriter.</p>  Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@on_main_process\ndef init_tensorboard(self, *args, **kwargs) -&gt; None:\n    r\"\"\"\n    Set up Tensoraoard SummaryWriter.\n    \"\"\"\n    from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n    if \"log_dir\" not in kwargs:\n        kwargs[\"log_dir\"] = self.dir\n\n    self.writer = SummaryWriter(*args, **kwargs)\n</code></pre>","location":"en/runner/torch_runner/#danling.runner.torch_runner.TorchRunner.init_tensorboard"},{"title":"<code>save(obj, f, main_process_only=True)</code>","text":"<p>Save object to a path or file.</p> <p>Returns:</p>    Type Description      <code>File</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@catch\ndef save(self, obj: Any, f: File, main_process_only: bool = True) -&gt; File:  # pylint: disable=C0103\n    r\"\"\"\n    Save object to a path or file.\n    Returns\n    -------\n    File\n    \"\"\"\n\n    if main_process_only and self.is_main_process or not on_main_process:\n        self.accelerator.save(obj, f)\n    return f\n</code></pre>","location":"en/runner/torch_runner/#danling.runner.torch_runner.TorchRunner.save"},{"title":"<code>load(f, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Load object from a path or file.</p> <p>Returns:</p>    Type Description      <code>Any</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@staticmethod\ndef load(f: File, *args, **kwargs) -&gt; Any:  # pylint: disable=C0103\n    r\"\"\"\n    Load object from a path or file.\n    Returns\n    -------\n    Any\n    \"\"\"\n\n    return torch.load(f, *args, **kwargs)  # type: ignore\n</code></pre>","location":"en/runner/torch_runner/#danling.runner.torch_runner.TorchRunner.load"},{"title":"<code>world_size()</code>  <code>property</code>","text":"<p>Number of Processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@property\ndef world_size(self) -&gt; int:\n    r\"\"\"\n    Number of Processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return self.accelerator.num_processes\n</code></pre>","location":"en/runner/torch_runner/#danling.runner.torch_runner.TorchRunner.world_size"},{"title":"<code>rank()</code>  <code>property</code>","text":"<p>Process index in all processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@property\ndef rank(self) -&gt; int:\n    r\"\"\"\n    Process index in all processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return self.accelerator.process_index\n</code></pre>","location":"en/runner/torch_runner/#danling.runner.torch_runner.TorchRunner.rank"},{"title":"<code>local_rank()</code>  <code>property</code>","text":"<p>Process index in local processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@property\ndef local_rank(self) -&gt; int:\n    r\"\"\"\n    Process index in local processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return self.accelerator.local_process_index\n</code></pre>","location":"en/runner/torch_runner/#danling.runner.torch_runner.TorchRunner.local_rank"},{"title":"<code>set_deterministic()</code>","text":"<p>Set up deterministic.</p>  Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def set_deterministic(self) -&gt; None:\n    r\"\"\"\n    Set up deterministic.\n    \"\"\"\n\n    cudnn.benchmark = False\n    cudnn.deterministic = True\n    if torch.__version__ &gt;= \"1.8.0\":\n        torch.use_deterministic_algorithms(True)\n</code></pre>","location":"en/runner/torch_runner/#danling.runner.torch_runner.TorchRunner.set_deterministic"},{"title":"<code>set_seed(bias=None)</code>","text":"<p>Set up random seed.</p> <p>Parameters:</p>    Name Type Description Default     <code>bias</code>  <code>Optional[int]</code>  <p>Make the seed different for each processes. This avoids same data augmentation are applied on every processes. Set to <code>False</code> to disable this feature.</p>  <code>None</code>      Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def set_seed(self, bias: Optional[int] = None) -&gt; None:\n    r\"\"\"\n    Set up random seed.\n\n    Parameters\n    ----------\n    bias: Optional[int] = self.rank\n        Make the seed different for each processes.\n        This avoids same data augmentation are applied on every processes.\n        Set to `False` to disable this feature.\n    \"\"\"\n\n    if self.distributed:\n        # TODO: use broadcast_object instead.\n        self.seed = (\n            self.accelerator.gather(torch.tensor(self.seed).cuda()).unsqueeze(0).flatten()[0]  # pylint: disable=E1101\n        )\n    if bias is None:\n        bias = self.rank\n    seed = self.seed + bias if bias else self.seed\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n</code></pre>","location":"en/runner/torch_runner/#danling.runner.torch_runner.TorchRunner.set_seed"},{"title":"<code>init_tensorboard(*args, **kwargs)</code>","text":"<p>Set up Tensoraoard SummaryWriter.</p>  Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@on_main_process\ndef init_tensorboard(self, *args, **kwargs) -&gt; None:\n    r\"\"\"\n    Set up Tensoraoard SummaryWriter.\n    \"\"\"\n    from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n    if \"log_dir\" not in kwargs:\n        kwargs[\"log_dir\"] = self.dir\n\n    self.writer = SummaryWriter(*args, **kwargs)\n</code></pre>","location":"en/runner/torch_runner/#danling.runner.torch_runner.TorchRunner.init_tensorboard"},{"title":"Runner","text":"<p>The Runner of DanLing sets up the basic environment for running neural networks.</p>","location":"en/tensors/"},{"title":"Components","text":"<p>For readability and maintainability, there are three levels of Runner:</p>","location":"en/tensors/#components"},{"title":"[<code>RunnerBase</code>][danling.runner.bases.RunnerBase]","text":"<p>[<code>RunnerBase</code>][danling.runner.bases.RunnerBase] gives you a basic instinct on what attributes and properties are provided by the Runner.</p> <p>It works in an AbstractBaseClass manner and should neither be used directly nor be inherited from.</p>","location":"en/tensors/#runnerbasedanlingrunnerbasesrunnerbase"},{"title":"[<code>BaseRunner</code>][danling.runner.BaseRunner]","text":"<p>[<code>BaseRunner</code>][danling.runner.BaseRunner] contains core methods of general basic functionality, such as <code>init_logging</code>, <code>append_result</code>, <code>print_result</code>.</p>","location":"en/tensors/#baserunnerdanlingrunnerbaserunner"},{"title":"[<code>Runner</code>][danling.runner.TorchRunner].","text":"<p><code>Runner</code> should only contain platform-specific features. Currently, only [<code>TorchRunner</code>][danling.runner.TorchRunner] is supported.</p>","location":"en/tensors/#runnerdanlingrunnertorchrunner"},{"title":"Philosophy","text":"<p>DanLing Runner is designed for a 3.5-level experiments management system: Project, Group, Experiment, and, Run.</p>","location":"en/tensors/#philosophy"},{"title":"Project","text":"<p>Project corresponds to your project.</p> <p>Generally speaking, there should be only one project for each repository.</p>","location":"en/tensors/#project"},{"title":"Group","text":"<p>A group groups multiple experiments with similar characteristics.</p> <p>For example, if you run multiple experiments on learning rate, you may want to group them into a group.</p> <p>Note that Group is a virtual level (which is why it only counts 0.5) and does not corresponds to anything.</p>","location":"en/tensors/#group"},{"title":"Experiment","text":"<p>Experiment is the basic unit of experiments.</p> <p>Experiment usually corresponds to a commit, which means the code should be consistent across the experiment.</p>","location":"en/tensors/#experiment"},{"title":"Run","text":"<p>Run is the basic unit of running.</p> <p>Run corresponds to a single run of an experiment, each run may have different hyperparameters.</p>","location":"en/tensors/#run"},{"title":"Attributes &amp; Properties","text":"","location":"en/tensors/#attributes-properties"},{"title":"General","text":"<ul> <li>id</li> <li>name</li> <li>seed</li> <li>deterministic</li> </ul>","location":"en/tensors/#general"},{"title":"Progress","text":"<ul> <li>iters</li> <li>steps</li> <li>epochs</li> <li>iter_end</li> <li>step_end</li> <li>epoch_end</li> <li>progress</li> </ul> <p>Note that generally you should only use one of <code>iter</code>, <code>step</code>, <code>epoch</code> to indicate the progress.</p>","location":"en/tensors/#progress"},{"title":"Model","text":"<ul> <li>model</li> <li>criterion</li> <li>optimizer</li> <li>scheduler</li> </ul>","location":"en/tensors/#model"},{"title":"Data","text":"<ul> <li>datasets</li> <li>datasamplers</li> <li>dataloaders</li> <li>batch_size</li> <li>batch_size_equivalent</li> </ul> <p><code>datasets</code>, <code>datasamplers</code>, <code>dataloaders</code> should be a dict with the same keys. Their keys should be <code>split</code> (e.g. <code>train</code>, <code>val</code>, <code>test</code>).</p>","location":"en/tensors/#data"},{"title":"Results","text":"<ul> <li>results</li> <li>latest_result</li> <li>best_result</li> <li>scores</li> <li>latest_score</li> <li>best_score</li> <li>index_set</li> <li>index</li> <li>is_best</li> </ul> <p><code>results</code> should be a list of <code>result</code>. <code>result</code> should be a dict with the same <code>split</code> as keys, like <code>dataloaders</code>. A typical <code>result</code> might look like this: Python<pre><code>{\n    \"train\": {\n        \"loss\": 0.1,\n        \"accuracy\": 0.9,\n    },\n    \"val\": {\n        \"loss\": 0.2,\n        \"accuracy\": 0.8,\n    },\n    \"test\": {\n        \"loss\": 0.3,\n        \"accuracy\": 0.7,\n    },\n}\n</code></pre></p> <p><code>scores</code> are usually a list of <code>float</code>, and are dynamically extracted from <code>results</code> by <code>index_set</code> and <code>index</code>. If <code>index_set = \"val\"</code>, <code>index = \"accuracy\"</code>, then <code>scores = 0.9</code>.</p>","location":"en/tensors/#results"},{"title":"DDP","text":"<ul> <li>world_size</li> <li>rank</li> <li>local_rank</li> <li>distributed</li> <li>is_main_process</li> <li>is_local_main_process</li> </ul>","location":"en/tensors/#ddp"},{"title":"IO","text":"<ul> <li>experiments_root</li> <li>dir</li> <li>checkpoint_dir</li> <li>log_path</li> <li>checkpoint_dir_name</li> </ul> <p><code>experiments_root</code> is the root directory of all experiments, and should be consistent across the Project.</p> <p><code>dir</code> is the directory of a certain Run.</p> <p>There is no attributes/properties for Group and Experiment.</p> <p><code>checkpoint_dir_name</code> is relative to <code>dir</code>, and is passed to generate <code>checkpoint_dir</code> (<code>checkpoint_dir = os.path.join(dir, checkpoint_dir_name)</code>). In practice, <code>checkpoint_dir_name</code> is rarely called.</p>","location":"en/tensors/#io"},{"title":"logging","text":"<ul> <li>log</li> <li>logger</li> <li>tensorboard</li> <li>writer</li> </ul>","location":"en/tensors/#logging"},{"title":"NestedTensor","text":"<p>Wrap a sequence of tensors into a single tensor with a mask.</p> <p>In sequence to sequence tasks, elements of a batch are usually not of the same length. This made it tricky to use a single tensor to represent a batch of sequences.</p> <p>NestedTensor allows to store a sequence of tensors of different lengths in a single object. It also provides a mask that can be used to retrieve the original sequence of tensors.</p> <p>Attributes:</p>    Name Type Description     <code>storage</code>  <code>Sequence[torch.Tensor]</code>  <p>The sequence of tensors.</p>   <code>batch_first</code>  <code>bool = True</code>  <p>Whether the first dimension of the tensors is the batch dimension.</p> <p>If <code>True</code>, the first dimension is the batch dimension, i.e., <code>B, N, *</code>.</p> <p>If <code>False</code>, the first dimension is the sequence dimension, i.e., <code>N, B, *</code></p>    <p>Parameters:</p>    Name Type Description Default     <code>tensors</code>  <code>Sequence[Tensor]</code>    required    <code>batch_first</code>  <code>bool</code>    <code>True</code>     <p>Raises:</p>    Type Description      <code>ValueError</code>  <p>If <code>tensors</code> is not a sequence.</p> <p>If <code>tensors</code> is empty.</p>","location":"en/tensors/nested_tensor/"},{"title":"Notes","text":"<p>We have rewritten the <code>__getattr__</code> function to support as much native tensor operations as possible. However, not all operations are tested.</p> <p>Please file an issue if you find any bugs.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.device\ndevice(type='cpu')\n&gt;&gt;&gt; nested_tensor.dtype\ntorch.int64\n&gt;&gt;&gt; nested_tensor.tensor\ntensor([[1, 2, 3],\n        [4, 5, 0]])\n&gt;&gt;&gt; nested_tensor.mask\ntensor([[ True,  True,  True],\n        [ True,  True, False]])\n&gt;&gt;&gt; nested_tensor.to(torch.float).tensor\ntensor([[1., 2., 3.],\n        [4., 5., 0.]])\n&gt;&gt;&gt; nested_tensor.half().tensor\ntensor([[1., 2., 3.],\n        [4., 5., 0.]], dtype=torch.float16)\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>class NestedTensor:\n    r\"\"\"\n    Wrap a sequence of tensors into a single tensor with a mask.\n\n    In sequence to sequence tasks, elements of a batch are usually not of the same length.\n    This made it tricky to use a single tensor to represent a batch of sequences.\n\n    NestedTensor allows to store a sequence of tensors of different lengths in a single object.\n    It also provides a mask that can be used to retrieve the original sequence of tensors.\n\n    Attributes\n    ----------\n    storage: Sequence[torch.Tensor]\n        The sequence of tensors.\n    batch_first: bool = True\n        Whether the first dimension of the tensors is the batch dimension.\n\n        If `True`, the first dimension is the batch dimension, i.e., `B, N, *`.\n\n        If `False`, the first dimension is the sequence dimension, i.e., `N, B, *`\n\n    Parameters\n    ----------\n    tensors: Sequence[torch.Tensor]\n    batch_first: bool = True\n\n    Raises\n    ------\n    ValueError\n        If `tensors` is not a sequence.\n\n        If `tensors` is empty.\n\n    Notes\n    -----\n    We have rewritten the `__getattr__` function to support as much native tensor operations as possible.\n    However, not all operations are tested.\n\n    Please file an issue if you find any bugs.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.shape\n    torch.Size([2, 3])\n    &gt;&gt;&gt; nested_tensor.device\n    device(type='cpu')\n    &gt;&gt;&gt; nested_tensor.dtype\n    torch.int64\n    &gt;&gt;&gt; nested_tensor.tensor\n    tensor([[1, 2, 3],\n            [4, 5, 0]])\n    &gt;&gt;&gt; nested_tensor.mask\n    tensor([[ True,  True,  True],\n            [ True,  True, False]])\n    &gt;&gt;&gt; nested_tensor.to(torch.float).tensor\n    tensor([[1., 2., 3.],\n            [4., 5., 0.]])\n    &gt;&gt;&gt; nested_tensor.half().tensor\n    tensor([[1., 2., 3.],\n            [4., 5., 0.]], dtype=torch.float16)\n\n    ```\n    \"\"\"\n\n    storage: Sequence[Tensor] = []\n    batch_first: bool = True\n\n    def __init__(self, tensors: Sequence[Tensor], batch_first: bool = True) -&gt; None:\n        if not isinstance(tensors, Sequence):\n            raise ValueError(f\"NestedTensor should be initialised with a Sequence, bug got {type(values)}.\")\n        if len(tensors) == 0:\n            raise ValueError(f\"NestedTensor should be initialised with a non-empty sequence.\")\n        if not isinstance(tensors[0], Tensor):\n            tensors = tuple(torch.tensor(tensor) for tensor in tensors)\n        self.storage = tensors\n        self.batch_first = batch_first\n\n    @property\n    def tensor(self) -&gt; Tensor:\n        r\"\"\"\n        Return a single tensor by padding all the tensors.\n\n        Returns\n        -------\n        torch.Tensor\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.tensor\n        tensor([[1, 2, 3],\n                [4, 5, 0]])\n\n        ```\n        \"\"\"\n\n        return self._tensor(tuple(self.storage), self.batch_first)\n\n    @property\n    def mask(self) -&gt; Tensor:\n        r\"\"\"\n        Padding mask of `tensor`.\n\n        Returns\n        -------\n        torch.Tensor\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.mask\n        tensor([[ True,  True,  True],\n                [ True,  True, False]])\n\n        ```\n        \"\"\"\n\n        return self._mask(tuple(self.storage))\n\n    @property\n    def device(self) -&gt; torch.device:\n        r\"\"\"\n        Device of the NestedTensor.\n\n        Returns\n        -------\n        torch.Tensor\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.device\n        device(type='cpu')\n\n        ```\n        \"\"\"\n\n        return self._device(tuple(self.storage))\n\n    @property\n    def shape(self) -&gt; torch.Size:\n        r\"\"\"\n        Alias for `size`.\n\n        Returns\n        -------\n        torch.Size\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.shape\n        torch.Size([2, 3])\n        &gt;&gt;&gt; nested_tensor.storage.append(torch.tensor([6, 7, 8, 9]))\n        &gt;&gt;&gt; nested_tensor.shape\n        torch.Size([3, 4])\n\n        ```\n        \"\"\"\n\n        return self.size()\n\n    def size(self) -&gt; torch.Size:\n        r\"\"\"\n        Shape of the NestedTensor.\n\n        Returns\n        -------\n        torch.Size\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.size()\n        torch.Size([2, 3])\n        &gt;&gt;&gt; nested_tensor.storage[1] = torch.tensor([4, 5, 6, 7])\n        &gt;&gt;&gt; nested_tensor.size()\n        torch.Size([2, 4])\n\n        ```\n        \"\"\"\n\n        return self._size(tuple(self.storage))\n\n    def __getitem__(self, index) -&gt; Tuple[Tensor, Tensor]:\n        ret = self.storage[index]\n        if isinstance(ret, Tensor):\n            return ret, torch.ones_like(ret)  # pylint: disable=E1101\n        return self.tensor, self.mask\n\n    def __getattr__(self, name) -&gt; Any:\n        if not self.storage:\n            raise ValueError(f\"Unable to get {name} from an empty {self.__class__.__name__}\")\n        ret = [getattr(i, name) for i in self.storage]\n        elem = ret[0]\n        if isinstance(elem, Tensor):\n            return NestedTensor(ret)\n        if callable(elem):\n            return _TensorFuncWrapper(ret)\n        if len(set(ret)) == 1:\n            return elem\n        return ret\n\n    def __len__(self) -&gt; int:\n        return len(self.storage)\n\n    def __setstate__(self, storage) -&gt; None:\n        self.storage = storage\n\n    def __getstate__(self) -&gt; Sequence[Tensor]:\n        return self.storage\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _tensor(storage, batch_first) -&gt; Tensor:\n        return pad_sequence(storage, batch_first=batch_first)\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _mask(storage) -&gt; Tensor:\n        lens = torch.tensor([len(t) for t in storage], device=storage[0].device)\n        return torch.arange(max(lens), device=storage[0].device)[None, :] &lt; lens[:, None]  # pylint: disable=E1101\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _device(storage) -&gt; torch.device:\n        return storage[0].device\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _size(storage) -&gt; torch.Size:\n        return torch.Size(  # pylint: disable=E1101\n            [len(storage), max(t.shape[0] for t in storage), *storage[0].shape[1:]]\n        )\n</code></pre>","location":"en/tensors/nested_tensor/#danling.tensors.NestedTensor--notes"},{"title":"<code>device()</code>  <code>property</code>","text":"<p>Device of the NestedTensor.</p> <p>Returns:</p>    Type Description      <code>torch.Tensor</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.device\ndevice(type='cpu')\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@property\ndef device(self) -&gt; torch.device:\n    r\"\"\"\n    Device of the NestedTensor.\n\n    Returns\n    -------\n    torch.Tensor\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.device\n    device(type='cpu')\n\n    ```\n    \"\"\"\n\n    return self._device(tuple(self.storage))\n</code></pre>","location":"en/tensors/nested_tensor/#danling.tensors.nested_tensor.NestedTensor.device"},{"title":"<code>mask()</code>  <code>property</code>","text":"<p>Padding mask of <code>tensor</code>.</p> <p>Returns:</p>    Type Description      <code>torch.Tensor</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.mask\ntensor([[ True,  True,  True],\n        [ True,  True, False]])\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@property\ndef mask(self) -&gt; Tensor:\n    r\"\"\"\n    Padding mask of `tensor`.\n\n    Returns\n    -------\n    torch.Tensor\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.mask\n    tensor([[ True,  True,  True],\n            [ True,  True, False]])\n\n    ```\n    \"\"\"\n\n    return self._mask(tuple(self.storage))\n</code></pre>","location":"en/tensors/nested_tensor/#danling.tensors.nested_tensor.NestedTensor.mask"},{"title":"<code>shape()</code>  <code>property</code>","text":"<p>Alias for <code>size</code>.</p> <p>Returns:</p>    Type Description      <code>torch.Size</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.storage.append(torch.tensor([6, 7, 8, 9]))\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([3, 4])\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@property\ndef shape(self) -&gt; torch.Size:\n    r\"\"\"\n    Alias for `size`.\n\n    Returns\n    -------\n    torch.Size\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.shape\n    torch.Size([2, 3])\n    &gt;&gt;&gt; nested_tensor.storage.append(torch.tensor([6, 7, 8, 9]))\n    &gt;&gt;&gt; nested_tensor.shape\n    torch.Size([3, 4])\n\n    ```\n    \"\"\"\n\n    return self.size()\n</code></pre>","location":"en/tensors/nested_tensor/#danling.tensors.nested_tensor.NestedTensor.shape"},{"title":"<code>size()</code>","text":"<p>Shape of the NestedTensor.</p> <p>Returns:</p>    Type Description      <code>torch.Size</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.size()\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.storage[1] = torch.tensor([4, 5, 6, 7])\n&gt;&gt;&gt; nested_tensor.size()\ntorch.Size([2, 4])\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def size(self) -&gt; torch.Size:\n    r\"\"\"\n    Shape of the NestedTensor.\n\n    Returns\n    -------\n    torch.Size\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.size()\n    torch.Size([2, 3])\n    &gt;&gt;&gt; nested_tensor.storage[1] = torch.tensor([4, 5, 6, 7])\n    &gt;&gt;&gt; nested_tensor.size()\n    torch.Size([2, 4])\n\n    ```\n    \"\"\"\n\n    return self._size(tuple(self.storage))\n</code></pre>","location":"en/tensors/nested_tensor/#danling.tensors.nested_tensor.NestedTensor.size"},{"title":"<code>tensor()</code>  <code>property</code>","text":"<p>Return a single tensor by padding all the tensors.</p> <p>Returns:</p>    Type Description      <code>torch.Tensor</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.tensor\ntensor([[1, 2, 3],\n        [4, 5, 0]])\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@property\ndef tensor(self) -&gt; Tensor:\n    r\"\"\"\n    Return a single tensor by padding all the tensors.\n\n    Returns\n    -------\n    torch.Tensor\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.tensor\n    tensor([[1, 2, 3],\n            [4, 5, 0]])\n\n    ```\n    \"\"\"\n\n    return self._tensor(tuple(self.storage), self.batch_first)\n</code></pre>","location":"en/tensors/nested_tensor/#danling.tensors.nested_tensor.NestedTensor.tensor"},{"title":"Decorator","text":"","location":"en/utils/decorator/"},{"title":"<code>catch(error=Exception, exclude=None, print_args=False)</code>","text":"<p>Decorator to catch <code>error</code> except for <code>exclude</code>. Detailed traceback will be printed to <code>stdout</code>.</p> <p><code>catch</code> is extremely useful for unfatal errors. For example, <code>Runner</code> by de</p> <p>Parameters:</p>    Name Type Description Default     <code>error</code>  <code>Exception</code>    <code>Exception</code>    <code>exclude</code>  <code>Optional[Exception]</code>    <code>None</code>    <code>print_args</code>  <code>bool</code>  <p>Whether to print the arguments passed to the function.</p>  <code>False</code>      Source code in <code>danling/utils/decorator.py</code> Python<pre><code>@flexible_decorator\ndef catch(error: Exception = Exception, exclude: Optional[Exception] = None, print_args: bool = False):\n    \"\"\"\n    Decorator to catch `error` except for `exclude`.\n    Detailed traceback will be printed to `stdout`.\n\n    `catch` is extremely useful for unfatal errors.\n    For example, `Runner` by de\n\n    Parameters\n    ----------\n    error: Exception\n    exclude: Optional[Exception] = None\n    print_args: bool = False\n        Whether to print the arguments passed to the function.\n    \"\"\"\n\n    def decorator(func, error: Exception = Exception, exclude: Optional[Exception] = None, print_args: bool = False):\n        @wraps(func)\n        def wrapper(*args, **kwargs):  # pylint: disable=R1710\n            try:\n                return func(*args, **kwargs)\n            except error as exc:  # pylint: disable=W0703\n                if exclude is not None and isinstance(exc, exclude):\n                    raise exc\n                message = format_exc()\n                message += f\"\\nencoutered when calling {func}\"\n                if print_args:\n                    message += (f\"with args {args} and kwargs {kwargs}\",)\n                print(message, file=stderr, force=True)\n\n        return wrapper\n\n    return lambda func: decorator(func, error, exclude, print_args)\n</code></pre>","location":"en/utils/decorator/#danling.utils.decorator.catch"},{"title":"<code>ensure_dir(func)</code>","text":"<p>Decorator to ensure a directory property exists.</p>  Source code in <code>danling/utils/decorator.py</code> Python<pre><code>def ensure_dir(func):\n    \"\"\"\n    Decorator to ensure a directory property exists.\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        path = abspath(func(*args, **kwargs))\n        makedirs(path, exist_ok=True)\n        return path\n\n    return wrapper\n</code></pre>","location":"en/utils/decorator/#danling.utils.decorator.ensure_dir"},{"title":"<code>flexible_decorator(maybe_decorator=None)</code>","text":"<p>Decorator to allow bracket-less when no arguments are passed.</p> <p>Examples:</p> <p>For decorator defined as follows:</p> Python Console Session<pre><code>&gt;&gt;&gt; @flexible_decorator\n&gt;&gt;&gt; def decorator(*args, **kwargs):\n...     pass\n</code></pre> <p>The following two are equivalent:</p> Python Console Session<pre><code>&gt;&gt;&gt; @decorator\n&gt;&gt;&gt; def func(*args, **kwargs):\n...     pass\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; @decorator()\n&gt;&gt;&gt; def func(*args, **kwargs):\n...     pass\n</code></pre>  Source code in <code>danling/utils/decorator.py</code> Python<pre><code>def flexible_decorator(maybe_decorator: Optional[Callable] = None):\n    \"\"\"\n    Decorator to allow bracket-less when no arguments are passed.\n\n    Examples\n    --------\n    For decorator defined as follows:\n    &gt;&gt;&gt; @flexible_decorator\n    &gt;&gt;&gt; def decorator(*args, **kwargs):\n    ...     pass\n\n    The following two are equivalent:\n    &gt;&gt;&gt; @decorator\n    &gt;&gt;&gt; def func(*args, **kwargs):\n    ...     pass\n\n    &gt;&gt;&gt; @decorator()\n    &gt;&gt;&gt; def func(*args, **kwargs):\n    ...     pass\n\n    \"\"\"\n\n    def decorator(func: Callable):\n        @wraps(decorator)\n        def wrapper(*args, **kwargs):\n            if len(args) == 1 and isfunction(args[0]):\n                return func(**kwargs)(args[0])\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    if maybe_decorator is None:\n        return decorator\n    return decorator(maybe_decorator)\n</code></pre>","location":"en/utils/decorator/#danling.utils.decorator.flexible_decorator"},{"title":"<code>method_cache(*cache_args, **lru_kwargs)</code>","text":"<p>Decorator to cache the result of an instance method.</p> <p><code>functools.lru_cache</code> uses a strong reference to the instance, which will make the instance immortal and break the garbage collection.</p> <p><code>method_cache</code> uses a weak reference to the instance and works fine.</p> <p>https://rednafi.github.io/reflections/dont-wrap-instance-methods-with-functoolslru_cache-decorator-in-python.html</p>  Source code in <code>danling/utils/decorator.py</code> Python<pre><code>def method_cache(*cache_args, **lru_kwargs):\n    r\"\"\"\n    Decorator to cache the result of an instance method.\n\n    `functools.lru_cache` uses a strong reference to the instance,\n    which will make the instance immortal and break the garbage collection.\n\n    `method_cache` uses a weak reference to the instance and works fine.\n\n    https://rednafi.github.io/reflections/dont-wrap-instance-methods-with-functoolslru_cache-decorator-in-python.html\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(self, *args, **kwargs):\n            self_ref = ref(self)\n\n            @wraps(func)\n            @lru_cache(*cache_args, **lru_kwargs)\n            def cached_method(*args, **kwargs):\n                return func(self_ref(), *args, **kwargs)\n\n            setattr(self, func.__name__, cached_method)\n            return cached_method(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n</code></pre>","location":"en/utils/decorator/#danling.utils.decorator.method_cache"},{"title":"IO","text":"","location":"en/utils/io/"},{"title":"<code>is_json_serializable(obj)</code>","text":"<p>Check if <code>obj</code> is JSON serializable.</p>  Source code in <code>danling/utils/io.py</code> Python<pre><code>def is_json_serializable(obj: Any) -&gt; bool:\n    \"\"\"\n    Check if `obj` is JSON serializable.\n    \"\"\"\n    try:\n        json_dumps(obj)\n        return True\n    except (TypeError, OverflowError):\n        return False\n</code></pre>","location":"en/utils/io/#danling.utils.io.is_json_serializable"},{"title":"<code>load(path, *args, **kwargs)</code>","text":"<p>Load any file with supported extensions.</p>  Source code in <code>danling/utils/io.py</code> Python<pre><code>def load(path: str, *args: List[Any], **kwargs: Dict[str, Any]) -&gt; Any:\n    \"\"\"\n    Load any file with supported extensions.\n    \"\"\"\n    if not os.path.isfile(path):\n        raise ValueError(f\"Trying to load {path} but it is not a file.\")\n    extension = os.path.splitext(path)[-1].lower()[1:]\n    if extension in PYTORCH:\n        if torch_load is None:\n            raise ImportError(f\"Trying to load {path} but torch is not installed.\")\n        return torch_load(path)\n    if extension in NUMPY:\n        if numpy_load is None:\n            raise ImportError(f\"Trying to load {path} but numpy is not installed.\")\n        return numpy_load(path, allow_pickle=True)\n    if extension in CSV:\n        if read_csv is None:\n            raise ImportError(f\"Trying to load {path} but pandas is not installed.\")\n        return read_csv(path, *args, **kwargs)\n    if extension in JSON:\n        with open(path, \"r\") as fp:  # pylint: disable=W1514, C0103\n            return json_load(fp, *args, **kwargs)  # type: ignore\n    if extension in PICKLE:\n        with open(path, \"rb\") as fp:  # pylint: disable=C0103\n            return pickle_load(fp, *args, **kwargs)  # type: ignore\n    raise ValueError(f\"Tying to load {path} with unsupported extension={extension}\")\n</code></pre>","location":"en/utils/io/#danling.utils.io.load"},{"title":"DanLing","text":"","location":"zh/"},{"title":"Introduction","text":"<p>DanLing (\u200b\u4e39\u7075\u200b) is a high-level library to help with running neural networks flexibly and transparently.</p> <p>DanLing is meant to be a scaffold for experienced researchers and engineers who know how to define a training loop, but are bored of writing the same boilerplate code, such as DDP, logging, checkpointing, etc., over and over again.</p> <p>Therefore, DanLing does not feature complex Runner designs with many pre-defined methods and complicated hooks. Instead, the Runner of DanLing just initialise the essential parts for you, and you can do whatever you want, however you want.</p> <p>Although many attributes and properties are pre-defined and are expected to be used in DanLing, you have full control over your code.</p> <p>DanLing also provides some utilities, such as [Registry][danling.registry.registry], [NestedTensor][danling.tensors.nestedtensor], [catch][danling.utils.catch], etc.</p>","location":"zh/#introduction"},{"title":"Installation","text":"<p>Install the most recent stable version on pypi:</p> Bash<pre><code>pip install danling\n</code></pre> <p>Install the latest version from source:</p> Bash<pre><code>pip install git+https://github.com/ZhiyuanChen/DanLing\n</code></pre> <p>It works the way it should have worked.</p>","location":"zh/#installation"},{"title":"License","text":"<p>DanLing is multi-licensed under the following licenses:</p> <ul> <li>Unlicense</li> <li>GNU GPL 2.0 (or any later version)</li> <li>MIT</li> <li>Apache 2.0</li> <li>BSD 2-Clause</li> <li>BSD 3-Clause</li> </ul> <p>You can choose any (one or more) of them if you use this work.</p> <p><code>SPDX-License-Identifier: Unlicense OR GPL-2.0-or-later OR MIT OR Apache-2.0 OR BSD-2-Clause OR BSD-3-Clause</code></p>","location":"zh/#license"},{"title":"DanLing","text":"","location":"zh/package/"},{"title":"<code>AverageMeter</code>","text":"<p>Computes and stores the average and current value.</p> <p>Attributes:</p>    Name Type Description     <code>val</code>  <code>int</code>  <p>Current value.</p>   <code>avg</code>  <code>float</code>  <p>Average value.</p>   <code>sum</code>  <code>float</code>  <p>Sum of values.</p>   <code>count</code>  <code>int</code>  <p>Number of values.</p>    <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.update(0.9)\n&gt;&gt;&gt; meter.val\n0.9\n&gt;&gt;&gt; meter.avg\n0.8\n&gt;&gt;&gt; meter.sum\n1.6\n&gt;&gt;&gt; meter.count\n2\n&gt;&gt;&gt; meter.reset()\n&gt;&gt;&gt; meter.val\n0\n&gt;&gt;&gt; meter.avg\n0\n</code></pre>  Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>class AverageMeter:\n    r\"\"\"\n    Computes and stores the average and current value.\n\n    Attributes\n    ----------\n    val: int\n        Current value.\n    avg: float\n        Average value.\n    sum: float\n        Sum of values.\n    count: int\n        Number of values.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; meter = AverageMeter()\n    &gt;&gt;&gt; meter.update(0.7)\n    &gt;&gt;&gt; meter.val\n    0.7\n    &gt;&gt;&gt; meter.avg\n    0.7\n    &gt;&gt;&gt; meter.update(0.9)\n    &gt;&gt;&gt; meter.val\n    0.9\n    &gt;&gt;&gt; meter.avg\n    0.8\n    &gt;&gt;&gt; meter.sum\n    1.6\n    &gt;&gt;&gt; meter.count\n    2\n    &gt;&gt;&gt; meter.reset()\n    &gt;&gt;&gt; meter.val\n    0\n    &gt;&gt;&gt; meter.avg\n    0\n\n    ```\n    \"\"\"\n\n    val: int = 0\n    avg: float = 0\n    sum: int = 0\n    count: int = 0\n\n    def __init__(self) -&gt; None:\n        self.reset()\n\n    def reset(self) -&gt; None:\n        r\"\"\"\n        Resets the meter.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; meter = AverageMeter()\n        &gt;&gt;&gt; meter.update(0.7)\n        &gt;&gt;&gt; meter.val\n        0.7\n        &gt;&gt;&gt; meter.avg\n        0.7\n        &gt;&gt;&gt; meter.reset()\n        &gt;&gt;&gt; meter.val\n        0\n        &gt;&gt;&gt; meter.avg\n        0\n\n        ```\n        \"\"\"\n\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n: int = 1) -&gt; None:\n        r\"\"\"\n        Updates the average and current value in the meter.\n\n        Parameters\n        ----------\n        val: int\n            Value to be added to the average.\n        n: int = 1\n            Number of values to be added.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; meter = AverageMeter()\n        &gt;&gt;&gt; meter.update(0.7)\n        &gt;&gt;&gt; meter.val\n        0.7\n        &gt;&gt;&gt; meter.avg\n        0.7\n        &gt;&gt;&gt; meter.update(0.9)\n        &gt;&gt;&gt; meter.val\n        0.9\n        &gt;&gt;&gt; meter.avg\n        0.8\n        &gt;&gt;&gt; meter.sum\n        1.6\n        &gt;&gt;&gt; meter.count\n        2\n\n        ```\n        \"\"\"\n\n        # pylint: disable=C0103\n\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n</code></pre>","location":"zh/package/#danling.AverageMeter"},{"title":"<code>reset()</code>","text":"<p>Resets the meter.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.reset()\n&gt;&gt;&gt; meter.val\n0\n&gt;&gt;&gt; meter.avg\n0\n</code></pre>  Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>def reset(self) -&gt; None:\n    r\"\"\"\n    Resets the meter.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; meter = AverageMeter()\n    &gt;&gt;&gt; meter.update(0.7)\n    &gt;&gt;&gt; meter.val\n    0.7\n    &gt;&gt;&gt; meter.avg\n    0.7\n    &gt;&gt;&gt; meter.reset()\n    &gt;&gt;&gt; meter.val\n    0\n    &gt;&gt;&gt; meter.avg\n    0\n\n    ```\n    \"\"\"\n\n    self.val = 0\n    self.avg = 0\n    self.sum = 0\n    self.count = 0\n</code></pre>","location":"zh/package/#danling.metrics.average_meter.AverageMeter.reset"},{"title":"<code>update(val, n=1)</code>","text":"<p>Updates the average and current value in the meter.</p> <p>Parameters:</p>    Name Type Description Default     <code>val</code>   <p>Value to be added to the average.</p>  required    <code>n</code>  <code>int</code>  <p>Number of values to be added.</p>  <code>1</code>     <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.update(0.9)\n&gt;&gt;&gt; meter.val\n0.9\n&gt;&gt;&gt; meter.avg\n0.8\n&gt;&gt;&gt; meter.sum\n1.6\n&gt;&gt;&gt; meter.count\n2\n</code></pre>  Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>def update(self, val, n: int = 1) -&gt; None:\n    r\"\"\"\n    Updates the average and current value in the meter.\n\n    Parameters\n    ----------\n    val: int\n        Value to be added to the average.\n    n: int = 1\n        Number of values to be added.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; meter = AverageMeter()\n    &gt;&gt;&gt; meter.update(0.7)\n    &gt;&gt;&gt; meter.val\n    0.7\n    &gt;&gt;&gt; meter.avg\n    0.7\n    &gt;&gt;&gt; meter.update(0.9)\n    &gt;&gt;&gt; meter.val\n    0.9\n    &gt;&gt;&gt; meter.avg\n    0.8\n    &gt;&gt;&gt; meter.sum\n    1.6\n    &gt;&gt;&gt; meter.count\n    2\n\n    ```\n    \"\"\"\n\n    # pylint: disable=C0103\n\n    self.val = val\n    self.sum += val * n\n    self.count += n\n    self.avg = self.sum / self.count\n</code></pre>","location":"zh/package/#danling.metrics.average_meter.AverageMeter.update"},{"title":"<code>MultiHeadAttention</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Allows the model to jointly attend to information from different representation subspaces. See <code>Attention Is All You Need &lt;https://arxiv.org/abs/1706.03762&gt;</code>_ .. math::     \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O where :math:<code>head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)</code>. Args:     embed_dim: total dimension of the model.     num_heads: parallel attention heads.     dropout: a Dropout layer on attn_output_weights. Default: 0.0.     bias: add bias as module parameter. Default: True.     add_bias_kv: add bias to the key and value sequences at dim=0.     add_zero_attn: add a new batch of zeros to the key and                    value sequences at dim=1.     k_dim: total number of features in key. Default: None.     v_dim: total number of features in value. Default: None.     batch_first: If <code>True</code>, then the input and output tensors are provided         as (batch, seq, feature). Default: <code>False</code> (seq, batch, feature). Note that if :attr:<code>k_dim</code> and :attr:<code>v_dim</code> are None, they will be set to :attr:<code>embed_dim</code> such that query, key, and value have the same number of features. Examples::     &gt;&gt;&gt; multihead_attn = dl.model.MultiHeadAttention(embed_dim, num_heads)     &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)</p>  Source code in <code>danling/models/transformer/attention/multihead_attention.py</code> Python<pre><code>class MultiHeadAttention(nn.Module):\n    r\"\"\"Allows the model to jointly attend to information\n    from different representation subspaces.\n    See `Attention Is All You Need &lt;https://arxiv.org/abs/1706.03762&gt;`_\n    .. math::\n        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n    where :math:`head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n    Args:\n        embed_dim: total dimension of the model.\n        num_heads: parallel attention heads.\n        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n        bias: add bias as module parameter. Default: True.\n        add_bias_kv: add bias to the key and value sequences at dim=0.\n        add_zero_attn: add a new batch of zeros to the key and\n                       value sequences at dim=1.\n        k_dim: total number of features in key. Default: None.\n        v_dim: total number of features in value. Default: None.\n        batch_first: If ``True``, then the input and output tensors are provided\n            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n    Note that if :attr:`k_dim` and :attr:`v_dim` are None, they will be set\n    to :attr:`embed_dim` such that query, key, and value have the same\n    number of features.\n    Examples::\n        &gt;&gt;&gt; multihead_attn = dl.model.MultiHeadAttention(embed_dim, num_heads)\n        &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)\n    \"\"\"\n    __constants__ = [\"batch_first\"]\n    bias_k: Optional[Tensor]\n    bias_v: Optional[Tensor]\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        attn_dropout: float = 0.0,\n        scale_factor: float = 1.0,\n        bias: bool = True,\n        add_bias_kv: bool = False,\n        add_zero_attn: bool = False,\n        k_dim: Optional[int] = None,\n        v_dim: Optional[int] = None,\n        batch_first: bool = True,\n        **kwargs: Optional[Dict[str, Any]],\n    ) -&gt; None:\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.k_dim = k_dim if k_dim is not None else self.embed_dim\n        self.v_dim = v_dim if v_dim is not None else self.embed_dim\n        self._qkv_same_embed_dim = self.embed_dim == self.k_dim == self.v_dim\n\n        self.num_heads = num_heads\n        self.scale_factor = scale_factor\n        self.batch_first = batch_first\n        self.head_dim = self.embed_dim // self.num_heads\n        self.scaling = float(self.head_dim * self.scale_factor) ** -0.5\n        if not self.head_dim * self.num_heads == self.embed_dim:\n            raise ValueError(f\"embed_dim {self.embed_dim} not divisible by num_heads {self.num_heads}\")\n\n        self.in_proj = nn.Linear(self.embed_dim, self.embed_dim + self.k_dim + self.v_dim, bias=bias)\n        self.dropout = nn.Dropout(attn_dropout)\n        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=bias)\n\n        if add_bias_kv:\n            self.bias_k = nn.Parameter(torch.empty((1, 1, self.embed_dim)))\n            self.bias_v = nn.Parameter(torch.empty((1, 1, self.embed_dim)))\n        else:\n            self.bias_k = self.bias_v = None\n\n        self.add_zero_attn = add_zero_attn\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        nn.init.xavier_uniform_(self.in_proj.weight)\n        if self.in_proj.bias is not None:\n            nn.init.constant_(self.in_proj.bias, 0.0)\n            nn.init.constant_(self.out_proj.bias, 0.0)\n        if self.bias_k is not None:\n            nn.init.xavier_normal_(self.bias_k)\n        if self.bias_v is not None:\n            nn.init.xavier_normal_(self.bias_v)\n\n    def __setstate__(self, state):\n        # Support loading old MultiHeadAttention checkpoints generated by v1.1.0\n        if \"_qkv_same_embed_dim\" not in state:\n            state[\"_qkv_same_embed_dim\"] = True\n        super(MultiHeadAttention, self).__setstate__(state)\n\n    def forward(\n        self,\n        query: Tensor,\n        key: Tensor,\n        value: Tensor,\n        attn_bias: Optional[Tensor] = None,\n        attn_mask: Optional[Tensor] = None,\n        key_padding_mask: Optional[Tensor] = None,\n        need_weights: bool = False,\n        static_k: Optional[Tensor] = None,\n        static_v: Optional[Tensor] = None,\n    ) -&gt; Tuple[Tensor, Optional[Tensor]]:\n        r\"\"\"\n        Args:\n            query, key, value: map a query and a set of key-value pairs to an output.\n                See \"Attention Is All You Need\" for more details.\n            attn_bias: 2D or 3D mask that add bias to attention output weights. Used for relative positional embedding.\n                A 2D bias will be broadcasted for all the batches while a 3D mask allows to specify a different mask for\n                the entries of each batch.\n            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n                the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n            key_padding_mask: if provided, specified padding elements in the key will\n                be ignored by the attention. When given a binary mask and a value is True,\n                the corresponding value on the attention layer will be ignored. When given\n                a byte mask and a value is non-zero, the corresponding value on the attention\n                layer will be ignored\n            need_weights: output attn_output_weights.\n            static_k, static_v: static key and value used for attention operators.\n        Shapes for inputs:\n            - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n                the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n            - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n                the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n            - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n                the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n            - attn_bias: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n                source sequence length.\n                If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n                length, S is the source sequence length. ``attn_bias`` allows to pass pos embed directly into attention\n                If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will\n                be unchanged. If a BoolTensor is provided, positions with ``True`` is not allowed to attend while ``False``\n                values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight.\n            - attn_mask: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n                source sequence length.\n                If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n                length, S is the source sequence length. ``attn_mask`` ensure that position i is allowed to attend\n                the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n                while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n                is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n                is provided, it will be added to the attention weight.\n            - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n                If a ByteTensor is provided, the non-zero positions will be ignored while the position\n                with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the\n                value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n            - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n                N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n            - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n                N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n        Outputs:\n            - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n                E is the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n            - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n                L is the target sequence length, S is the source sequence length.\n        \"\"\"\n        if self.batch_first:\n            query, key, value = [x.transpose(0, 1) for x in (query, key, value)]\n\n        # set up shape vars\n        target_len, batch_size, embed_dim = query.shape\n        source_len, _, _ = key.shape\n        if not key.shape[:2] == value.shape[:2]:\n            raise ValueError(f\"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}\")\n\n        q, k, v = self.in_projection(query, key, value)\n\n        # prep attention mask\n        if attn_mask is not None:\n            if attn_mask.dtype == torch.uint8:\n                warnings.warn(\n                    \"attn_mask is of type uint8. This type is deprecated. Please use bool or float tensors instead.\"\n                )\n                attn_mask = attn_mask.to(torch.bool)\n            elif not (attn_mask.is_floating_point() or attn_mask.dtype == torch.bool):\n                raise ValueError(f\"attn_mask should have type float or bool, but got {attn_mask.dtype}.\")\n            # ensure attn_mask's dim is 3\n            if attn_mask.dim() == 2:\n                correct_shape = (target_len, source_len)\n                if attn_mask.shape != correct_shape:\n                    raise ValueError(f\"attn_mask should have shape {correct_shape}, but got {attn_mask.shape}.\")\n                attn_mask = attn_mask.unsqueeze(0)\n            elif attn_mask.dim() == 3:\n                correct_shape = (batch_size * self.num_heads, target_len, source_len)  # type: ignore\n                if attn_mask.shape != correct_shape:\n                    raise ValueError(f\"attn_mask should have shape {correct_shape}, but got {attn_mask.shape}.\")\n            else:\n                raise RuntimeError(f\"attn_mask should have dimension 2 or 3, bug got {attn_mask.dim()}.\")\n\n        # prep key padding mask\n        if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n            warnings.warn(\n                \"key_padding_mask is of type uint8. This type is deprecated. Please use bool or float tensors instead.\"\n            )\n            key_padding_mask = key_padding_mask.to(torch.bool)\n\n        # add bias along batch dimension (currently second)\n        if self.bias_k is not None and self.bias_v is not None:\n            assert static_k is None, \"bias cannot be added to static key.\"\n            assert static_v is None, \"bias cannot be added to static value.\"\n            k = torch.cat([k, self.bias_k.repeat(1, batch_size, 1)])\n            v = torch.cat([v, self.bias_v.repeat(1, batch_size, 1)])\n            if attn_mask is not None:\n                attn_mask = F.pad(attn_mask, (0, 1))\n            if key_padding_mask is not None:\n                key_padding_mask = F.pad(key_padding_mask, (0, 1))\n        else:\n            assert self.bias_k is None\n            assert self.bias_v is None\n\n        # reshape q, k, v for multihead attention and make em batch first\n        q = q.reshape(target_len, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n        k = k.reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1) if static_k is None else static_k\n        v = v.reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1) if static_v is None else static_v\n\n        if static_k is not None:\n            correct_shape = (  # type: ignore\n                batch_size * self.num_heads,\n                static_k.shape[1],\n                self.head_dim,\n            )\n            if static_k.shape != correct_shape:\n                raise ValueError(f\"static_k should have shape {correct_shape}, but got {static_k.shape}.\")\n        if static_v is not None:\n            correct_shape = (  # type: ignore\n                batch_size * self.num_heads,\n                static_v.shape[1],\n                self.head_dim,\n            )\n            if static_v.shape != correct_shape:\n                raise ValueError(f\"static_v should have shape {correct_shape}, but got {static_v.shape}.\")\n\n        # add zero attention along batch dimension (now first)\n        if self.add_zero_attn:\n            zero_attn_shape = (batch_size * self.num_heads, 1, self.head_dim)\n            k = torch.cat([k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1)\n            v = torch.cat([v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1)\n            if attn_mask is not None:\n                attn_mask = F.pad(attn_mask, (0, 1))\n            if key_padding_mask is not None:\n                key_padding_mask = F.pad(key_padding_mask, (0, 1))\n\n        # update source sequence length after adjustments\n        source_len = k.shape[1]\n\n        # merge key padding and attention masks\n        if key_padding_mask is not None:\n            if key_padding_mask.shape != (batch_size, source_len):\n                raise ValueError(\n                    f\"key_padding_mask should have shape {(batch_size, source_len)}, but got {key_padding_mask.shape}\"\n                )\n            key_padding_mask = (\n                key_padding_mask.view(batch_size, 1, 1, source_len)\n                .expand(-1, self.num_heads, -1, -1)\n                .reshape(batch_size * self.num_heads, 1, source_len)\n            )\n            if attn_mask is None:\n                attn_mask = key_padding_mask\n            elif attn_mask.dtype == torch.bool:\n                attn_mask = attn_mask.logical_or(key_padding_mask)\n            else:\n                attn_mask = attn_mask.masked_fill(key_padding_mask, float(\"-inf\"))\n\n        # convert mask to float\n        if attn_mask is not None and attn_mask.dtype == torch.bool:\n            new_attn_mask = torch.zeros_like(attn_mask, dtype=torch.float)\n            new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n            attn_mask = new_attn_mask\n\n        # (deep breath) calculate attention and out projection\n        attn_output, attn_output_weights = self.attention(q, k, v, attn_bias, attn_mask)\n        attn_output = attn_output.transpose(0, 1).reshape(target_len, batch_size, embed_dim)\n        attn_output = self.out_projection(attn_output)\n\n        # attn_output_weights is set to torch.empty(0, requires_grad=False) to avoid errors in DDP\n        attn_output_weights = (\n            attn_output_weights.view(batch_size, self.num_heads, target_len, source_len)\n            if need_weights\n            else torch.empty(0, requires_grad=False)\n        )\n\n        if self.batch_first:\n            return attn_output.transpose(0, 1), attn_output_weights\n        else:\n            return attn_output, attn_output_weights\n\n    def in_projection(self, q: Tensor, k: Tensor, v: Tensor) -&gt; Tuple[Tensor, Tensor, Tensor]:\n        r\"\"\"\n        Performs the in-projection step of the attention operation, using packed weights.\n        Output is a triple containing projection tensors for query, key and value.\n        Args:\n            q, k, v: query, key and value tensors to be projected. For self-attention,\n                these are typically the same tensor; for encoder-decoder attention,\n                k and v are typically the same tensor. (We take advantage of these\n                identities for performance if they are present.) Regardless, q, k and v\n                must share a common embedding dimension; otherwise their shapes may vary.\n        Shape:\n            Inputs:\n            - q: :math:`(..., E)` where E is the embedding dimension\n            - k: :math:`(..., E)` where E is the embedding dimension\n            - v: :math:`(..., E)` where E is the embedding dimension\n            Output:\n            - in output list :math:`[q', k', v']`, each output tensor will have the\n                same shape as the corresponding input tensor.\n        \"\"\"\n        if k is v:\n            # self-attention\n            if q is k:\n                return self.in_proj(q).split((self.embed_dim, self.k_dim, self.v_dim), dim=-1)\n            # encoder-decoder attention\n            else:\n                w_q, w_kv = self.in_proj.weight.split([self.embed_dim, self.k_dim + self.v_dim])\n                b_q, b_kv = (\n                    None\n                    if self.in_proj.bias is None\n                    else self.in_proj.bias.split([self.embed_dim, self.k_dim + self.v_dim])\n                )\n                return (F.linear(q, w_q, b_q),) + F.linear(k, w_kv, b_kv).split((self.k_dim, self.v_dim), dim=-1)\n        else:\n            w_q, w_k, w_v = self.in_proj.weight.split([self.embed_dim, self.k_dim, self.v_dim])\n            b_q, b_k, b_v = (\n                None if self.in_proj.bias is None else self.in_proj.bias.split([self.embed_dim, self.k_dim, self.v_dim])\n            )\n            return F.linear(q, w_q, b_q), F.linear(k, w_k, b_k), F.linear(v, w_v, b_v)\n\n    def attention(\n        self,\n        q: Tensor,\n        k: Tensor,\n        v: Tensor,\n        attn_bias: Optional[Tensor] = None,\n        attn_mask: Optional[Tensor] = None,\n    ) -&gt; Tuple[Tensor, Tensor]:\n        r\"\"\"\n        Computes scaled dot product attention on query, key and value tensors, using\n        an optional attention mask if passed, and applying dropout if a probability\n        greater than 0.0 is specified.\n        Returns a tensor pair containing attended values and attention weights.\n        Args:\n            q, k, v: query, key and value tensors. See Shape section for shape details.\n            attn_mask: optional tensor containing mask values to be added to calculated\n                attention. May be 2D or 3D; see Shape section for details.\n            attn_bias: optional tensor containing bias values to be added to calculated\n                attention. Used for relative positional embedding. May be 2D or 3D; see\n                Shape section for details.\n        Shape:\n            - q: :math:`(B, Nt, E)` where B is batch size, Nt is the target sequence length,\n                and E is embedding dimension.\n            - key: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n                and E is embedding dimension.\n            - value: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n                and E is embedding dimension.\n            - attn_bias: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n                shape :math:`(Nt, Ns)`.\n            - attn_mask: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n                shape :math:`(Nt, Ns)`.\n            - Output: attention values have shape :math:`(B, Nt, E)`; attention weights\n                have shape :math:`(B, Nt, Ns)`\n        \"\"\"\n        q *= self.scaling\n        # (B, Nt, E) x (B, E, Ns) -&gt; (B, Nt, Ns)\n        attn = torch.bmm(q, k.transpose(-2, -1))\n        if attn_bias is not None:\n            attn += attn_bias\n        if attn_mask is not None:\n            attn += attn_mask\n        attn = F.softmax(attn, dim=-1)\n        attn = self.dropout(attn)\n        # (B, Nt, Ns) x (B, Ns, E) -&gt; (B, Nt, E)\n        output = torch.bmm(attn, v)\n        return output, attn\n\n    def out_projection(self, attn_output: Tensor) -&gt; Tensor:\n        return self.out_proj(attn_output)\n</code></pre>","location":"zh/package/#danling.MultiHeadAttention"},{"title":"<code>attention(q, k, v, attn_bias=None, attn_mask=None)</code>","text":"<p>Computes scaled dot product attention on query, key and value tensors, using an optional attention mask if passed, and applying dropout if a probability greater than 0.0 is specified. Returns a tensor pair containing attended values and attention weights. Args:     q, k, v: query, key and value tensors. See Shape section for shape details.     attn_mask: optional tensor containing mask values to be added to calculated         attention. May be 2D or 3D; see Shape section for details.     attn_bias: optional tensor containing bias values to be added to calculated         attention. Used for relative positional embedding. May be 2D or 3D; see         Shape section for details. Shape:     - q: :math:<code>(B, Nt, E)</code> where B is batch size, Nt is the target sequence length,         and E is embedding dimension.     - key: :math:<code>(B, Ns, E)</code> where B is batch size, Ns is the source sequence length,         and E is embedding dimension.     - value: :math:<code>(B, Ns, E)</code> where B is batch size, Ns is the source sequence length,         and E is embedding dimension.     - attn_bias: either a 3D tensor of shape :math:<code>(B, Nt, Ns)</code> or a 2D tensor of         shape :math:<code>(Nt, Ns)</code>.     - attn_mask: either a 3D tensor of shape :math:<code>(B, Nt, Ns)</code> or a 2D tensor of         shape :math:<code>(Nt, Ns)</code>.     - Output: attention values have shape :math:<code>(B, Nt, E)</code>; attention weights         have shape :math:<code>(B, Nt, Ns)</code></p>  Source code in <code>danling/models/transformer/attention/multihead_attention.py</code> Python<pre><code>def attention(\n    self,\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    attn_bias: Optional[Tensor] = None,\n    attn_mask: Optional[Tensor] = None,\n) -&gt; Tuple[Tensor, Tensor]:\n    r\"\"\"\n    Computes scaled dot product attention on query, key and value tensors, using\n    an optional attention mask if passed, and applying dropout if a probability\n    greater than 0.0 is specified.\n    Returns a tensor pair containing attended values and attention weights.\n    Args:\n        q, k, v: query, key and value tensors. See Shape section for shape details.\n        attn_mask: optional tensor containing mask values to be added to calculated\n            attention. May be 2D or 3D; see Shape section for details.\n        attn_bias: optional tensor containing bias values to be added to calculated\n            attention. Used for relative positional embedding. May be 2D or 3D; see\n            Shape section for details.\n    Shape:\n        - q: :math:`(B, Nt, E)` where B is batch size, Nt is the target sequence length,\n            and E is embedding dimension.\n        - key: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n            and E is embedding dimension.\n        - value: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n            and E is embedding dimension.\n        - attn_bias: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n            shape :math:`(Nt, Ns)`.\n        - attn_mask: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n            shape :math:`(Nt, Ns)`.\n        - Output: attention values have shape :math:`(B, Nt, E)`; attention weights\n            have shape :math:`(B, Nt, Ns)`\n    \"\"\"\n    q *= self.scaling\n    # (B, Nt, E) x (B, E, Ns) -&gt; (B, Nt, Ns)\n    attn = torch.bmm(q, k.transpose(-2, -1))\n    if attn_bias is not None:\n        attn += attn_bias\n    if attn_mask is not None:\n        attn += attn_mask\n    attn = F.softmax(attn, dim=-1)\n    attn = self.dropout(attn)\n    # (B, Nt, Ns) x (B, Ns, E) -&gt; (B, Nt, E)\n    output = torch.bmm(attn, v)\n    return output, attn\n</code></pre>","location":"zh/package/#danling.models.transformer.attention.multihead_attention.MultiHeadAttention.attention"},{"title":"<code>forward(query, key, value, attn_bias=None, attn_mask=None, key_padding_mask=None, need_weights=False, static_k=None, static_v=None)</code>","text":"<p>Shapes for inputs:     - query: :math:<code>(L, N, E)</code> where L is the target sequence length, N is the batch size, E is         the embedding dimension. :math:<code>(N, L, E)</code> if <code>batch_first</code> is <code>True</code>.     - key: :math:<code>(S, N, E)</code>, where S is the source sequence length, N is the batch size, E is         the embedding dimension. :math:<code>(N, S, E)</code> if <code>batch_first</code> is <code>True</code>.     - value: :math:<code>(S, N, E)</code> where S is the source sequence length, N is the batch size, E is         the embedding dimension. :math:<code>(N, S, E)</code> if <code>batch_first</code> is <code>True</code>.     - attn_bias: if a 2D mask: :math:<code>(L, S)</code> where L is the target sequence length, S is the         source sequence length.         If a 3D mask: :math:<code>(N\\cdot\\text{num\\_heads}, L, S)</code> where N is the batch size, L is the target sequence         length, S is the source sequence length. <code>attn_bias</code> allows to pass pos embed directly into attention         If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will         be unchanged. If a BoolTensor is provided, positions with <code>True</code> is not allowed to attend while <code>False</code>         values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight.     - attn_mask: if a 2D mask: :math:<code>(L, S)</code> where L is the target sequence length, S is the         source sequence length.         If a 3D mask: :math:<code>(N\\cdot\\text{num\\_heads}, L, S)</code> where N is the batch size, L is the target sequence         length, S is the source sequence length. <code>attn_mask</code> ensure that position i is allowed to attend         the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend         while the zero positions will be unchanged. If a BoolTensor is provided, positions with <code>True</code>         is not allowed to attend while <code>False</code> values will be unchanged. If a FloatTensor         is provided, it will be added to the attention weight.     - key_padding_mask: :math:<code>(N, S)</code> where N is the batch size, S is the source sequence length.         If a ByteTensor is provided, the non-zero positions will be ignored while the position         with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the         value of <code>True</code> will be ignored while the position with the value of <code>False</code> will be unchanged.     - static_k: :math:<code>(N*num_heads, S, E/num_heads)</code>, where S is the source sequence length,         N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.     - static_v: :math:<code>(N*num_heads, S, E/num_heads)</code>, where S is the source sequence length,         N is the batch size, E is the embedding dimension. E/num_heads is the head dimension. Outputs:     - attn_output: :math:<code>(L, N, E)</code> where L is the target sequence length, N is the batch size,         E is the embedding dimension. :math:<code>(N, L, E)</code> if <code>batch_first</code> is <code>True</code>.     - attn_output_weights: :math:<code>(N, L, S)</code> where N is the batch size,         L is the target sequence length, S is the source sequence length.</p>  Source code in <code>danling/models/transformer/attention/multihead_attention.py</code> Python<pre><code>def forward(\n    self,\n    query: Tensor,\n    key: Tensor,\n    value: Tensor,\n    attn_bias: Optional[Tensor] = None,\n    attn_mask: Optional[Tensor] = None,\n    key_padding_mask: Optional[Tensor] = None,\n    need_weights: bool = False,\n    static_k: Optional[Tensor] = None,\n    static_v: Optional[Tensor] = None,\n) -&gt; Tuple[Tensor, Optional[Tensor]]:\n    r\"\"\"\n    Args:\n        query, key, value: map a query and a set of key-value pairs to an output.\n            See \"Attention Is All You Need\" for more details.\n        attn_bias: 2D or 3D mask that add bias to attention output weights. Used for relative positional embedding.\n            A 2D bias will be broadcasted for all the batches while a 3D mask allows to specify a different mask for\n            the entries of each batch.\n        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n        key_padding_mask: if provided, specified padding elements in the key will\n            be ignored by the attention. When given a binary mask and a value is True,\n            the corresponding value on the attention layer will be ignored. When given\n            a byte mask and a value is non-zero, the corresponding value on the attention\n            layer will be ignored\n        need_weights: output attn_output_weights.\n        static_k, static_v: static key and value used for attention operators.\n    Shapes for inputs:\n        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n            the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n            the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n            the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n        - attn_bias: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n            source sequence length.\n            If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n            length, S is the source sequence length. ``attn_bias`` allows to pass pos embed directly into attention\n            If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will\n            be unchanged. If a BoolTensor is provided, positions with ``True`` is not allowed to attend while ``False``\n            values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight.\n        - attn_mask: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n            source sequence length.\n            If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n            length, S is the source sequence length. ``attn_mask`` ensure that position i is allowed to attend\n            the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n            while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n            is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n            is provided, it will be added to the attention weight.\n        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n            If a ByteTensor is provided, the non-zero positions will be ignored while the position\n            with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the\n            value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n            N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n            N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n    Outputs:\n        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n            E is the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n            L is the target sequence length, S is the source sequence length.\n    \"\"\"\n    if self.batch_first:\n        query, key, value = [x.transpose(0, 1) for x in (query, key, value)]\n\n    # set up shape vars\n    target_len, batch_size, embed_dim = query.shape\n    source_len, _, _ = key.shape\n    if not key.shape[:2] == value.shape[:2]:\n        raise ValueError(f\"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}\")\n\n    q, k, v = self.in_projection(query, key, value)\n\n    # prep attention mask\n    if attn_mask is not None:\n        if attn_mask.dtype == torch.uint8:\n            warnings.warn(\n                \"attn_mask is of type uint8. This type is deprecated. Please use bool or float tensors instead.\"\n            )\n            attn_mask = attn_mask.to(torch.bool)\n        elif not (attn_mask.is_floating_point() or attn_mask.dtype == torch.bool):\n            raise ValueError(f\"attn_mask should have type float or bool, but got {attn_mask.dtype}.\")\n        # ensure attn_mask's dim is 3\n        if attn_mask.dim() == 2:\n            correct_shape = (target_len, source_len)\n            if attn_mask.shape != correct_shape:\n                raise ValueError(f\"attn_mask should have shape {correct_shape}, but got {attn_mask.shape}.\")\n            attn_mask = attn_mask.unsqueeze(0)\n        elif attn_mask.dim() == 3:\n            correct_shape = (batch_size * self.num_heads, target_len, source_len)  # type: ignore\n            if attn_mask.shape != correct_shape:\n                raise ValueError(f\"attn_mask should have shape {correct_shape}, but got {attn_mask.shape}.\")\n        else:\n            raise RuntimeError(f\"attn_mask should have dimension 2 or 3, bug got {attn_mask.dim()}.\")\n\n    # prep key padding mask\n    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n        warnings.warn(\n            \"key_padding_mask is of type uint8. This type is deprecated. Please use bool or float tensors instead.\"\n        )\n        key_padding_mask = key_padding_mask.to(torch.bool)\n\n    # add bias along batch dimension (currently second)\n    if self.bias_k is not None and self.bias_v is not None:\n        assert static_k is None, \"bias cannot be added to static key.\"\n        assert static_v is None, \"bias cannot be added to static value.\"\n        k = torch.cat([k, self.bias_k.repeat(1, batch_size, 1)])\n        v = torch.cat([v, self.bias_v.repeat(1, batch_size, 1)])\n        if attn_mask is not None:\n            attn_mask = F.pad(attn_mask, (0, 1))\n        if key_padding_mask is not None:\n            key_padding_mask = F.pad(key_padding_mask, (0, 1))\n    else:\n        assert self.bias_k is None\n        assert self.bias_v is None\n\n    # reshape q, k, v for multihead attention and make em batch first\n    q = q.reshape(target_len, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n    k = k.reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1) if static_k is None else static_k\n    v = v.reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1) if static_v is None else static_v\n\n    if static_k is not None:\n        correct_shape = (  # type: ignore\n            batch_size * self.num_heads,\n            static_k.shape[1],\n            self.head_dim,\n        )\n        if static_k.shape != correct_shape:\n            raise ValueError(f\"static_k should have shape {correct_shape}, but got {static_k.shape}.\")\n    if static_v is not None:\n        correct_shape = (  # type: ignore\n            batch_size * self.num_heads,\n            static_v.shape[1],\n            self.head_dim,\n        )\n        if static_v.shape != correct_shape:\n            raise ValueError(f\"static_v should have shape {correct_shape}, but got {static_v.shape}.\")\n\n    # add zero attention along batch dimension (now first)\n    if self.add_zero_attn:\n        zero_attn_shape = (batch_size * self.num_heads, 1, self.head_dim)\n        k = torch.cat([k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1)\n        v = torch.cat([v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1)\n        if attn_mask is not None:\n            attn_mask = F.pad(attn_mask, (0, 1))\n        if key_padding_mask is not None:\n            key_padding_mask = F.pad(key_padding_mask, (0, 1))\n\n    # update source sequence length after adjustments\n    source_len = k.shape[1]\n\n    # merge key padding and attention masks\n    if key_padding_mask is not None:\n        if key_padding_mask.shape != (batch_size, source_len):\n            raise ValueError(\n                f\"key_padding_mask should have shape {(batch_size, source_len)}, but got {key_padding_mask.shape}\"\n            )\n        key_padding_mask = (\n            key_padding_mask.view(batch_size, 1, 1, source_len)\n            .expand(-1, self.num_heads, -1, -1)\n            .reshape(batch_size * self.num_heads, 1, source_len)\n        )\n        if attn_mask is None:\n            attn_mask = key_padding_mask\n        elif attn_mask.dtype == torch.bool:\n            attn_mask = attn_mask.logical_or(key_padding_mask)\n        else:\n            attn_mask = attn_mask.masked_fill(key_padding_mask, float(\"-inf\"))\n\n    # convert mask to float\n    if attn_mask is not None and attn_mask.dtype == torch.bool:\n        new_attn_mask = torch.zeros_like(attn_mask, dtype=torch.float)\n        new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n        attn_mask = new_attn_mask\n\n    # (deep breath) calculate attention and out projection\n    attn_output, attn_output_weights = self.attention(q, k, v, attn_bias, attn_mask)\n    attn_output = attn_output.transpose(0, 1).reshape(target_len, batch_size, embed_dim)\n    attn_output = self.out_projection(attn_output)\n\n    # attn_output_weights is set to torch.empty(0, requires_grad=False) to avoid errors in DDP\n    attn_output_weights = (\n        attn_output_weights.view(batch_size, self.num_heads, target_len, source_len)\n        if need_weights\n        else torch.empty(0, requires_grad=False)\n    )\n\n    if self.batch_first:\n        return attn_output.transpose(0, 1), attn_output_weights\n    else:\n        return attn_output, attn_output_weights\n</code></pre>","location":"zh/package/#danling.models.transformer.attention.multihead_attention.MultiHeadAttention.forward"},{"title":"<code>in_projection(q, k, v)</code>","text":"<p>Performs the in-projection step of the attention operation, using packed weights. Output is a triple containing projection tensors for query, key and value. Args:     q, k, v: query, key and value tensors to be projected. For self-attention,         these are typically the same tensor; for encoder-decoder attention,         k and v are typically the same tensor. (We take advantage of these         identities for performance if they are present.) Regardless, q, k and v         must share a common embedding dimension; otherwise their shapes may vary. Shape:     Inputs:     - q: :math:<code>(..., E)</code> where E is the embedding dimension     - k: :math:<code>(..., E)</code> where E is the embedding dimension     - v: :math:<code>(..., E)</code> where E is the embedding dimension     Output:     - in output list :math:<code>[q', k', v']</code>, each output tensor will have the         same shape as the corresponding input tensor.</p>  Source code in <code>danling/models/transformer/attention/multihead_attention.py</code> Python<pre><code>def in_projection(self, q: Tensor, k: Tensor, v: Tensor) -&gt; Tuple[Tensor, Tensor, Tensor]:\n    r\"\"\"\n    Performs the in-projection step of the attention operation, using packed weights.\n    Output is a triple containing projection tensors for query, key and value.\n    Args:\n        q, k, v: query, key and value tensors to be projected. For self-attention,\n            these are typically the same tensor; for encoder-decoder attention,\n            k and v are typically the same tensor. (We take advantage of these\n            identities for performance if they are present.) Regardless, q, k and v\n            must share a common embedding dimension; otherwise their shapes may vary.\n    Shape:\n        Inputs:\n        - q: :math:`(..., E)` where E is the embedding dimension\n        - k: :math:`(..., E)` where E is the embedding dimension\n        - v: :math:`(..., E)` where E is the embedding dimension\n        Output:\n        - in output list :math:`[q', k', v']`, each output tensor will have the\n            same shape as the corresponding input tensor.\n    \"\"\"\n    if k is v:\n        # self-attention\n        if q is k:\n            return self.in_proj(q).split((self.embed_dim, self.k_dim, self.v_dim), dim=-1)\n        # encoder-decoder attention\n        else:\n            w_q, w_kv = self.in_proj.weight.split([self.embed_dim, self.k_dim + self.v_dim])\n            b_q, b_kv = (\n                None\n                if self.in_proj.bias is None\n                else self.in_proj.bias.split([self.embed_dim, self.k_dim + self.v_dim])\n            )\n            return (F.linear(q, w_q, b_q),) + F.linear(k, w_kv, b_kv).split((self.k_dim, self.v_dim), dim=-1)\n    else:\n        w_q, w_k, w_v = self.in_proj.weight.split([self.embed_dim, self.k_dim, self.v_dim])\n        b_q, b_k, b_v = (\n            None if self.in_proj.bias is None else self.in_proj.bias.split([self.embed_dim, self.k_dim, self.v_dim])\n        )\n        return F.linear(q, w_q, b_q), F.linear(k, w_k, b_k), F.linear(v, w_v, b_v)\n</code></pre>","location":"zh/package/#danling.models.transformer.attention.multihead_attention.MultiHeadAttention.in_projection"},{"title":"<code>NestedTensor</code>","text":"<p>Wrap a sequence of tensors into a single tensor with a mask.</p> <p>In sequence to sequence tasks, elements of a batch are usually not of the same length. This made it tricky to use a single tensor to represent a batch of sequences.</p> <p>NestedTensor allows to store a sequence of tensors of different lengths in a single object. It also provides a mask that can be used to retrieve the original sequence of tensors.</p> <p>Attributes:</p>    Name Type Description     <code>storage</code>  <code>Sequence[torch.Tensor]</code>  <p>The sequence of tensors.</p>   <code>batch_first</code>  <code>bool = True</code>  <p>Whether the first dimension of the tensors is the batch dimension.</p> <p>If <code>True</code>, the first dimension is the batch dimension, i.e., <code>B, N, *</code>.</p> <p>If <code>False</code>, the first dimension is the sequence dimension, i.e., <code>N, B, *</code></p>    <p>Parameters:</p>    Name Type Description Default     <code>tensors</code>  <code>Sequence[Tensor]</code>    required    <code>batch_first</code>  <code>bool</code>    <code>True</code>     <p>Raises:</p>    Type Description      <code>ValueError</code>  <p>If <code>tensors</code> is not a sequence.</p> <p>If <code>tensors</code> is empty.</p>","location":"zh/package/#danling.NestedTensor"},{"title":"Notes","text":"<p>We have rewritten the <code>__getattr__</code> function to support as much native tensor operations as possible. However, not all operations are tested.</p> <p>Please file an issue if you find any bugs.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.device\ndevice(type='cpu')\n&gt;&gt;&gt; nested_tensor.dtype\ntorch.int64\n&gt;&gt;&gt; nested_tensor.tensor\ntensor([[1, 2, 3],\n        [4, 5, 0]])\n&gt;&gt;&gt; nested_tensor.mask\ntensor([[ True,  True,  True],\n        [ True,  True, False]])\n&gt;&gt;&gt; nested_tensor.to(torch.float).tensor\ntensor([[1., 2., 3.],\n        [4., 5., 0.]])\n&gt;&gt;&gt; nested_tensor.half().tensor\ntensor([[1., 2., 3.],\n        [4., 5., 0.]], dtype=torch.float16)\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>class NestedTensor:\n    r\"\"\"\n    Wrap a sequence of tensors into a single tensor with a mask.\n\n    In sequence to sequence tasks, elements of a batch are usually not of the same length.\n    This made it tricky to use a single tensor to represent a batch of sequences.\n\n    NestedTensor allows to store a sequence of tensors of different lengths in a single object.\n    It also provides a mask that can be used to retrieve the original sequence of tensors.\n\n    Attributes\n    ----------\n    storage: Sequence[torch.Tensor]\n        The sequence of tensors.\n    batch_first: bool = True\n        Whether the first dimension of the tensors is the batch dimension.\n\n        If `True`, the first dimension is the batch dimension, i.e., `B, N, *`.\n\n        If `False`, the first dimension is the sequence dimension, i.e., `N, B, *`\n\n    Parameters\n    ----------\n    tensors: Sequence[torch.Tensor]\n    batch_first: bool = True\n\n    Raises\n    ------\n    ValueError\n        If `tensors` is not a sequence.\n\n        If `tensors` is empty.\n\n    Notes\n    -----\n    We have rewritten the `__getattr__` function to support as much native tensor operations as possible.\n    However, not all operations are tested.\n\n    Please file an issue if you find any bugs.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.shape\n    torch.Size([2, 3])\n    &gt;&gt;&gt; nested_tensor.device\n    device(type='cpu')\n    &gt;&gt;&gt; nested_tensor.dtype\n    torch.int64\n    &gt;&gt;&gt; nested_tensor.tensor\n    tensor([[1, 2, 3],\n            [4, 5, 0]])\n    &gt;&gt;&gt; nested_tensor.mask\n    tensor([[ True,  True,  True],\n            [ True,  True, False]])\n    &gt;&gt;&gt; nested_tensor.to(torch.float).tensor\n    tensor([[1., 2., 3.],\n            [4., 5., 0.]])\n    &gt;&gt;&gt; nested_tensor.half().tensor\n    tensor([[1., 2., 3.],\n            [4., 5., 0.]], dtype=torch.float16)\n\n    ```\n    \"\"\"\n\n    storage: Sequence[Tensor] = []\n    batch_first: bool = True\n\n    def __init__(self, tensors: Sequence[Tensor], batch_first: bool = True) -&gt; None:\n        if not isinstance(tensors, Sequence):\n            raise ValueError(f\"NestedTensor should be initialised with a Sequence, bug got {type(values)}.\")\n        if len(tensors) == 0:\n            raise ValueError(f\"NestedTensor should be initialised with a non-empty sequence.\")\n        if not isinstance(tensors[0], Tensor):\n            tensors = tuple(torch.tensor(tensor) for tensor in tensors)\n        self.storage = tensors\n        self.batch_first = batch_first\n\n    @property\n    def tensor(self) -&gt; Tensor:\n        r\"\"\"\n        Return a single tensor by padding all the tensors.\n\n        Returns\n        -------\n        torch.Tensor\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.tensor\n        tensor([[1, 2, 3],\n                [4, 5, 0]])\n\n        ```\n        \"\"\"\n\n        return self._tensor(tuple(self.storage), self.batch_first)\n\n    @property\n    def mask(self) -&gt; Tensor:\n        r\"\"\"\n        Padding mask of `tensor`.\n\n        Returns\n        -------\n        torch.Tensor\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.mask\n        tensor([[ True,  True,  True],\n                [ True,  True, False]])\n\n        ```\n        \"\"\"\n\n        return self._mask(tuple(self.storage))\n\n    @property\n    def device(self) -&gt; torch.device:\n        r\"\"\"\n        Device of the NestedTensor.\n\n        Returns\n        -------\n        torch.Tensor\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.device\n        device(type='cpu')\n\n        ```\n        \"\"\"\n\n        return self._device(tuple(self.storage))\n\n    @property\n    def shape(self) -&gt; torch.Size:\n        r\"\"\"\n        Alias for `size`.\n\n        Returns\n        -------\n        torch.Size\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.shape\n        torch.Size([2, 3])\n        &gt;&gt;&gt; nested_tensor.storage.append(torch.tensor([6, 7, 8, 9]))\n        &gt;&gt;&gt; nested_tensor.shape\n        torch.Size([3, 4])\n\n        ```\n        \"\"\"\n\n        return self.size()\n\n    def size(self) -&gt; torch.Size:\n        r\"\"\"\n        Shape of the NestedTensor.\n\n        Returns\n        -------\n        torch.Size\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.size()\n        torch.Size([2, 3])\n        &gt;&gt;&gt; nested_tensor.storage[1] = torch.tensor([4, 5, 6, 7])\n        &gt;&gt;&gt; nested_tensor.size()\n        torch.Size([2, 4])\n\n        ```\n        \"\"\"\n\n        return self._size(tuple(self.storage))\n\n    def __getitem__(self, index) -&gt; Tuple[Tensor, Tensor]:\n        ret = self.storage[index]\n        if isinstance(ret, Tensor):\n            return ret, torch.ones_like(ret)  # pylint: disable=E1101\n        return self.tensor, self.mask\n\n    def __getattr__(self, name) -&gt; Any:\n        if not self.storage:\n            raise ValueError(f\"Unable to get {name} from an empty {self.__class__.__name__}\")\n        ret = [getattr(i, name) for i in self.storage]\n        elem = ret[0]\n        if isinstance(elem, Tensor):\n            return NestedTensor(ret)\n        if callable(elem):\n            return _TensorFuncWrapper(ret)\n        if len(set(ret)) == 1:\n            return elem\n        return ret\n\n    def __len__(self) -&gt; int:\n        return len(self.storage)\n\n    def __setstate__(self, storage) -&gt; None:\n        self.storage = storage\n\n    def __getstate__(self) -&gt; Sequence[Tensor]:\n        return self.storage\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _tensor(storage, batch_first) -&gt; Tensor:\n        return pad_sequence(storage, batch_first=batch_first)\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _mask(storage) -&gt; Tensor:\n        lens = torch.tensor([len(t) for t in storage], device=storage[0].device)\n        return torch.arange(max(lens), device=storage[0].device)[None, :] &lt; lens[:, None]  # pylint: disable=E1101\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _device(storage) -&gt; torch.device:\n        return storage[0].device\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _size(storage) -&gt; torch.Size:\n        return torch.Size(  # pylint: disable=E1101\n            [len(storage), max(t.shape[0] for t in storage), *storage[0].shape[1:]]\n        )\n</code></pre>","location":"zh/package/#danling.NestedTensor--notes"},{"title":"<code>device()</code>  <code>property</code>","text":"<p>Device of the NestedTensor.</p> <p>Returns:</p>    Type Description      <code>torch.Tensor</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.device\ndevice(type='cpu')\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@property\ndef device(self) -&gt; torch.device:\n    r\"\"\"\n    Device of the NestedTensor.\n\n    Returns\n    -------\n    torch.Tensor\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.device\n    device(type='cpu')\n\n    ```\n    \"\"\"\n\n    return self._device(tuple(self.storage))\n</code></pre>","location":"zh/package/#danling.tensors.nested_tensor.NestedTensor.device"},{"title":"<code>mask()</code>  <code>property</code>","text":"<p>Padding mask of <code>tensor</code>.</p> <p>Returns:</p>    Type Description      <code>torch.Tensor</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.mask\ntensor([[ True,  True,  True],\n        [ True,  True, False]])\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@property\ndef mask(self) -&gt; Tensor:\n    r\"\"\"\n    Padding mask of `tensor`.\n\n    Returns\n    -------\n    torch.Tensor\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.mask\n    tensor([[ True,  True,  True],\n            [ True,  True, False]])\n\n    ```\n    \"\"\"\n\n    return self._mask(tuple(self.storage))\n</code></pre>","location":"zh/package/#danling.tensors.nested_tensor.NestedTensor.mask"},{"title":"<code>shape()</code>  <code>property</code>","text":"<p>Alias for <code>size</code>.</p> <p>Returns:</p>    Type Description      <code>torch.Size</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.storage.append(torch.tensor([6, 7, 8, 9]))\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([3, 4])\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@property\ndef shape(self) -&gt; torch.Size:\n    r\"\"\"\n    Alias for `size`.\n\n    Returns\n    -------\n    torch.Size\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.shape\n    torch.Size([2, 3])\n    &gt;&gt;&gt; nested_tensor.storage.append(torch.tensor([6, 7, 8, 9]))\n    &gt;&gt;&gt; nested_tensor.shape\n    torch.Size([3, 4])\n\n    ```\n    \"\"\"\n\n    return self.size()\n</code></pre>","location":"zh/package/#danling.tensors.nested_tensor.NestedTensor.shape"},{"title":"<code>size()</code>","text":"<p>Shape of the NestedTensor.</p> <p>Returns:</p>    Type Description      <code>torch.Size</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.size()\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.storage[1] = torch.tensor([4, 5, 6, 7])\n&gt;&gt;&gt; nested_tensor.size()\ntorch.Size([2, 4])\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def size(self) -&gt; torch.Size:\n    r\"\"\"\n    Shape of the NestedTensor.\n\n    Returns\n    -------\n    torch.Size\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.size()\n    torch.Size([2, 3])\n    &gt;&gt;&gt; nested_tensor.storage[1] = torch.tensor([4, 5, 6, 7])\n    &gt;&gt;&gt; nested_tensor.size()\n    torch.Size([2, 4])\n\n    ```\n    \"\"\"\n\n    return self._size(tuple(self.storage))\n</code></pre>","location":"zh/package/#danling.tensors.nested_tensor.NestedTensor.size"},{"title":"<code>tensor()</code>  <code>property</code>","text":"<p>Return a single tensor by padding all the tensors.</p> <p>Returns:</p>    Type Description      <code>torch.Tensor</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.tensor\ntensor([[1, 2, 3],\n        [4, 5, 0]])\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@property\ndef tensor(self) -&gt; Tensor:\n    r\"\"\"\n    Return a single tensor by padding all the tensors.\n\n    Returns\n    -------\n    torch.Tensor\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.tensor\n    tensor([[1, 2, 3],\n            [4, 5, 0]])\n\n    ```\n    \"\"\"\n\n    return self._tensor(tuple(self.storage), self.batch_first)\n</code></pre>","location":"zh/package/#danling.tensors.nested_tensor.NestedTensor.tensor"},{"title":"<code>Registry</code>","text":"<p>         Bases: <code>NestedDict</code></p> <p><code>Registry</code> for components.</p>","location":"zh/package/#danling.Registry"},{"title":"Notes","text":"<p><code>Registry</code> inherits from <code>NestedDict</code>.</p> <p>Therefore, <code>Registry</code> comes in a nested structure by nature. You could create a sub-registry by simply calling <code>registry.sub_registry = Registry</code>, and access through <code>registry.sub_registry.register()</code>.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; registry = Registry(\"test\")\n&gt;&gt;&gt; @registry.register\n... @registry.register(\"Module1\")\n... class Module:\n...     def __init__(self, a, b):\n...         self.a = a\n...         self.b = b\n&gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n&gt;&gt;&gt; registry\nRegistry(\n  (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n  (Module): &lt;class 'danling.registry.registry.Module'&gt;\n  (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n)\n&gt;&gt;&gt; registry.lookup(\"Module\")\n&lt;class 'danling.registry.registry.Module'&gt;\n&gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n&gt;&gt;&gt; # registry.register(Module)\n&gt;&gt;&gt; module = registry.build(config[\"module\"])\n&gt;&gt;&gt; type(module)\n&lt;class 'danling.registry.registry.Module'&gt;\n&gt;&gt;&gt; module.a\n1\n&gt;&gt;&gt; module.b\n2\n</code></pre>  Source code in <code>danling/registry/registry.py</code> Python<pre><code>class Registry(NestedDict):\n    \"\"\"\n    `Registry` for components.\n\n    Notes\n    -----\n\n    `Registry` inherits from [`NestedDict`](https://chanfig.danling.org/nested_dict/).\n\n    Therefore, `Registry` comes in a nested structure by nature.\n    You could create a sub-registry by simply calling `registry.sub_registry = Registry`,\n    and access through `registry.sub_registry.register()`.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; registry = Registry(\"test\")\n    &gt;&gt;&gt; @registry.register\n    ... @registry.register(\"Module1\")\n    ... class Module:\n    ...     def __init__(self, a, b):\n    ...         self.a = a\n    ...         self.b = b\n    &gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n    &gt;&gt;&gt; registry\n    Registry(\n      (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n      (Module): &lt;class 'danling.registry.registry.Module'&gt;\n      (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n    )\n    &gt;&gt;&gt; registry.lookup(\"Module\")\n    &lt;class 'danling.registry.registry.Module'&gt;\n    &gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n    &gt;&gt;&gt; # registry.register(Module)\n    &gt;&gt;&gt; module = registry.build(config[\"module\"])\n    &gt;&gt;&gt; type(module)\n    &lt;class 'danling.registry.registry.Module'&gt;\n    &gt;&gt;&gt; module.a\n    1\n    &gt;&gt;&gt; module.b\n    2\n\n    ```\n    \"\"\"\n\n    override: bool = False\n\n    def __init__(self, override: bool = False):\n        super().__init__()\n        self.setattr(\"override\", override)\n\n    def register(self, component: Optional[Callable] = None, name: Optional[str] = None) -&gt; Callable:\n        r\"\"\"\n        Register a new component\n\n        Parameters\n        ----------\n        component: Optional[Callable] = None\n            The component to register.\n        name: Optional[str] = component.__name__\n            The name of the component.\n\n        Returns\n        -------\n        component: Callable\n            The registered component.\n\n        Raises\n        ------\n        ValueError\n            If the component with the same name already exists and `Registry.override=False`.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; registry = Registry(\"test\")\n        &gt;&gt;&gt; @registry.register\n        ... @registry.register(\"Module1\")\n        ... class Module:\n        ...     def __init__(self, a, b):\n        ...         self.a = a\n        ...         self.b = b\n        &gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n        &gt;&gt;&gt; registry\n        Registry(\n          (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n          (Module): &lt;class 'danling.registry.registry.Module'&gt;\n          (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n        )\n\n        ```\n        \"\"\"\n\n        if isinstance(name, str) and name in self and not self.override:\n            raise ValueError(f\"Component with name {name} already exists\")\n\n        # Registry.register()\n        if name is not None:\n            self.set(name, component)\n\n        # @Registry.register()\n        @wraps(self.register)\n        def register(component, name=None):\n            if name is None:\n                name = component.__name__\n            self.set(name, component)\n            return component\n\n        # @Registry.register\n        if callable(component) and name is None:\n            return register(component)\n\n        return lambda x: register(x, component)\n\n    def lookup(self, name: str) -&gt; Any:\n        r\"\"\"\n        Lookup for a component.\n\n        Parameters\n        ----------\n        name: str\n            The name of the component.\n\n        Returns\n        -------\n        value: Any\n\n        Raises\n        ------\n        KeyError\n            If the component is not registered.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; registry = Registry(\"test\")\n        &gt;&gt;&gt; @registry.register\n        ... class Module:\n        ...     def __init__(self, a, b):\n        ...         self.a = a\n        ...         self.b = b\n        &gt;&gt;&gt; registry.lookup(\"Module\")\n        &lt;class 'danling.registry.registry.Module'&gt;\n\n        ```\n        \"\"\"\n\n        return self.get(name)\n\n    def build(self, name: str, *args, **kwargs):\n        r\"\"\"\n        Build a component.\n\n        Parameters\n        ----------\n        name: str\n            The name of the component.\n        *args\n            The arguments to pass to the component.\n        **kwargs\n            The keyword arguments to pass to the component.\n\n        Returns\n        -------\n        component: Callable\n\n        Raises\n        ------\n        KeyError\n            If the component is not registered.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; registry = Registry(\"test\")\n        &gt;&gt;&gt; @registry.register\n        ... class Module:\n        ...     def __init__(self, a, b):\n        ...         self.a = a\n        ...         self.b = b\n        &gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n        &gt;&gt;&gt; # registry.register(Module)\n        &gt;&gt;&gt; module = registry.build(config[\"module\"])\n        &gt;&gt;&gt; type(module)\n        &lt;class 'danling.registry.registry.Module'&gt;\n        &gt;&gt;&gt; module.a\n        1\n        &gt;&gt;&gt; module.b\n        2\n\n        ```\n        \"\"\"\n\n        if isinstance(name, Mapping) and not args and not kwargs:\n            name, kwargs = name.pop(\"name\"), name  # type: ignore\n        return self.get(name)(*args, **kwargs)\n\n    def __wrapped__(self):\n        pass\n</code></pre>","location":"zh/package/#danling.Registry--notes"},{"title":"<code>build(name, *args, **kwargs)</code>","text":"<p>Build a component.</p> <p>Parameters:</p>    Name Type Description Default     <code>name</code>  <code>str</code>  <p>The name of the component.</p>  required    <code>*args</code>   <p>The arguments to pass to the component.</p>  <code>()</code>    <code>**kwargs</code>   <p>The keyword arguments to pass to the component.</p>  <code>{}</code>     <p>Returns:</p>    Name Type Description     <code>component</code>  <code>Callable</code>      <p>Raises:</p>    Type Description      <code>KeyError</code>  <p>If the component is not registered.</p>    <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; registry = Registry(\"test\")\n&gt;&gt;&gt; @registry.register\n... class Module:\n...     def __init__(self, a, b):\n...         self.a = a\n...         self.b = b\n&gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n&gt;&gt;&gt; # registry.register(Module)\n&gt;&gt;&gt; module = registry.build(config[\"module\"])\n&gt;&gt;&gt; type(module)\n&lt;class 'danling.registry.registry.Module'&gt;\n&gt;&gt;&gt; module.a\n1\n&gt;&gt;&gt; module.b\n2\n</code></pre>  Source code in <code>danling/registry/registry.py</code> Python<pre><code>def build(self, name: str, *args, **kwargs):\n    r\"\"\"\n    Build a component.\n\n    Parameters\n    ----------\n    name: str\n        The name of the component.\n    *args\n        The arguments to pass to the component.\n    **kwargs\n        The keyword arguments to pass to the component.\n\n    Returns\n    -------\n    component: Callable\n\n    Raises\n    ------\n    KeyError\n        If the component is not registered.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; registry = Registry(\"test\")\n    &gt;&gt;&gt; @registry.register\n    ... class Module:\n    ...     def __init__(self, a, b):\n    ...         self.a = a\n    ...         self.b = b\n    &gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n    &gt;&gt;&gt; # registry.register(Module)\n    &gt;&gt;&gt; module = registry.build(config[\"module\"])\n    &gt;&gt;&gt; type(module)\n    &lt;class 'danling.registry.registry.Module'&gt;\n    &gt;&gt;&gt; module.a\n    1\n    &gt;&gt;&gt; module.b\n    2\n\n    ```\n    \"\"\"\n\n    if isinstance(name, Mapping) and not args and not kwargs:\n        name, kwargs = name.pop(\"name\"), name  # type: ignore\n    return self.get(name)(*args, **kwargs)\n</code></pre>","location":"zh/package/#danling.registry.registry.Registry.build"},{"title":"<code>lookup(name)</code>","text":"<p>Lookup for a component.</p> <p>Parameters:</p>    Name Type Description Default     <code>name</code>  <code>str</code>  <p>The name of the component.</p>  required     <p>Returns:</p>    Name Type Description     <code>value</code>  <code>Any</code>      <p>Raises:</p>    Type Description      <code>KeyError</code>  <p>If the component is not registered.</p>    <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; registry = Registry(\"test\")\n&gt;&gt;&gt; @registry.register\n... class Module:\n...     def __init__(self, a, b):\n...         self.a = a\n...         self.b = b\n&gt;&gt;&gt; registry.lookup(\"Module\")\n&lt;class 'danling.registry.registry.Module'&gt;\n</code></pre>  Source code in <code>danling/registry/registry.py</code> Python<pre><code>def lookup(self, name: str) -&gt; Any:\n    r\"\"\"\n    Lookup for a component.\n\n    Parameters\n    ----------\n    name: str\n        The name of the component.\n\n    Returns\n    -------\n    value: Any\n\n    Raises\n    ------\n    KeyError\n        If the component is not registered.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; registry = Registry(\"test\")\n    &gt;&gt;&gt; @registry.register\n    ... class Module:\n    ...     def __init__(self, a, b):\n    ...         self.a = a\n    ...         self.b = b\n    &gt;&gt;&gt; registry.lookup(\"Module\")\n    &lt;class 'danling.registry.registry.Module'&gt;\n\n    ```\n    \"\"\"\n\n    return self.get(name)\n</code></pre>","location":"zh/package/#danling.registry.registry.Registry.lookup"},{"title":"<code>register(component=None, name=None)</code>","text":"<p>Register a new component</p> <p>Parameters:</p>    Name Type Description Default     <code>component</code>  <code>Optional[Callable]</code>  <p>The component to register.</p>  <code>None</code>    <code>name</code>  <code>Optional[str]</code>  <p>The name of the component.</p>  <code>None</code>     <p>Returns:</p>    Name Type Description     <code>component</code>  <code>Callable</code>  <p>The registered component.</p>    <p>Raises:</p>    Type Description      <code>ValueError</code>  <p>If the component with the same name already exists and <code>Registry.override=False</code>.</p>    <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; registry = Registry(\"test\")\n&gt;&gt;&gt; @registry.register\n... @registry.register(\"Module1\")\n... class Module:\n...     def __init__(self, a, b):\n...         self.a = a\n...         self.b = b\n&gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n&gt;&gt;&gt; registry\nRegistry(\n  (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n  (Module): &lt;class 'danling.registry.registry.Module'&gt;\n  (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n)\n</code></pre>  Source code in <code>danling/registry/registry.py</code> Python<pre><code>def register(self, component: Optional[Callable] = None, name: Optional[str] = None) -&gt; Callable:\n    r\"\"\"\n    Register a new component\n\n    Parameters\n    ----------\n    component: Optional[Callable] = None\n        The component to register.\n    name: Optional[str] = component.__name__\n        The name of the component.\n\n    Returns\n    -------\n    component: Callable\n        The registered component.\n\n    Raises\n    ------\n    ValueError\n        If the component with the same name already exists and `Registry.override=False`.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; registry = Registry(\"test\")\n    &gt;&gt;&gt; @registry.register\n    ... @registry.register(\"Module1\")\n    ... class Module:\n    ...     def __init__(self, a, b):\n    ...         self.a = a\n    ...         self.b = b\n    &gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n    &gt;&gt;&gt; registry\n    Registry(\n      (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n      (Module): &lt;class 'danling.registry.registry.Module'&gt;\n      (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n    )\n\n    ```\n    \"\"\"\n\n    if isinstance(name, str) and name in self and not self.override:\n        raise ValueError(f\"Component with name {name} already exists\")\n\n    # Registry.register()\n    if name is not None:\n        self.set(name, component)\n\n    # @Registry.register()\n    @wraps(self.register)\n    def register(component, name=None):\n        if name is None:\n            name = component.__name__\n        self.set(name, component)\n        return component\n\n    # @Registry.register\n    if callable(component) and name is None:\n        return register(component)\n\n    return lambda x: register(x, component)\n</code></pre>","location":"zh/package/#danling.registry.registry.Registry.register"},{"title":"<code>SelfAttention</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Allows the model to jointly attend to information from different representation subspaces. See <code>Attention Is All You Need &lt;https://arxiv.org/abs/1706.03762&gt;</code>_ .. math::     \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O where :math:<code>head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)</code>. Args:     embed_dim: total dimension of the model.     num_heads: parallel attention heads.     dropout: a Dropout layer on attn_output_weights. Default: 0.0.     bias: add bias as module parameter. Default: True.     add_bias_kv: add bias to the key and value sequences at dim=0.     add_zero_attn: add a new batch of zeros to the key and                    value sequences at dim=1.     k_dim: total number of features in key. Default: None.     v_dim: total number of features in value. Default: None.     batch_first: If <code>False</code>, then the input and output tensors are provided         as (seq, batch, feature). Default: <code>True</code> (batch, seq, feature). Note that if :attr:<code>k_dim</code> and :attr:<code>v_dim</code> are None, they will be set to :attr:<code>embed_dim</code> such that query, key, and value have the same number of features. Examples::     &gt;&gt;&gt; multihead_attn = dl.models.SelfAttention(embed_dim, num_heads)     &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)</p>  Source code in <code>danling/models/transformer/attention/self_attention.py</code> Python<pre><code>class SelfAttention(nn.Module):\n    r\"\"\"Allows the model to jointly attend to information\n    from different representation subspaces.\n    See `Attention Is All You Need &lt;https://arxiv.org/abs/1706.03762&gt;`_\n    .. math::\n        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n    where :math:`head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n    Args:\n        embed_dim: total dimension of the model.\n        num_heads: parallel attention heads.\n        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n        bias: add bias as module parameter. Default: True.\n        add_bias_kv: add bias to the key and value sequences at dim=0.\n        add_zero_attn: add a new batch of zeros to the key and\n                       value sequences at dim=1.\n        k_dim: total number of features in key. Default: None.\n        v_dim: total number of features in value. Default: None.\n        batch_first: If ``False``, then the input and output tensors are provided\n            as (seq, batch, feature). Default: ``True`` (batch, seq, feature).\n    Note that if :attr:`k_dim` and :attr:`v_dim` are None, they will be set\n    to :attr:`embed_dim` such that query, key, and value have the same\n    number of features.\n    Examples::\n        &gt;&gt;&gt; multihead_attn = dl.models.SelfAttention(embed_dim, num_heads)\n        &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)\n    \"\"\"\n    __constants__ = [\"batch_first\"]\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        attn_dropout: float = 0.0,\n        scale_factor: float = 1.0,\n        bias: bool = True,\n        batch_first: bool = True,\n        **kwargs: Optional[Dict[str, Any]],\n    ) -&gt; None:\n        super(SelfAttention, self).__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.scale_factor = scale_factor\n        self.batch_first = batch_first\n        self.head_dim = self.embed_dim // self.num_heads\n        self.scaling = float(self.head_dim * self.scale_factor) ** -0.5\n        if not self.head_dim * self.num_heads == self.embed_dim:\n            raise ValueError(f\"embed_dim {self.embed_dim} not divisible by num_heads {self.num_heads}\")\n\n        self.in_proj = nn.Linear(self.embed_dim, self.embed_dim + self.k_dim + self.v_dim, bias=bias)\n        self.dropout = nn.Dropout(attn_dropout)\n        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=bias)\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        nn.init.xavier_uniform_(self.in_proj.weight)\n        if self.in_proj.bias is not None:\n            nn.init.constant_(self.in_proj.bias, 0.0)\n            nn.init.constant_(self.out_proj.bias, 0.0)\n\n    def forward(\n        self,\n        query: Tensor,\n        key: Tensor,\n        value: Tensor,\n        attn_bias: Optional[Tensor] = None,\n        attn_mask: Optional[Tensor] = None,\n        key_padding_mask: Optional[Tensor] = None,\n        need_weights: bool = False,\n    ) -&gt; Tuple[Tensor, Optional[Tensor]]:\n        r\"\"\"\n        Args:\n            query, key, value: map a query and a set of key-value pairs to an output.\n                See \"Attention Is All You Need\" for more details.\n            attn_bias: 2D or 3D mask that add bias to attention output weights. Used for relative positional embedding.\n                A 2D bias will be broadcasted for all the batches while a 3D mask allows to specify a different mask for\n                the entries of each batch.\n            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n                the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n            key_padding_mask: if provided, specified padding elements in the key will\n                be ignored by the attention. When given a binary mask and a value is True,\n                the corresponding value on the attention layer will be ignored. When given\n                a byte mask and a value is non-zero, the corresponding value on the attention\n                layer will be ignored\n            need_weights: output attn_output_weights.\n        Shapes for inputs:\n            - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n                the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n            - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n                the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n            - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n                the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n            - attn_bias: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n                source sequence length.\n                If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n                length, S is the source sequence length. ``attn_bias`` allows to pass pos embed directly into attention\n                If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will\n                be unchanged. If a BoolTensor is provided, positions with ``True`` is not allowed to attend while ``False``\n                values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight.\n            - attn_mask: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n                source sequence length.\n                If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n                length, S is the source sequence length. ``attn_mask`` ensure that position i is allowed to attend\n                the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n                while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n                is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n                is provided, it will be added to the attention weight.\n            - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n                If a ByteTensor is provided, the non-zero positions will be ignored while the position\n                with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the\n                value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n        Outputs:\n            - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n                E is the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n            - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n                L is the target sequence length, S is the source sequence length.\n        \"\"\"\n        if self.batch_first:\n            query, key, value = [x.transpose(0, 1) for x in (query, key, value)]\n\n        # set up shape vars\n        target_len, batch_size, embed_dim = query.shape\n        source_len, _, _ = key.shape\n        if not key.shape[:2] == value.shape[:2]:\n            raise ValueError(f\"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}\")\n\n        q, k, v = self.in_projection(query, key, value)\n\n        # prep attention mask\n        if attn_mask is not None:\n            if attn_mask.dtype == torch.uint8:\n                warnings.warn(\n                    \"attn_mask is of type uint8. This type is deprecated. Please use bool or float tensors instead.\"\n                )\n                attn_mask = attn_mask.to(torch.bool)\n            elif not (attn_mask.is_floating_point() or attn_mask.dtype == torch.bool):\n                raise ValueError(f\"attn_mask should have type float or bool, but got {attn_mask.dtype}.\")\n            # ensure attn_mask's dim is 3\n            if attn_mask.dim() == 2:\n                correct_shape = (target_len, source_len)\n                if attn_mask.shape != correct_shape:\n                    raise ValueError(f\"attn_mask should have shape {correct_shape}, but got {attn_mask.shape}.\")\n                attn_mask = attn_mask.unsqueeze(0)\n            elif attn_mask.dim() == 3:\n                correct_shape = (batch_size * self.num_heads, target_len, source_len)  # type: ignore\n                if attn_mask.shape != correct_shape:\n                    raise ValueError(f\"attn_mask should have shape {correct_shape}, but got {attn_mask.shape}.\")\n            else:\n                raise RuntimeError(f\"attn_mask should have dimension 2 or 3, bug got {attn_mask.dim()}.\")\n\n        # prep key padding mask\n        if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n            warnings.warn(\n                \"key_padding_mask is of type uint8. This type is deprecated. Please use bool or float tensors instead.\"\n            )\n            key_padding_mask = key_padding_mask.to(torch.bool)\n\n        # reshape q, k, v for multihead attention and make em batch first\n        q = q.reshape(target_len, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n        k = k.reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n        v = v.reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n\n        # merge key padding and attention masks\n        if key_padding_mask is not None:\n            if key_padding_mask.shape != (batch_size, source_len):\n                raise ValueError(\n                    f\"key_padding_mask should have shape {(batch_size, source_len)}, but got {key_padding_mask.shape}\"\n                )\n            key_padding_mask = (\n                key_padding_mask.view(batch_size, 1, 1, source_len)\n                .expand(-1, self.num_heads, -1, -1)\n                .reshape(batch_size * self.num_heads, 1, source_len)\n            )\n            if attn_mask is None:\n                attn_mask = key_padding_mask\n            elif attn_mask.dtype == torch.bool:\n                attn_mask = attn_mask.logical_or(key_padding_mask)\n            else:\n                attn_mask = attn_mask.masked_fill(key_padding_mask, float(\"-inf\"))\n\n        # convert mask to float\n        if attn_mask is not None and attn_mask.dtype == torch.bool:\n            new_attn_mask = torch.zeros_like(attn_mask, dtype=torch.float)\n            new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n            attn_mask = new_attn_mask\n\n        # (deep breath) calculate attention and out projection\n        attn_output, attn_output_weights = self.attention(q, k, v, attn_bias, attn_mask)\n        attn_output = attn_output.transpose(0, 1).reshape(target_len, batch_size, embed_dim)\n        attn_output = self.out_projection(attn_output)\n\n        # attn_output_weights is set to torch.empty(0, requires_grad=False) to avoid errors in DDP\n        attn_output_weights = (\n            attn_output_weights.view(batch_size, self.num_heads, target_len, source_len)\n            if need_weights\n            else torch.empty(0, requires_grad=False)\n        )\n\n        if self.batch_first:\n            return attn_output.transpose(0, 1), attn_output_weights\n        else:\n            return attn_output, attn_output_weights\n\n    def in_projection(self, q: Tensor, k: Tensor, v: Tensor) -&gt; Tuple[Tensor, Tensor, Tensor]:\n        r\"\"\"\n        Performs the in-projection step of the attention operation, using packed weights.\n        Output is a triple containing projection tensors for query, key and value.\n        Args:\n            q, k, v: query, key and value tensors to be projected. For self-attention,\n                these are typically the same tensor; for encoder-decoder attention,\n                k and v are typically the same tensor. (We take advantage of these\n                identities for performance if they are present.) Regardless, q, k and v\n                must share a common embedding dimension; otherwise their shapes may vary.\n        Shape:\n            Inputs:\n            - q: :math:`(..., E)` where E is the embedding dimension\n            - k: :math:`(..., E)` where E is the embedding dimension\n            - v: :math:`(..., E)` where E is the embedding dimension\n            Output:\n            - in output list :math:`[q', k', v']`, each output tensor will have the\n                same shape as the corresponding input tensor.\n        \"\"\"\n        if k is v:\n            # self-attention\n            if q is k:\n                return self.in_proj(q).split((self.embed_dim, self.k_dim, self.v_dim), dim=-1)\n            # encoder-decoder attention\n            else:\n                w_q, w_kv = self.in_proj.weight.split([self.embed_dim, self.k_dim + self.v_dim])\n                b_q, b_kv = (\n                    None\n                    if self.in_proj.bias is None\n                    else self.in_proj.bias.split([self.embed_dim, self.k_dim + self.v_dim])\n                )\n                return (F.linear(q, w_q, b_q),) + F.linear(k, w_kv, b_kv).split((self.k_dim, self.v_dim), dim=-1)\n        else:\n            w_q, w_k, w_v = self.in_proj.weight.split([self.embed_dim, self.k_dim, self.v_dim])\n            b_q, b_k, b_v = (\n                None if self.in_proj.bias is None else self.in_proj.bias.split([self.embed_dim, self.k_dim, self.v_dim])\n            )\n            return F.linear(q, w_q, b_q), F.linear(k, w_k, b_k), F.linear(v, w_v, b_v)\n\n    def attention(\n        self,\n        q: Tensor,\n        k: Tensor,\n        v: Tensor,\n        attn_bias: Optional[Tensor] = None,\n        attn_mask: Optional[Tensor] = None,\n    ) -&gt; Tuple[Tensor, Tensor]:\n        r\"\"\"\n        Computes scaled dot product attention on query, key and value tensors, using\n        an optional attention mask if passed, and applying dropout if a probability\n        greater than 0.0 is specified.\n        Returns a tensor pair containing attended values and attention weights.\n        Args:\n            q, k, v: query, key and value tensors. See Shape section for shape details.\n            attn_mask: optional tensor containing mask values to be added to calculated\n                attention. May be 2D or 3D; see Shape section for details.\n            attn_bias: optional tensor containing bias values to be added to calculated\n                attention. Used for relative positional embedding. May be 2D or 3D; see\n                Shape section for details.\n        Shape:\n            - q: :math:`(B, Nt, E)` where B is batch size, Nt is the target sequence length,\n                and E is embedding dimension.\n            - key: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n                and E is embedding dimension.\n            - value: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n                and E is embedding dimension.\n            - attn_bias: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n                shape :math:`(Nt, Ns)`.\n            - attn_mask: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n                shape :math:`(Nt, Ns)`.\n            - Output: attention values have shape :math:`(B, Nt, E)`; attention weights\n                have shape :math:`(B, Nt, Ns)`\n        \"\"\"\n        q *= self.scaling\n        # (B, Nt, E) x (B, E, Ns) -&gt; (B, Nt, Ns)\n        attn = torch.bmm(q, k.transpose(-2, -1))\n        if attn_bias is not None:\n            attn += attn_bias\n        if attn_mask is not None:\n            attn += attn_mask\n        attn = F.softmax(attn, dim=-1)\n        attn = self.dropout(attn)\n        # (B, Nt, Ns) x (B, Ns, E) -&gt; (B, Nt, E)\n        output = torch.bmm(attn, v)\n        return output, attn\n\n    def out_projection(self, attn_output: Tensor) -&gt; Tensor:\n        return self.out_proj(attn_output)\n</code></pre>","location":"zh/package/#danling.SelfAttention"},{"title":"<code>attention(q, k, v, attn_bias=None, attn_mask=None)</code>","text":"<p>Computes scaled dot product attention on query, key and value tensors, using an optional attention mask if passed, and applying dropout if a probability greater than 0.0 is specified. Returns a tensor pair containing attended values and attention weights. Args:     q, k, v: query, key and value tensors. See Shape section for shape details.     attn_mask: optional tensor containing mask values to be added to calculated         attention. May be 2D or 3D; see Shape section for details.     attn_bias: optional tensor containing bias values to be added to calculated         attention. Used for relative positional embedding. May be 2D or 3D; see         Shape section for details. Shape:     - q: :math:<code>(B, Nt, E)</code> where B is batch size, Nt is the target sequence length,         and E is embedding dimension.     - key: :math:<code>(B, Ns, E)</code> where B is batch size, Ns is the source sequence length,         and E is embedding dimension.     - value: :math:<code>(B, Ns, E)</code> where B is batch size, Ns is the source sequence length,         and E is embedding dimension.     - attn_bias: either a 3D tensor of shape :math:<code>(B, Nt, Ns)</code> or a 2D tensor of         shape :math:<code>(Nt, Ns)</code>.     - attn_mask: either a 3D tensor of shape :math:<code>(B, Nt, Ns)</code> or a 2D tensor of         shape :math:<code>(Nt, Ns)</code>.     - Output: attention values have shape :math:<code>(B, Nt, E)</code>; attention weights         have shape :math:<code>(B, Nt, Ns)</code></p>  Source code in <code>danling/models/transformer/attention/self_attention.py</code> Python<pre><code>def attention(\n    self,\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    attn_bias: Optional[Tensor] = None,\n    attn_mask: Optional[Tensor] = None,\n) -&gt; Tuple[Tensor, Tensor]:\n    r\"\"\"\n    Computes scaled dot product attention on query, key and value tensors, using\n    an optional attention mask if passed, and applying dropout if a probability\n    greater than 0.0 is specified.\n    Returns a tensor pair containing attended values and attention weights.\n    Args:\n        q, k, v: query, key and value tensors. See Shape section for shape details.\n        attn_mask: optional tensor containing mask values to be added to calculated\n            attention. May be 2D or 3D; see Shape section for details.\n        attn_bias: optional tensor containing bias values to be added to calculated\n            attention. Used for relative positional embedding. May be 2D or 3D; see\n            Shape section for details.\n    Shape:\n        - q: :math:`(B, Nt, E)` where B is batch size, Nt is the target sequence length,\n            and E is embedding dimension.\n        - key: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n            and E is embedding dimension.\n        - value: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n            and E is embedding dimension.\n        - attn_bias: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n            shape :math:`(Nt, Ns)`.\n        - attn_mask: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n            shape :math:`(Nt, Ns)`.\n        - Output: attention values have shape :math:`(B, Nt, E)`; attention weights\n            have shape :math:`(B, Nt, Ns)`\n    \"\"\"\n    q *= self.scaling\n    # (B, Nt, E) x (B, E, Ns) -&gt; (B, Nt, Ns)\n    attn = torch.bmm(q, k.transpose(-2, -1))\n    if attn_bias is not None:\n        attn += attn_bias\n    if attn_mask is not None:\n        attn += attn_mask\n    attn = F.softmax(attn, dim=-1)\n    attn = self.dropout(attn)\n    # (B, Nt, Ns) x (B, Ns, E) -&gt; (B, Nt, E)\n    output = torch.bmm(attn, v)\n    return output, attn\n</code></pre>","location":"zh/package/#danling.models.transformer.attention.self_attention.SelfAttention.attention"},{"title":"<code>forward(query, key, value, attn_bias=None, attn_mask=None, key_padding_mask=None, need_weights=False)</code>","text":"<p>Shapes for inputs:     - query: :math:<code>(L, N, E)</code> where L is the target sequence length, N is the batch size, E is         the embedding dimension. :math:<code>(N, L, E)</code> if <code>batch_first</code> is <code>True</code>.     - key: :math:<code>(S, N, E)</code>, where S is the source sequence length, N is the batch size, E is         the embedding dimension. :math:<code>(N, S, E)</code> if <code>batch_first</code> is <code>True</code>.     - value: :math:<code>(S, N, E)</code> where S is the source sequence length, N is the batch size, E is         the embedding dimension. :math:<code>(N, S, E)</code> if <code>batch_first</code> is <code>True</code>.     - attn_bias: if a 2D mask: :math:<code>(L, S)</code> where L is the target sequence length, S is the         source sequence length.         If a 3D mask: :math:<code>(N\\cdot\\text{num\\_heads}, L, S)</code> where N is the batch size, L is the target sequence         length, S is the source sequence length. <code>attn_bias</code> allows to pass pos embed directly into attention         If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will         be unchanged. If a BoolTensor is provided, positions with <code>True</code> is not allowed to attend while <code>False</code>         values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight.     - attn_mask: if a 2D mask: :math:<code>(L, S)</code> where L is the target sequence length, S is the         source sequence length.         If a 3D mask: :math:<code>(N\\cdot\\text{num\\_heads}, L, S)</code> where N is the batch size, L is the target sequence         length, S is the source sequence length. <code>attn_mask</code> ensure that position i is allowed to attend         the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend         while the zero positions will be unchanged. If a BoolTensor is provided, positions with <code>True</code>         is not allowed to attend while <code>False</code> values will be unchanged. If a FloatTensor         is provided, it will be added to the attention weight.     - key_padding_mask: :math:<code>(N, S)</code> where N is the batch size, S is the source sequence length.         If a ByteTensor is provided, the non-zero positions will be ignored while the position         with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the         value of <code>True</code> will be ignored while the position with the value of <code>False</code> will be unchanged. Outputs:     - attn_output: :math:<code>(L, N, E)</code> where L is the target sequence length, N is the batch size,         E is the embedding dimension. :math:<code>(N, L, E)</code> if <code>batch_first</code> is <code>True</code>.     - attn_output_weights: :math:<code>(N, L, S)</code> where N is the batch size,         L is the target sequence length, S is the source sequence length.</p>  Source code in <code>danling/models/transformer/attention/self_attention.py</code> Python<pre><code>def forward(\n    self,\n    query: Tensor,\n    key: Tensor,\n    value: Tensor,\n    attn_bias: Optional[Tensor] = None,\n    attn_mask: Optional[Tensor] = None,\n    key_padding_mask: Optional[Tensor] = None,\n    need_weights: bool = False,\n) -&gt; Tuple[Tensor, Optional[Tensor]]:\n    r\"\"\"\n    Args:\n        query, key, value: map a query and a set of key-value pairs to an output.\n            See \"Attention Is All You Need\" for more details.\n        attn_bias: 2D or 3D mask that add bias to attention output weights. Used for relative positional embedding.\n            A 2D bias will be broadcasted for all the batches while a 3D mask allows to specify a different mask for\n            the entries of each batch.\n        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n        key_padding_mask: if provided, specified padding elements in the key will\n            be ignored by the attention. When given a binary mask and a value is True,\n            the corresponding value on the attention layer will be ignored. When given\n            a byte mask and a value is non-zero, the corresponding value on the attention\n            layer will be ignored\n        need_weights: output attn_output_weights.\n    Shapes for inputs:\n        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n            the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n            the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n            the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n        - attn_bias: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n            source sequence length.\n            If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n            length, S is the source sequence length. ``attn_bias`` allows to pass pos embed directly into attention\n            If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will\n            be unchanged. If a BoolTensor is provided, positions with ``True`` is not allowed to attend while ``False``\n            values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight.\n        - attn_mask: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n            source sequence length.\n            If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n            length, S is the source sequence length. ``attn_mask`` ensure that position i is allowed to attend\n            the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n            while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n            is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n            is provided, it will be added to the attention weight.\n        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n            If a ByteTensor is provided, the non-zero positions will be ignored while the position\n            with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the\n            value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n    Outputs:\n        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n            E is the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n            L is the target sequence length, S is the source sequence length.\n    \"\"\"\n    if self.batch_first:\n        query, key, value = [x.transpose(0, 1) for x in (query, key, value)]\n\n    # set up shape vars\n    target_len, batch_size, embed_dim = query.shape\n    source_len, _, _ = key.shape\n    if not key.shape[:2] == value.shape[:2]:\n        raise ValueError(f\"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}\")\n\n    q, k, v = self.in_projection(query, key, value)\n\n    # prep attention mask\n    if attn_mask is not None:\n        if attn_mask.dtype == torch.uint8:\n            warnings.warn(\n                \"attn_mask is of type uint8. This type is deprecated. Please use bool or float tensors instead.\"\n            )\n            attn_mask = attn_mask.to(torch.bool)\n        elif not (attn_mask.is_floating_point() or attn_mask.dtype == torch.bool):\n            raise ValueError(f\"attn_mask should have type float or bool, but got {attn_mask.dtype}.\")\n        # ensure attn_mask's dim is 3\n        if attn_mask.dim() == 2:\n            correct_shape = (target_len, source_len)\n            if attn_mask.shape != correct_shape:\n                raise ValueError(f\"attn_mask should have shape {correct_shape}, but got {attn_mask.shape}.\")\n            attn_mask = attn_mask.unsqueeze(0)\n        elif attn_mask.dim() == 3:\n            correct_shape = (batch_size * self.num_heads, target_len, source_len)  # type: ignore\n            if attn_mask.shape != correct_shape:\n                raise ValueError(f\"attn_mask should have shape {correct_shape}, but got {attn_mask.shape}.\")\n        else:\n            raise RuntimeError(f\"attn_mask should have dimension 2 or 3, bug got {attn_mask.dim()}.\")\n\n    # prep key padding mask\n    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n        warnings.warn(\n            \"key_padding_mask is of type uint8. This type is deprecated. Please use bool or float tensors instead.\"\n        )\n        key_padding_mask = key_padding_mask.to(torch.bool)\n\n    # reshape q, k, v for multihead attention and make em batch first\n    q = q.reshape(target_len, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n    k = k.reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n    v = v.reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n\n    # merge key padding and attention masks\n    if key_padding_mask is not None:\n        if key_padding_mask.shape != (batch_size, source_len):\n            raise ValueError(\n                f\"key_padding_mask should have shape {(batch_size, source_len)}, but got {key_padding_mask.shape}\"\n            )\n        key_padding_mask = (\n            key_padding_mask.view(batch_size, 1, 1, source_len)\n            .expand(-1, self.num_heads, -1, -1)\n            .reshape(batch_size * self.num_heads, 1, source_len)\n        )\n        if attn_mask is None:\n            attn_mask = key_padding_mask\n        elif attn_mask.dtype == torch.bool:\n            attn_mask = attn_mask.logical_or(key_padding_mask)\n        else:\n            attn_mask = attn_mask.masked_fill(key_padding_mask, float(\"-inf\"))\n\n    # convert mask to float\n    if attn_mask is not None and attn_mask.dtype == torch.bool:\n        new_attn_mask = torch.zeros_like(attn_mask, dtype=torch.float)\n        new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n        attn_mask = new_attn_mask\n\n    # (deep breath) calculate attention and out projection\n    attn_output, attn_output_weights = self.attention(q, k, v, attn_bias, attn_mask)\n    attn_output = attn_output.transpose(0, 1).reshape(target_len, batch_size, embed_dim)\n    attn_output = self.out_projection(attn_output)\n\n    # attn_output_weights is set to torch.empty(0, requires_grad=False) to avoid errors in DDP\n    attn_output_weights = (\n        attn_output_weights.view(batch_size, self.num_heads, target_len, source_len)\n        if need_weights\n        else torch.empty(0, requires_grad=False)\n    )\n\n    if self.batch_first:\n        return attn_output.transpose(0, 1), attn_output_weights\n    else:\n        return attn_output, attn_output_weights\n</code></pre>","location":"zh/package/#danling.models.transformer.attention.self_attention.SelfAttention.forward"},{"title":"<code>in_projection(q, k, v)</code>","text":"<p>Performs the in-projection step of the attention operation, using packed weights. Output is a triple containing projection tensors for query, key and value. Args:     q, k, v: query, key and value tensors to be projected. For self-attention,         these are typically the same tensor; for encoder-decoder attention,         k and v are typically the same tensor. (We take advantage of these         identities for performance if they are present.) Regardless, q, k and v         must share a common embedding dimension; otherwise their shapes may vary. Shape:     Inputs:     - q: :math:<code>(..., E)</code> where E is the embedding dimension     - k: :math:<code>(..., E)</code> where E is the embedding dimension     - v: :math:<code>(..., E)</code> where E is the embedding dimension     Output:     - in output list :math:<code>[q', k', v']</code>, each output tensor will have the         same shape as the corresponding input tensor.</p>  Source code in <code>danling/models/transformer/attention/self_attention.py</code> Python<pre><code>def in_projection(self, q: Tensor, k: Tensor, v: Tensor) -&gt; Tuple[Tensor, Tensor, Tensor]:\n    r\"\"\"\n    Performs the in-projection step of the attention operation, using packed weights.\n    Output is a triple containing projection tensors for query, key and value.\n    Args:\n        q, k, v: query, key and value tensors to be projected. For self-attention,\n            these are typically the same tensor; for encoder-decoder attention,\n            k and v are typically the same tensor. (We take advantage of these\n            identities for performance if they are present.) Regardless, q, k and v\n            must share a common embedding dimension; otherwise their shapes may vary.\n    Shape:\n        Inputs:\n        - q: :math:`(..., E)` where E is the embedding dimension\n        - k: :math:`(..., E)` where E is the embedding dimension\n        - v: :math:`(..., E)` where E is the embedding dimension\n        Output:\n        - in output list :math:`[q', k', v']`, each output tensor will have the\n            same shape as the corresponding input tensor.\n    \"\"\"\n    if k is v:\n        # self-attention\n        if q is k:\n            return self.in_proj(q).split((self.embed_dim, self.k_dim, self.v_dim), dim=-1)\n        # encoder-decoder attention\n        else:\n            w_q, w_kv = self.in_proj.weight.split([self.embed_dim, self.k_dim + self.v_dim])\n            b_q, b_kv = (\n                None\n                if self.in_proj.bias is None\n                else self.in_proj.bias.split([self.embed_dim, self.k_dim + self.v_dim])\n            )\n            return (F.linear(q, w_q, b_q),) + F.linear(k, w_kv, b_kv).split((self.k_dim, self.v_dim), dim=-1)\n    else:\n        w_q, w_k, w_v = self.in_proj.weight.split([self.embed_dim, self.k_dim, self.v_dim])\n        b_q, b_k, b_v = (\n            None if self.in_proj.bias is None else self.in_proj.bias.split([self.embed_dim, self.k_dim, self.v_dim])\n        )\n        return F.linear(q, w_q, b_q), F.linear(k, w_k, b_k), F.linear(v, w_v, b_v)\n</code></pre>","location":"zh/package/#danling.models.transformer.attention.self_attention.SelfAttention.in_projection"},{"title":"<code>TorchRunner</code>","text":"<p>         Bases: <code>BaseRunner</code></p> <p>Set up everything for running a job.</p> <p>Attributes:</p>    Name Type Description     <code>accelerator</code>  <code>Accelerator</code>     <code>accelerate</code>  <code>Dict[str, Any] = {}</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>class TorchRunner(BaseRunner):\n    r\"\"\"\n    Set up everything for running a job.\n\n    Attributes\n    ----------\n    accelerator: Accelerator\n    accelerate: Dict[str, Any] = {}\n    \"\"\"\n\n    # pylint: disable=R0902\n\n    accelerator: Accelerator\n    accelerate: Dict[str, Any] = {}\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        self.accelerator = Accelerator(**self.accelerate)\n        super().__init__(*args, **kwargs)\n\n    @on_main_process\n    def init_tensorboard(self, *args, **kwargs) -&gt; None:\n        r\"\"\"\n        Set up Tensoraoard SummaryWriter.\n        \"\"\"\n        from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n        if \"log_dir\" not in kwargs:\n            kwargs[\"log_dir\"] = self.dir\n\n        self.writer = SummaryWriter(*args, **kwargs)\n\n    def set_seed(self, bias: Optional[int] = None) -&gt; None:\n        r\"\"\"\n        Set up random seed.\n\n        Parameters\n        ----------\n        bias: Optional[int] = self.rank\n            Make the seed different for each processes.\n            This avoids same data augmentation are applied on every processes.\n            Set to `False` to disable this feature.\n        \"\"\"\n\n        if self.distributed:\n            # TODO: use broadcast_object instead.\n            self.seed = (\n                self.accelerator.gather(torch.tensor(self.seed).cuda()).unsqueeze(0).flatten()[0]  # pylint: disable=E1101\n            )\n        if bias is None:\n            bias = self.rank\n        seed = self.seed + bias if bias else self.seed\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        np.random.seed(seed)\n        random.seed(seed)\n\n    def set_deterministic(self) -&gt; None:\n        r\"\"\"\n        Set up deterministic.\n        \"\"\"\n\n        cudnn.benchmark = False\n        cudnn.deterministic = True\n        if torch.__version__ &gt;= \"1.8.0\":\n            torch.use_deterministic_algorithms(True)\n\n    def init_distributed(self) -&gt; None:\n        r\"\"\"\n        Set up distributed training.\n\n        Initialise process group and set up DDP variables.\n\n        .. deprecated:: 0.1.0\n            `init_distributed` is deprecated in favor of `Accelerator()`.\n        \"\"\"\n\n        # pylint: disable=W0201\n\n        dist.init_process_group(backend=\"nccl\")\n        self.rank = dist.get_rank()\n        self.world_size = dist.get_world_size()\n        self.local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n        torch.cuda.set_device(self.local_rank)\n        self.is_main_process = self.rank == 0\n        self.is_local_main_process = self.local_rank == 0\n\n    @catch\n    def save(self, obj: Any, f: File, main_process_only: bool = True) -&gt; File:  # pylint: disable=C0103\n        r\"\"\"\n        Save object to a path or file.\n        Returns\n        -------\n        File\n        \"\"\"\n\n        if main_process_only and self.is_main_process or not on_main_process:\n            self.accelerator.save(obj, f)\n        return f\n\n    @staticmethod\n    def load(f: File, *args, **kwargs) -&gt; Any:  # pylint: disable=C0103\n        r\"\"\"\n        Load object from a path or file.\n        Returns\n        -------\n        Any\n        \"\"\"\n\n        return torch.load(f, *args, **kwargs)  # type: ignore\n\n    @property\n    def world_size(self) -&gt; int:\n        r\"\"\"\n        Number of Processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return self.accelerator.num_processes\n\n    @property\n    def rank(self) -&gt; int:\n        r\"\"\"\n        Process index in all processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return self.accelerator.process_index\n\n    @property\n    def local_rank(self) -&gt; int:\n        r\"\"\"\n        Process index in local processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return self.accelerator.local_process_index\n</code></pre>","location":"zh/package/#danling.TorchRunner"},{"title":"<code>init_distributed()</code>","text":"<p>Set up distributed training.</p> <p>Initialise process group and set up DDP variables.</p> <p>.. deprecated:: 0.1.0     <code>init_distributed</code> is deprecated in favor of <code>Accelerator()</code>.</p>  Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def init_distributed(self) -&gt; None:\n    r\"\"\"\n    Set up distributed training.\n\n    Initialise process group and set up DDP variables.\n\n    .. deprecated:: 0.1.0\n        `init_distributed` is deprecated in favor of `Accelerator()`.\n    \"\"\"\n\n    # pylint: disable=W0201\n\n    dist.init_process_group(backend=\"nccl\")\n    self.rank = dist.get_rank()\n    self.world_size = dist.get_world_size()\n    self.local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n    torch.cuda.set_device(self.local_rank)\n    self.is_main_process = self.rank == 0\n    self.is_local_main_process = self.local_rank == 0\n</code></pre>","location":"zh/package/#danling.runner.torch_runner.TorchRunner.init_distributed"},{"title":"<code>init_tensorboard(*args, **kwargs)</code>","text":"<p>Set up Tensoraoard SummaryWriter.</p>  Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@on_main_process\ndef init_tensorboard(self, *args, **kwargs) -&gt; None:\n    r\"\"\"\n    Set up Tensoraoard SummaryWriter.\n    \"\"\"\n    from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n    if \"log_dir\" not in kwargs:\n        kwargs[\"log_dir\"] = self.dir\n\n    self.writer = SummaryWriter(*args, **kwargs)\n</code></pre>","location":"zh/package/#danling.runner.torch_runner.TorchRunner.init_tensorboard"},{"title":"<code>load(f, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Load object from a path or file.</p> <p>Returns:</p>    Type Description      <code>Any</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@staticmethod\ndef load(f: File, *args, **kwargs) -&gt; Any:  # pylint: disable=C0103\n    r\"\"\"\n    Load object from a path or file.\n    Returns\n    -------\n    Any\n    \"\"\"\n\n    return torch.load(f, *args, **kwargs)  # type: ignore\n</code></pre>","location":"zh/package/#danling.runner.torch_runner.TorchRunner.load"},{"title":"<code>local_rank()</code>  <code>property</code>","text":"<p>Process index in local processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@property\ndef local_rank(self) -&gt; int:\n    r\"\"\"\n    Process index in local processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return self.accelerator.local_process_index\n</code></pre>","location":"zh/package/#danling.runner.torch_runner.TorchRunner.local_rank"},{"title":"<code>rank()</code>  <code>property</code>","text":"<p>Process index in all processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@property\ndef rank(self) -&gt; int:\n    r\"\"\"\n    Process index in all processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return self.accelerator.process_index\n</code></pre>","location":"zh/package/#danling.runner.torch_runner.TorchRunner.rank"},{"title":"<code>save(obj, f, main_process_only=True)</code>","text":"<p>Save object to a path or file.</p> <p>Returns:</p>    Type Description      <code>File</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@catch\ndef save(self, obj: Any, f: File, main_process_only: bool = True) -&gt; File:  # pylint: disable=C0103\n    r\"\"\"\n    Save object to a path or file.\n    Returns\n    -------\n    File\n    \"\"\"\n\n    if main_process_only and self.is_main_process or not on_main_process:\n        self.accelerator.save(obj, f)\n    return f\n</code></pre>","location":"zh/package/#danling.runner.torch_runner.TorchRunner.save"},{"title":"<code>set_deterministic()</code>","text":"<p>Set up deterministic.</p>  Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def set_deterministic(self) -&gt; None:\n    r\"\"\"\n    Set up deterministic.\n    \"\"\"\n\n    cudnn.benchmark = False\n    cudnn.deterministic = True\n    if torch.__version__ &gt;= \"1.8.0\":\n        torch.use_deterministic_algorithms(True)\n</code></pre>","location":"zh/package/#danling.runner.torch_runner.TorchRunner.set_deterministic"},{"title":"<code>set_seed(bias=None)</code>","text":"<p>Set up random seed.</p> <p>Parameters:</p>    Name Type Description Default     <code>bias</code>  <code>Optional[int]</code>  <p>Make the seed different for each processes. This avoids same data augmentation are applied on every processes. Set to <code>False</code> to disable this feature.</p>  <code>None</code>      Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def set_seed(self, bias: Optional[int] = None) -&gt; None:\n    r\"\"\"\n    Set up random seed.\n\n    Parameters\n    ----------\n    bias: Optional[int] = self.rank\n        Make the seed different for each processes.\n        This avoids same data augmentation are applied on every processes.\n        Set to `False` to disable this feature.\n    \"\"\"\n\n    if self.distributed:\n        # TODO: use broadcast_object instead.\n        self.seed = (\n            self.accelerator.gather(torch.tensor(self.seed).cuda()).unsqueeze(0).flatten()[0]  # pylint: disable=E1101\n        )\n    if bias is None:\n        bias = self.rank\n    seed = self.seed + bias if bias else self.seed\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n</code></pre>","location":"zh/package/#danling.runner.torch_runner.TorchRunner.set_seed"},{"title":"<code>world_size()</code>  <code>property</code>","text":"<p>Number of Processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@property\ndef world_size(self) -&gt; int:\n    r\"\"\"\n    Number of Processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return self.accelerator.num_processes\n</code></pre>","location":"zh/package/#danling.runner.torch_runner.TorchRunner.world_size"},{"title":"<code>TransformerDecoder</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>TransformerDecoder is a stack of N decoder layers Args:     num_layers: the number of sub-decoder-layers in the decoder (required).     layer: the sub-decoder-layer in the decoder (default=TransformerDecoderLayer).     drop_layer: the drop layer rate (default=0.0). Examples::     &gt;&gt;&gt; transformer_decoder = dl.model.TransformerDecoder(num_layers=6)     &gt;&gt;&gt; src = torch.rand(10, 32, 512)     &gt;&gt;&gt; out = transformer_decoder(src)</p>  Source code in <code>danling/models/transformer/decoder.py</code> Python<pre><code>class TransformerDecoder(nn.Module):\n    r\"\"\"TransformerDecoder is a stack of N decoder layers\n    Args:\n        num_layers: the number of sub-decoder-layers in the decoder (required).\n        layer: the sub-decoder-layer in the decoder (default=TransformerDecoderLayer).\n        drop_layer: the drop layer rate (default=0.0).\n    Examples::\n        &gt;&gt;&gt; transformer_decoder = dl.model.TransformerDecoder(num_layers=6)\n        &gt;&gt;&gt; src = torch.rand(10, 32, 512)\n        &gt;&gt;&gt; out = transformer_decoder(src)\n    \"\"\"\n    __constants__ = [\"norm\"]\n\n    def __init__(self, layer: TransformerDecoderLayer, num_layers: int = 6, **kwargs: Optional[Dict[str, Any]]) -&gt; None:\n        super().__init__()\n        self.num_layers = num_layers\n        self.layers = nn.ModuleList([])\n        self.layers.extend([layer for _ in range(self.num_layers)])\n\n    def forward(\n        self,\n        tgt: Tensor,\n        mem: Tensor,\n        tgt_bias: Optional[Tensor] = None,\n        tgt_mask: Optional[Tensor] = None,\n        tgt_key_padding_mask: Optional[Tensor] = None,\n        mem_bias: Optional[Tensor] = None,\n        mem_mask: Optional[Tensor] = None,\n        mem_key_padding_mask: Optional[Tensor] = None,\n        need_weights: bool = False,\n        gradient_checkpoint: bool = False,\n    ) -&gt; Tensor:\n        r\"\"\"Pass the input through the decoder layers in turn.\n        Args:\n            src: the sequence to the decoder (required).\n            attn_mask: the mask for the src sequence (optional).\n            key_padding_mask: the mask for the src keys per batch (optional).\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"\n        output = tgt\n        # attn_weights is set to torch.empty(0, requires_grad=False) to avoid errors in DDP\n        attn_weights = [] if need_weights else torch.empty(0, requires_grad=False)\n\n        for layer in self.layers:\n            if gradient_checkpoint and self.training:\n                layer = partial(checkpoint, layer)\n                need_weights = torch.tensor(need_weights)\n            output, weights = layer(\n                output,\n                mem,\n                tgt_bias,\n                tgt_mask,\n                tgt_key_padding_mask,\n                mem_bias,\n                mem_mask,\n                mem_key_padding_mask,\n                need_weights,\n            )\n            if need_weights:\n                attn_weights.append(weights)  # type: ignore\n\n        if need_weights:\n            attn_weights = torch.stack(attn_weights).cpu().detach()  # type: ignore\n\n        return output, attn_weights\n</code></pre>","location":"zh/package/#danling.TransformerDecoder"},{"title":"<code>forward(tgt, mem, tgt_bias=None, tgt_mask=None, tgt_key_padding_mask=None, mem_bias=None, mem_mask=None, mem_key_padding_mask=None, need_weights=False, gradient_checkpoint=False)</code>","text":"<p>Pass the input through the decoder layers in turn. Args:     src: the sequence to the decoder (required).     attn_mask: the mask for the src sequence (optional).     key_padding_mask: the mask for the src keys per batch (optional). Shape:     see the docs in Transformer class.</p>  Source code in <code>danling/models/transformer/decoder.py</code> Python<pre><code>def forward(\n    self,\n    tgt: Tensor,\n    mem: Tensor,\n    tgt_bias: Optional[Tensor] = None,\n    tgt_mask: Optional[Tensor] = None,\n    tgt_key_padding_mask: Optional[Tensor] = None,\n    mem_bias: Optional[Tensor] = None,\n    mem_mask: Optional[Tensor] = None,\n    mem_key_padding_mask: Optional[Tensor] = None,\n    need_weights: bool = False,\n    gradient_checkpoint: bool = False,\n) -&gt; Tensor:\n    r\"\"\"Pass the input through the decoder layers in turn.\n    Args:\n        src: the sequence to the decoder (required).\n        attn_mask: the mask for the src sequence (optional).\n        key_padding_mask: the mask for the src keys per batch (optional).\n    Shape:\n        see the docs in Transformer class.\n    \"\"\"\n    output = tgt\n    # attn_weights is set to torch.empty(0, requires_grad=False) to avoid errors in DDP\n    attn_weights = [] if need_weights else torch.empty(0, requires_grad=False)\n\n    for layer in self.layers:\n        if gradient_checkpoint and self.training:\n            layer = partial(checkpoint, layer)\n            need_weights = torch.tensor(need_weights)\n        output, weights = layer(\n            output,\n            mem,\n            tgt_bias,\n            tgt_mask,\n            tgt_key_padding_mask,\n            mem_bias,\n            mem_mask,\n            mem_key_padding_mask,\n            need_weights,\n        )\n        if need_weights:\n            attn_weights.append(weights)  # type: ignore\n\n    if need_weights:\n        attn_weights = torch.stack(attn_weights).cpu().detach()  # type: ignore\n\n    return output, attn_weights\n</code></pre>","location":"zh/package/#danling.models.transformer.decoder.TransformerDecoder.forward"},{"title":"<code>TransformerDecoderLayer</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>TransformerDecoderLayer is made up of self-attn and feedforward network. This standard decoder layer is based on the paper \u201cAttention Is All You Need\u201d. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application. Args:     embed_dim: the number of expected features in the input (required).     num_heads: the number of heads in the multi head attention models (required).     ffn_dim: the dimension of the feedforward network model (default=embed_dim*4).     dropout: the dropout value (default=0.1).     activation: the activation function of intermediate layer, relu or gelu (default=relu).     layer_norm_eps: the eps value in layer normalization components (default=1e-5).     batch_first: If <code>True</code>, then the input and output tensors are provided         as (batch, seq, feature). Default: <code>False</code>.     norm_first: if <code>True</code>, layer norm is done prior to attention and feedforward         operations, respectivaly. Otherwise it\u2019s done after. Default: <code>False</code> (after). Examples::     &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(embed_dim=512, num_heads=8)     &gt;&gt;&gt; src = torch.rand(10, 32, 512)     &gt;&gt;&gt; out = decoder_layer(src) Alternatively, when <code>batch_first</code> is <code>True</code>:     &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(embed_dim=512, num_heads=8, batch_first=True)     &gt;&gt;&gt; src = torch.rand(32, 10, 512)     &gt;&gt;&gt; out = decoder_layer(src)</p>  Source code in <code>danling/models/transformer/decoder.py</code> Python<pre><code>class TransformerDecoderLayer(nn.Module):\n    r\"\"\"TransformerDecoderLayer is made up of self-attn and feedforward network.\n    This standard decoder layer is based on the paper \"Attention Is All You Need\".\n    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n    in a different way during application.\n    Args:\n        embed_dim: the number of expected features in the input (required).\n        num_heads: the number of heads in the multi head attention models (required).\n        ffn_dim: the dimension of the feedforward network model (default=embed_dim*4).\n        dropout: the dropout value (default=0.1).\n        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n        batch_first: If ``True``, then the input and output tensors are provided\n            as (batch, seq, feature). Default: ``False``.\n        norm_first: if ``True``, layer norm is done prior to attention and feedforward\n            operations, respectivaly. Otherwise it's done after. Default: ``False`` (after).\n    Examples::\n        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(embed_dim=512, num_heads=8)\n        &gt;&gt;&gt; src = torch.rand(10, 32, 512)\n        &gt;&gt;&gt; out = decoder_layer(src)\n    Alternatively, when ``batch_first`` is ``True``:\n        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(embed_dim=512, num_heads=8, batch_first=True)\n        &gt;&gt;&gt; src = torch.rand(32, 10, 512)\n        &gt;&gt;&gt; out = decoder_layer(src)\n    \"\"\"\n    __constants__ = [\"batch_first\", \"norm_first\"]\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        ffn_dim: Optional[int] = None,\n        dropout: float = 0.1,\n        attn_dropout: float = 0.1,\n        ffn_dropout: float = 0.1,\n        activation: str = \"GELU\",\n        layer_norm_eps: float = 1e-5,\n        scale_factor: float = 1.0,\n        bias: bool = True,\n        add_bias_kv: bool = False,\n        add_zero_attn: bool = False,\n        batch_first: bool = True,\n        norm_first: bool = False,\n        Attention: nn.Module = MultiHeadAttention,\n        FeedForwardNetwork: nn.Module = FullyConnectedNetwork,\n        **kwargs: Optional[Dict[str, Any]]\n    ) -&gt; None:\n        super().__init__()\n        if ffn_dim is None:\n            ffn_dim = embed_dim * 4\n        self.norm_first = norm_first\n        self.self_attn = Attention(\n            embed_dim,\n            num_heads,\n            attn_dropout=attn_dropout,\n            scale_factor=scale_factor,\n            bias=bias,\n            add_bias_kv=add_bias_kv,\n            add_zero_attn=add_zero_attn,\n            batch_first=batch_first,\n            **kwargs\n        )\n        self.cross_attn = Attention(\n            embed_dim,\n            num_heads,\n            attn_dropout=attn_dropout,\n            scale_factor=scale_factor,\n            bias=bias,\n            add_bias_kv=add_bias_kv,\n            add_zero_attn=add_zero_attn,\n            batch_first=batch_first,\n            **kwargs\n        )\n        self.norm1 = nn.LayerNorm(embed_dim, eps=layer_norm_eps)\n        self.ffn = FeedForwardNetwork(embed_dim, ffn_dim, activation, ffn_dropout, **kwargs)\n        self.norm2 = nn.LayerNorm(embed_dim, eps=layer_norm_eps)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(\n        self,\n        tgt: Tensor,\n        mem: Tensor,\n        tgt_bias: Optional[Tensor] = None,\n        tgt_mask: Optional[Tensor] = None,\n        tgt_key_padding_mask: Optional[Tensor] = None,\n        mem_bias: Optional[Tensor] = None,\n        mem_mask: Optional[Tensor] = None,\n        mem_key_padding_mask: Optional[Tensor] = None,\n        need_weights: bool = False,\n    ) -&gt; Tensor:\n        r\"\"\"Pass the input through the decoder layer.\n        Args:\n            src: the sequence to the decoder layer (required).\n            attn_mask: the mask for the src sequence (optional).\n            key_padding_mask: the mask for the src keys per batch (optional).\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"\n\n        if self.norm_first:\n            tgt = self.norm1(tgt)\n\n        self_attn, weights = self.self_attn(\n            tgt,\n            tgt,\n            tgt,\n            attn_bias=tgt_bias,\n            attn_mask=tgt_mask,\n            key_padding_mask=tgt_key_padding_mask,\n            need_weights=need_weights,\n        )\n        self_attn = tgt + self.dropout(self_attn)\n        cross_attn, weights = self.cross_attn(\n            self_attn,\n            mem,\n            mem,\n            attn_bias=mem_bias,\n            attn_mask=mem_mask,\n            key_padding_mask=mem_key_padding_mask,\n            need_weights=need_weights,\n        )\n        cross_attn = self_attn + self.dropout(cross_attn)\n        attn = self.norm1(cross_attn) if not self.norm_first else self.norm2(cross_attn)\n\n        ffn = self.ffn(attn)\n        ffn = attn + self.dropout(ffn)\n\n        if not self.norm_first:\n            ffn = self.norm2(ffn)\n\n        return ffn, weights\n</code></pre>","location":"zh/package/#danling.TransformerDecoderLayer"},{"title":"<code>forward(tgt, mem, tgt_bias=None, tgt_mask=None, tgt_key_padding_mask=None, mem_bias=None, mem_mask=None, mem_key_padding_mask=None, need_weights=False)</code>","text":"<p>Pass the input through the decoder layer. Args:     src: the sequence to the decoder layer (required).     attn_mask: the mask for the src sequence (optional).     key_padding_mask: the mask for the src keys per batch (optional). Shape:     see the docs in Transformer class.</p>  Source code in <code>danling/models/transformer/decoder.py</code> Python<pre><code>def forward(\n    self,\n    tgt: Tensor,\n    mem: Tensor,\n    tgt_bias: Optional[Tensor] = None,\n    tgt_mask: Optional[Tensor] = None,\n    tgt_key_padding_mask: Optional[Tensor] = None,\n    mem_bias: Optional[Tensor] = None,\n    mem_mask: Optional[Tensor] = None,\n    mem_key_padding_mask: Optional[Tensor] = None,\n    need_weights: bool = False,\n) -&gt; Tensor:\n    r\"\"\"Pass the input through the decoder layer.\n    Args:\n        src: the sequence to the decoder layer (required).\n        attn_mask: the mask for the src sequence (optional).\n        key_padding_mask: the mask for the src keys per batch (optional).\n    Shape:\n        see the docs in Transformer class.\n    \"\"\"\n\n    if self.norm_first:\n        tgt = self.norm1(tgt)\n\n    self_attn, weights = self.self_attn(\n        tgt,\n        tgt,\n        tgt,\n        attn_bias=tgt_bias,\n        attn_mask=tgt_mask,\n        key_padding_mask=tgt_key_padding_mask,\n        need_weights=need_weights,\n    )\n    self_attn = tgt + self.dropout(self_attn)\n    cross_attn, weights = self.cross_attn(\n        self_attn,\n        mem,\n        mem,\n        attn_bias=mem_bias,\n        attn_mask=mem_mask,\n        key_padding_mask=mem_key_padding_mask,\n        need_weights=need_weights,\n    )\n    cross_attn = self_attn + self.dropout(cross_attn)\n    attn = self.norm1(cross_attn) if not self.norm_first else self.norm2(cross_attn)\n\n    ffn = self.ffn(attn)\n    ffn = attn + self.dropout(ffn)\n\n    if not self.norm_first:\n        ffn = self.norm2(ffn)\n\n    return ffn, weights\n</code></pre>","location":"zh/package/#danling.models.transformer.decoder.TransformerDecoderLayer.forward"},{"title":"<code>TransformerEncoder</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>TransformerEncoder is a stack of N encoder layers Args:     num_layers: the number of sub-encoder-layers in the encoder (required).     layer: the sub-encoder-layer in the encoder (default=TransformerEncoderLayer).     drop_layer: the drop layer rate (default=0.0). Examples::     &gt;&gt;&gt; transformer_encoder = dl.model.TransformerEncoder(num_layers=6)     &gt;&gt;&gt; src = torch.rand(10, 32, 512)     &gt;&gt;&gt; out = transformer_encoder(src)</p>  Source code in <code>danling/models/transformer/encoder.py</code> Python<pre><code>class TransformerEncoder(nn.Module):\n    r\"\"\"TransformerEncoder is a stack of N encoder layers\n    Args:\n        num_layers: the number of sub-encoder-layers in the encoder (required).\n        layer: the sub-encoder-layer in the encoder (default=TransformerEncoderLayer).\n        drop_layer: the drop layer rate (default=0.0).\n    Examples::\n        &gt;&gt;&gt; transformer_encoder = dl.model.TransformerEncoder(num_layers=6)\n        &gt;&gt;&gt; src = torch.rand(10, 32, 512)\n        &gt;&gt;&gt; out = transformer_encoder(src)\n    \"\"\"\n    __constants__ = [\"norm\"]\n\n    def __init__(self, layer: TransformerEncoderLayer, num_layers: int = 6, **kwargs: Optional[Dict[str, Any]]) -&gt; None:\n        super().__init__()\n        self.num_layers = num_layers\n        self.layers = nn.ModuleList([])\n        self.layers.extend([layer for _ in range(self.num_layers)])\n\n    def forward(\n        self,\n        src: Tensor,\n        attn_bias: Optional[Tensor] = None,\n        attn_mask: Optional[Tensor] = None,\n        key_padding_mask: Optional[Tensor] = None,\n        need_weights: bool = False,\n        gradient_checkpoint: bool = False,\n    ) -&gt; Tensor:\n        r\"\"\"Pass the input through the encoder layers in turn.\n        Args:\n            src: the sequence to the encoder (required).\n            attn_mask: the mask for the src sequence (optional).\n            key_padding_mask: the mask for the src keys per batch (optional).\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"\n        output = src\n        # attn_weights is set to torch.empty(0, requires_grad=False) to avoid errors in DDP\n        attn_weights = [] if need_weights else torch.empty(0, requires_grad=False)\n\n        for layer in self.layers:\n            if gradient_checkpoint and self.training:\n                layer = partial(checkpoint, layer)\n                need_weights = torch.tensor(need_weights)\n            output, weights = layer(output, attn_bias, attn_mask, key_padding_mask, need_weights)\n            if need_weights:\n                attn_weights.append(weights)  # type: ignore\n\n        if need_weights:\n            attn_weights = torch.stack(attn_weights).cpu().detach()  # type: ignore\n\n        return output, attn_weights\n</code></pre>","location":"zh/package/#danling.TransformerEncoder"},{"title":"<code>forward(src, attn_bias=None, attn_mask=None, key_padding_mask=None, need_weights=False, gradient_checkpoint=False)</code>","text":"<p>Pass the input through the encoder layers in turn. Args:     src: the sequence to the encoder (required).     attn_mask: the mask for the src sequence (optional).     key_padding_mask: the mask for the src keys per batch (optional). Shape:     see the docs in Transformer class.</p>  Source code in <code>danling/models/transformer/encoder.py</code> Python<pre><code>def forward(\n    self,\n    src: Tensor,\n    attn_bias: Optional[Tensor] = None,\n    attn_mask: Optional[Tensor] = None,\n    key_padding_mask: Optional[Tensor] = None,\n    need_weights: bool = False,\n    gradient_checkpoint: bool = False,\n) -&gt; Tensor:\n    r\"\"\"Pass the input through the encoder layers in turn.\n    Args:\n        src: the sequence to the encoder (required).\n        attn_mask: the mask for the src sequence (optional).\n        key_padding_mask: the mask for the src keys per batch (optional).\n    Shape:\n        see the docs in Transformer class.\n    \"\"\"\n    output = src\n    # attn_weights is set to torch.empty(0, requires_grad=False) to avoid errors in DDP\n    attn_weights = [] if need_weights else torch.empty(0, requires_grad=False)\n\n    for layer in self.layers:\n        if gradient_checkpoint and self.training:\n            layer = partial(checkpoint, layer)\n            need_weights = torch.tensor(need_weights)\n        output, weights = layer(output, attn_bias, attn_mask, key_padding_mask, need_weights)\n        if need_weights:\n            attn_weights.append(weights)  # type: ignore\n\n    if need_weights:\n        attn_weights = torch.stack(attn_weights).cpu().detach()  # type: ignore\n\n    return output, attn_weights\n</code></pre>","location":"zh/package/#danling.models.transformer.encoder.TransformerEncoder.forward"},{"title":"<code>TransformerEncoderLayer</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>TransformerEncoderLayer is made up of self-attn and feedforward network. This standard encoder layer is based on the paper \u201cAttention Is All You Need\u201d. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application. Args:     embed_dim: the number of expected features in the input (required).     num_heads: the number of heads in the multi head attention models (required).     ffn_dim: the dimension of the feedforward network model (default=embed_dim*4).     dropout: the dropout value (default=0.1).     activation: the activation function of intermediate layer, relu or gelu (default=relu).     layer_norm_eps: the eps value in layer normalization components (default=1e-5).     batch_first: If <code>True</code>, then the input and output tensors are provided         as (batch, seq, feature). Default: <code>False</code>.     norm_first: if <code>True</code>, layer norm is done prior to attention and feedforward         operations, respectivaly. Otherwise it\u2019s done after. Default: <code>False</code> (after). Examples::     &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(embed_dim=512, num_heads=8)     &gt;&gt;&gt; src = torch.rand(10, 32, 512)     &gt;&gt;&gt; out = encoder_layer(src) Alternatively, when <code>batch_first</code> is <code>True</code>:     &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(embed_dim=512, num_heads=8, batch_first=True)     &gt;&gt;&gt; src = torch.rand(32, 10, 512)     &gt;&gt;&gt; out = encoder_layer(src)</p>  Source code in <code>danling/models/transformer/encoder.py</code> Python<pre><code>class TransformerEncoderLayer(nn.Module):\n    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n    in a different way during application.\n    Args:\n        embed_dim: the number of expected features in the input (required).\n        num_heads: the number of heads in the multi head attention models (required).\n        ffn_dim: the dimension of the feedforward network model (default=embed_dim*4).\n        dropout: the dropout value (default=0.1).\n        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n        batch_first: If ``True``, then the input and output tensors are provided\n            as (batch, seq, feature). Default: ``False``.\n        norm_first: if ``True``, layer norm is done prior to attention and feedforward\n            operations, respectivaly. Otherwise it's done after. Default: ``False`` (after).\n    Examples::\n        &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(embed_dim=512, num_heads=8)\n        &gt;&gt;&gt; src = torch.rand(10, 32, 512)\n        &gt;&gt;&gt; out = encoder_layer(src)\n    Alternatively, when ``batch_first`` is ``True``:\n        &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(embed_dim=512, num_heads=8, batch_first=True)\n        &gt;&gt;&gt; src = torch.rand(32, 10, 512)\n        &gt;&gt;&gt; out = encoder_layer(src)\n    \"\"\"\n    __constants__ = [\"batch_first\", \"norm_first\"]\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        ffn_dim: Optional[int] = None,\n        dropout: float = 0.1,\n        attn_dropout: float = 0.1,\n        ffn_dropout: float = 0.1,\n        activation: str = \"GELU\",\n        layer_norm_eps: float = 1e-5,\n        scale_factor: float = 1.0,\n        bias: bool = True,\n        add_bias_kv: bool = False,\n        add_zero_attn: bool = False,\n        batch_first: bool = True,\n        norm_first: bool = False,\n        Attention: nn.Module = MultiHeadAttention,\n        FeedForwardNetwork: nn.Module = FullyConnectedNetwork,\n        **kwargs: Optional[Dict[str, Any]]\n    ) -&gt; None:\n        super().__init__()\n        if ffn_dim is None:\n            ffn_dim = embed_dim * 4\n        self.norm_first = norm_first\n        self.attn = Attention(\n            embed_dim,\n            num_heads,\n            attn_dropout=attn_dropout,\n            scale_factor=scale_factor,\n            bias=bias,\n            add_bias_kv=add_bias_kv,\n            add_zero_attn=add_zero_attn,\n            batch_first=batch_first,\n            **kwargs\n        )\n        self.norm1 = nn.LayerNorm(embed_dim, eps=layer_norm_eps)\n        self.ffn = FeedForwardNetwork(embed_dim, ffn_dim, activation, ffn_dropout, **kwargs)\n        self.norm2 = nn.LayerNorm(embed_dim, eps=layer_norm_eps)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(\n        self,\n        src: Tensor,\n        attn_bias: Optional[Tensor] = None,\n        attn_mask: Optional[Tensor] = None,\n        key_padding_mask: Optional[Tensor] = None,\n        need_weights: bool = False,\n    ) -&gt; Tensor:\n        r\"\"\"Pass the input through the encoder layer.\n        Args:\n            src: the sequence to the encoder layer (required).\n            attn_mask: the mask for the src sequence (optional).\n            key_padding_mask: the mask for the src keys per batch (optional).\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"\n\n        if self.norm_first:\n            src = self.norm1(src)\n\n        attn, weights = self.attn(\n            src,\n            src,\n            src,\n            attn_bias=attn_bias,\n            attn_mask=attn_mask,\n            key_padding_mask=key_padding_mask,\n            need_weights=need_weights,\n        )\n        attn = src + self.dropout(attn)\n        attn = self.norm1(attn) if not self.norm_first else self.norm2(attn)\n\n        ffn = self.ffn(attn)\n        ffn = attn + self.dropout(ffn)\n\n        if not self.norm_first:\n            ffn = self.norm2(ffn)\n\n        return ffn, weights\n</code></pre>","location":"zh/package/#danling.TransformerEncoderLayer"},{"title":"<code>forward(src, attn_bias=None, attn_mask=None, key_padding_mask=None, need_weights=False)</code>","text":"<p>Pass the input through the encoder layer. Args:     src: the sequence to the encoder layer (required).     attn_mask: the mask for the src sequence (optional).     key_padding_mask: the mask for the src keys per batch (optional). Shape:     see the docs in Transformer class.</p>  Source code in <code>danling/models/transformer/encoder.py</code> Python<pre><code>def forward(\n    self,\n    src: Tensor,\n    attn_bias: Optional[Tensor] = None,\n    attn_mask: Optional[Tensor] = None,\n    key_padding_mask: Optional[Tensor] = None,\n    need_weights: bool = False,\n) -&gt; Tensor:\n    r\"\"\"Pass the input through the encoder layer.\n    Args:\n        src: the sequence to the encoder layer (required).\n        attn_mask: the mask for the src sequence (optional).\n        key_padding_mask: the mask for the src keys per batch (optional).\n    Shape:\n        see the docs in Transformer class.\n    \"\"\"\n\n    if self.norm_first:\n        src = self.norm1(src)\n\n    attn, weights = self.attn(\n        src,\n        src,\n        src,\n        attn_bias=attn_bias,\n        attn_mask=attn_mask,\n        key_padding_mask=key_padding_mask,\n        need_weights=need_weights,\n    )\n    attn = src + self.dropout(attn)\n    attn = self.norm1(attn) if not self.norm_first else self.norm2(attn)\n\n    ffn = self.ffn(attn)\n    ffn = attn + self.dropout(ffn)\n\n    if not self.norm_first:\n        ffn = self.norm2(ffn)\n\n    return ffn, weights\n</code></pre>","location":"zh/package/#danling.models.transformer.encoder.TransformerEncoderLayer.forward"},{"title":"<code>UnitedPositionEmbedding</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>United Position Embedding See <code>Rethinking Positional Encoding in Language Pre-training &lt;https://arxiv.org/abs/2006.15595&gt;</code>_ .. math::     \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O where :math:<code>head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)</code>. Args:     embed_dim: total dimension of the model.     num_heads: parallel attention heads.     dropout: a Dropout layer on attn_output_weights. Default: 0.0.     bias: add bias as module parameter. Default: True.     add_bias_kv: add bias to the key and value sequences at dim=0.     add_zero_attn: add a new batch of zeros to the key and                    value sequences at dim=1.     k_dim: total number of features in key. Default: None.     v_dim: total number of features in value. Default: None.     batch_first: If <code>True</code>, then the input and output tensors are provided         as (batch, seq, feature). Default: <code>False</code> (seq, batch, feature). Note that if :attr:<code>k_dim</code> and :attr:<code>v_dim</code> are None, they will be set to :attr:<code>embed_dim</code> such that query, key, and value have the same number of features. Examples::     &gt;&gt;&gt; multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)     &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)</p>  Source code in <code>danling/models/transformer/pos_embed/pos_embed.py</code> Python<pre><code>class UnitedPositionEmbedding(nn.Module):\n    r\"\"\"United Position Embedding\n    See `Rethinking Positional Encoding in Language Pre-training &lt;https://arxiv.org/abs/2006.15595&gt;`_\n    .. math::\n        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n    where :math:`head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n    Args:\n        embed_dim: total dimension of the model.\n        num_heads: parallel attention heads.\n        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n        bias: add bias as module parameter. Default: True.\n        add_bias_kv: add bias to the key and value sequences at dim=0.\n        add_zero_attn: add a new batch of zeros to the key and\n                       value sequences at dim=1.\n        k_dim: total number of features in key. Default: None.\n        v_dim: total number of features in value. Default: None.\n        batch_first: If ``True``, then the input and output tensors are provided\n            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n    Note that if :attr:`k_dim` and :attr:`v_dim` are None, they will be set\n    to :attr:`embed_dim` such that query, key, and value have the same\n    number of features.\n    Examples::\n        &gt;&gt;&gt; multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n        &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        seq_len_max: int,\n        rel_pos_embed: bool = False,\n        rel_pos_embed_buckets: int = 32,\n        rel_pos_embed_max: int = 128,\n        pos_embed_dropout: float = 0.0,\n        pos_scale_factor: int = 1,\n        has_cls_token: bool = True,\n    ) -&gt; None:\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.seq_len_max = seq_len_max\n        self.has_cls_token = has_cls_token\n        self.dropout = nn.Dropout(pos_embed_dropout)\n        if self.has_cls_token:\n            # make room for [CLS]-to-others and others-to-[CLS]\n            self.seq_len_max += 2\n        self.abs_pos_embed = nn.Parameter(torch.randn(self.seq_len_max, self.embed_dim))\n        self.ln = nn.LayerNorm(self.embed_dim)\n        self.in_proj = nn.Linear(self.embed_dim, self.embed_dim * 2)\n        self.scaling = (embed_dim / num_heads * pos_scale_factor) ** -0.5\n\n        self.rel_pos_embed = None\n        if rel_pos_embed:\n            assert rel_pos_embed_buckets % 2 == 0\n            self.rel_pos_embed_buckets = rel_pos_embed_buckets\n            self.rel_pos_embed_max = rel_pos_embed_max\n            self.rel_pos_embed = nn.Embedding(self.rel_pos_embed_buckets + 1, self.num_heads)\n            self.rel_pos_embed_bucket = relative_position_bucket(\n                seq_len_max=self.seq_len_max,\n                num_buckets=self.rel_pos_embed_buckets,\n                max_distance=self.rel_pos_embed_max,\n            )\n\n    def forward(self, src: Tensor, cls_token_index: Optional[Tensor] = None) -&gt; Tensor:\n        B, N, C = src.shape\n        # 0 is for others-to-[CLS] 1 is for [CLS]-to-others\n        # Assume the input is ordered. If your input token is permuted, you may need to update this accordingly\n        if self.has_cls_token:\n            # only plus 1 here since because [CLS] already plused 1\n            N += 1\n        weight = self.ln(self.abs_pos_embed[:N, :])\n        q, k = self.in_proj(weight).reshape(N, 2, self.num_heads, C // self.num_heads).permute(1, 2, 0, 3)\n        q = q * self.scaling\n        pos_embed = torch.bmm(q, k.transpose(1, 2))\n        if self.has_cls_token:\n            # p_0 \\dot p_0 is [CLS]-to-others\n            cls_2_others = pos_embed[:, 0, 0]\n            # p_1 \\dot p_1 is others-to-[CLS]\n            others_2_cls = pos_embed[:, 1, 1]\n            # offset\n            pos_embed = pos_embed[:, 1:, 1:]\n            # if [CLS] is not the first token\n            if cls_token_index is not None:\n                pos_embed = pos_embed.repeat(B, 1, 1, 1)\n                pos_embed[torch.arange(B), :, cls_token_index, :] = cls_2_others.expand(B, -1).unsqueeze(-1)\n                pos_embed[torch.arange(B), :, :, cls_token_index] = others_2_cls.expand(B, -1).unsqueeze(-1)\n            else:\n                pos_embed[:, 0, :] = cls_2_others.unsqueeze(-1)\n                pos_embed[:, :, 0] = others_2_cls.unsqueeze(-1)\n            N -= 1\n        rel_pos_embed = torch.zeros_like(pos_embed)\n        if self.rel_pos_embed:\n            rel_pos_embed_bucket = self.rel_pos_embed_bucket[:N, :N]\n            if self.has_cls_token:\n                if cls_token_index is not None:\n                    rel_pos_embed_bucket = rel_pos_embed_bucket.repeat(B, 1, 1)\n                    rel_pos_embed_bucket[torch.arange(B), cls_token_index, :] = self.rel_pos_embed_buckets // 2\n                    rel_pos_embed_bucket[torch.arange(B), :, cls_token_index] = self.rel_pos_embed_buckets\n                    rel_pos_embed = self.rel_pos_embed(rel_pos_embed_bucket).permute(0, 3, 1, 2)\n                else:\n                    rel_pos_embed_bucket[0, :] = self.rel_pos_embed_buckets // 2\n                    rel_pos_embed_bucket[:, 0] = self.rel_pos_embed_buckets\n                    rel_pos_embed = self.rel_pos_embed(rel_pos_embed_bucket).permute(2, 0, 1)\n            else:\n                rel_pos_embed = self.rel_pos_embed(rel_pos_embed_bucket).permute(2, 0, 1)\n            pos_embed += rel_pos_embed\n\n        pos_embed = (\n            pos_embed.view(-1, *pos_embed.shape[2:]) if cls_token_index is not None else pos_embed.repeat(B, 1, 1)\n        )\n        return self.dropout(pos_embed)\n</code></pre>","location":"zh/package/#danling.UnitedPositionEmbedding"},{"title":"<code>catch(error=Exception, exclude=None, print_args=False)</code>","text":"<p>Decorator to catch <code>error</code> except for <code>exclude</code>. Detailed traceback will be printed to <code>stdout</code>.</p> <p><code>catch</code> is extremely useful for unfatal errors. For example, <code>Runner</code> by de</p> <p>Parameters:</p>    Name Type Description Default     <code>error</code>  <code>Exception</code>    <code>Exception</code>    <code>exclude</code>  <code>Optional[Exception]</code>    <code>None</code>    <code>print_args</code>  <code>bool</code>  <p>Whether to print the arguments passed to the function.</p>  <code>False</code>      Source code in <code>danling/utils/decorator.py</code> Python<pre><code>@flexible_decorator\ndef catch(error: Exception = Exception, exclude: Optional[Exception] = None, print_args: bool = False):\n    \"\"\"\n    Decorator to catch `error` except for `exclude`.\n    Detailed traceback will be printed to `stdout`.\n\n    `catch` is extremely useful for unfatal errors.\n    For example, `Runner` by de\n\n    Parameters\n    ----------\n    error: Exception\n    exclude: Optional[Exception] = None\n    print_args: bool = False\n        Whether to print the arguments passed to the function.\n    \"\"\"\n\n    def decorator(func, error: Exception = Exception, exclude: Optional[Exception] = None, print_args: bool = False):\n        @wraps(func)\n        def wrapper(*args, **kwargs):  # pylint: disable=R1710\n            try:\n                return func(*args, **kwargs)\n            except error as exc:  # pylint: disable=W0703\n                if exclude is not None and isinstance(exc, exclude):\n                    raise exc\n                message = format_exc()\n                message += f\"\\nencoutered when calling {func}\"\n                if print_args:\n                    message += (f\"with args {args} and kwargs {kwargs}\",)\n                print(message, file=stderr, force=True)\n\n        return wrapper\n\n    return lambda func: decorator(func, error, exclude, print_args)\n</code></pre>","location":"zh/package/#danling.catch"},{"title":"<code>load(path, *args, **kwargs)</code>","text":"<p>Load any file with supported extensions.</p>  Source code in <code>danling/utils/io.py</code> Python<pre><code>def load(path: str, *args: List[Any], **kwargs: Dict[str, Any]) -&gt; Any:\n    \"\"\"\n    Load any file with supported extensions.\n    \"\"\"\n    if not os.path.isfile(path):\n        raise ValueError(f\"Trying to load {path} but it is not a file.\")\n    extension = os.path.splitext(path)[-1].lower()[1:]\n    if extension in PYTORCH:\n        if torch_load is None:\n            raise ImportError(f\"Trying to load {path} but torch is not installed.\")\n        return torch_load(path)\n    if extension in NUMPY:\n        if numpy_load is None:\n            raise ImportError(f\"Trying to load {path} but numpy is not installed.\")\n        return numpy_load(path, allow_pickle=True)\n    if extension in CSV:\n        if read_csv is None:\n            raise ImportError(f\"Trying to load {path} but pandas is not installed.\")\n        return read_csv(path, *args, **kwargs)\n    if extension in JSON:\n        with open(path, \"r\") as fp:  # pylint: disable=W1514, C0103\n            return json_load(fp, *args, **kwargs)  # type: ignore\n    if extension in PICKLE:\n        with open(path, \"rb\") as fp:  # pylint: disable=C0103\n            return pickle_load(fp, *args, **kwargs)  # type: ignore\n    raise ValueError(f\"Tying to load {path} with unsupported extension={extension}\")\n</code></pre>","location":"zh/package/#danling.load"},{"title":"Registry","text":"<p>         Bases: <code>NestedDict</code></p> <p><code>Registry</code> for components.</p>","location":"zh/registry/"},{"title":"Notes","text":"<p><code>Registry</code> inherits from <code>NestedDict</code>.</p> <p>Therefore, <code>Registry</code> comes in a nested structure by nature. You could create a sub-registry by simply calling <code>registry.sub_registry = Registry</code>, and access through <code>registry.sub_registry.register()</code>.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; registry = Registry(\"test\")\n&gt;&gt;&gt; @registry.register\n... @registry.register(\"Module1\")\n... class Module:\n...     def __init__(self, a, b):\n...         self.a = a\n...         self.b = b\n&gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n&gt;&gt;&gt; registry\nRegistry(\n  (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n  (Module): &lt;class 'danling.registry.registry.Module'&gt;\n  (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n)\n&gt;&gt;&gt; registry.lookup(\"Module\")\n&lt;class 'danling.registry.registry.Module'&gt;\n&gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n&gt;&gt;&gt; # registry.register(Module)\n&gt;&gt;&gt; module = registry.build(config[\"module\"])\n&gt;&gt;&gt; type(module)\n&lt;class 'danling.registry.registry.Module'&gt;\n&gt;&gt;&gt; module.a\n1\n&gt;&gt;&gt; module.b\n2\n</code></pre>  Source code in <code>danling/registry/registry.py</code> Python<pre><code>class Registry(NestedDict):\n    \"\"\"\n    `Registry` for components.\n\n    Notes\n    -----\n\n    `Registry` inherits from [`NestedDict`](https://chanfig.danling.org/nested_dict/).\n\n    Therefore, `Registry` comes in a nested structure by nature.\n    You could create a sub-registry by simply calling `registry.sub_registry = Registry`,\n    and access through `registry.sub_registry.register()`.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; registry = Registry(\"test\")\n    &gt;&gt;&gt; @registry.register\n    ... @registry.register(\"Module1\")\n    ... class Module:\n    ...     def __init__(self, a, b):\n    ...         self.a = a\n    ...         self.b = b\n    &gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n    &gt;&gt;&gt; registry\n    Registry(\n      (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n      (Module): &lt;class 'danling.registry.registry.Module'&gt;\n      (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n    )\n    &gt;&gt;&gt; registry.lookup(\"Module\")\n    &lt;class 'danling.registry.registry.Module'&gt;\n    &gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n    &gt;&gt;&gt; # registry.register(Module)\n    &gt;&gt;&gt; module = registry.build(config[\"module\"])\n    &gt;&gt;&gt; type(module)\n    &lt;class 'danling.registry.registry.Module'&gt;\n    &gt;&gt;&gt; module.a\n    1\n    &gt;&gt;&gt; module.b\n    2\n\n    ```\n    \"\"\"\n\n    override: bool = False\n\n    def __init__(self, override: bool = False):\n        super().__init__()\n        self.setattr(\"override\", override)\n\n    def register(self, component: Optional[Callable] = None, name: Optional[str] = None) -&gt; Callable:\n        r\"\"\"\n        Register a new component\n\n        Parameters\n        ----------\n        component: Optional[Callable] = None\n            The component to register.\n        name: Optional[str] = component.__name__\n            The name of the component.\n\n        Returns\n        -------\n        component: Callable\n            The registered component.\n\n        Raises\n        ------\n        ValueError\n            If the component with the same name already exists and `Registry.override=False`.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; registry = Registry(\"test\")\n        &gt;&gt;&gt; @registry.register\n        ... @registry.register(\"Module1\")\n        ... class Module:\n        ...     def __init__(self, a, b):\n        ...         self.a = a\n        ...         self.b = b\n        &gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n        &gt;&gt;&gt; registry\n        Registry(\n          (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n          (Module): &lt;class 'danling.registry.registry.Module'&gt;\n          (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n        )\n\n        ```\n        \"\"\"\n\n        if isinstance(name, str) and name in self and not self.override:\n            raise ValueError(f\"Component with name {name} already exists\")\n\n        # Registry.register()\n        if name is not None:\n            self.set(name, component)\n\n        # @Registry.register()\n        @wraps(self.register)\n        def register(component, name=None):\n            if name is None:\n                name = component.__name__\n            self.set(name, component)\n            return component\n\n        # @Registry.register\n        if callable(component) and name is None:\n            return register(component)\n\n        return lambda x: register(x, component)\n\n    def lookup(self, name: str) -&gt; Any:\n        r\"\"\"\n        Lookup for a component.\n\n        Parameters\n        ----------\n        name: str\n            The name of the component.\n\n        Returns\n        -------\n        value: Any\n\n        Raises\n        ------\n        KeyError\n            If the component is not registered.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; registry = Registry(\"test\")\n        &gt;&gt;&gt; @registry.register\n        ... class Module:\n        ...     def __init__(self, a, b):\n        ...         self.a = a\n        ...         self.b = b\n        &gt;&gt;&gt; registry.lookup(\"Module\")\n        &lt;class 'danling.registry.registry.Module'&gt;\n\n        ```\n        \"\"\"\n\n        return self.get(name)\n\n    def build(self, name: str, *args, **kwargs):\n        r\"\"\"\n        Build a component.\n\n        Parameters\n        ----------\n        name: str\n            The name of the component.\n        *args\n            The arguments to pass to the component.\n        **kwargs\n            The keyword arguments to pass to the component.\n\n        Returns\n        -------\n        component: Callable\n\n        Raises\n        ------\n        KeyError\n            If the component is not registered.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; registry = Registry(\"test\")\n        &gt;&gt;&gt; @registry.register\n        ... class Module:\n        ...     def __init__(self, a, b):\n        ...         self.a = a\n        ...         self.b = b\n        &gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n        &gt;&gt;&gt; # registry.register(Module)\n        &gt;&gt;&gt; module = registry.build(config[\"module\"])\n        &gt;&gt;&gt; type(module)\n        &lt;class 'danling.registry.registry.Module'&gt;\n        &gt;&gt;&gt; module.a\n        1\n        &gt;&gt;&gt; module.b\n        2\n\n        ```\n        \"\"\"\n\n        if isinstance(name, Mapping) and not args and not kwargs:\n            name, kwargs = name.pop(\"name\"), name  # type: ignore\n        return self.get(name)(*args, **kwargs)\n\n    def __wrapped__(self):\n        pass\n</code></pre>","location":"zh/registry/#danling.registry.Registry--notes"},{"title":"<code>register(component=None, name=None)</code>","text":"<p>Register a new component</p> <p>Parameters:</p>    Name Type Description Default     <code>component</code>  <code>Optional[Callable]</code>  <p>The component to register.</p>  <code>None</code>    <code>name</code>  <code>Optional[str]</code>  <p>The name of the component.</p>  <code>None</code>     <p>Returns:</p>    Name Type Description     <code>component</code>  <code>Callable</code>  <p>The registered component.</p>    <p>Raises:</p>    Type Description      <code>ValueError</code>  <p>If the component with the same name already exists and <code>Registry.override=False</code>.</p>    <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; registry = Registry(\"test\")\n&gt;&gt;&gt; @registry.register\n... @registry.register(\"Module1\")\n... class Module:\n...     def __init__(self, a, b):\n...         self.a = a\n...         self.b = b\n&gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n&gt;&gt;&gt; registry\nRegistry(\n  (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n  (Module): &lt;class 'danling.registry.registry.Module'&gt;\n  (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n)\n</code></pre>  Source code in <code>danling/registry/registry.py</code> Python<pre><code>def register(self, component: Optional[Callable] = None, name: Optional[str] = None) -&gt; Callable:\n    r\"\"\"\n    Register a new component\n\n    Parameters\n    ----------\n    component: Optional[Callable] = None\n        The component to register.\n    name: Optional[str] = component.__name__\n        The name of the component.\n\n    Returns\n    -------\n    component: Callable\n        The registered component.\n\n    Raises\n    ------\n    ValueError\n        If the component with the same name already exists and `Registry.override=False`.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; registry = Registry(\"test\")\n    &gt;&gt;&gt; @registry.register\n    ... @registry.register(\"Module1\")\n    ... class Module:\n    ...     def __init__(self, a, b):\n    ...         self.a = a\n    ...         self.b = b\n    &gt;&gt;&gt; module = registry.register(Module, \"Module2\")\n    &gt;&gt;&gt; registry\n    Registry(\n      (Module1): &lt;class 'danling.registry.registry.Module'&gt;\n      (Module): &lt;class 'danling.registry.registry.Module'&gt;\n      (Module2): &lt;class 'danling.registry.registry.Module'&gt;\n    )\n\n    ```\n    \"\"\"\n\n    if isinstance(name, str) and name in self and not self.override:\n        raise ValueError(f\"Component with name {name} already exists\")\n\n    # Registry.register()\n    if name is not None:\n        self.set(name, component)\n\n    # @Registry.register()\n    @wraps(self.register)\n    def register(component, name=None):\n        if name is None:\n            name = component.__name__\n        self.set(name, component)\n        return component\n\n    # @Registry.register\n    if callable(component) and name is None:\n        return register(component)\n\n    return lambda x: register(x, component)\n</code></pre>","location":"zh/registry/#danling.registry.registry.Registry.register"},{"title":"<code>lookup(name)</code>","text":"<p>Lookup for a component.</p> <p>Parameters:</p>    Name Type Description Default     <code>name</code>  <code>str</code>  <p>The name of the component.</p>  required     <p>Returns:</p>    Name Type Description     <code>value</code>  <code>Any</code>      <p>Raises:</p>    Type Description      <code>KeyError</code>  <p>If the component is not registered.</p>    <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; registry = Registry(\"test\")\n&gt;&gt;&gt; @registry.register\n... class Module:\n...     def __init__(self, a, b):\n...         self.a = a\n...         self.b = b\n&gt;&gt;&gt; registry.lookup(\"Module\")\n&lt;class 'danling.registry.registry.Module'&gt;\n</code></pre>  Source code in <code>danling/registry/registry.py</code> Python<pre><code>def lookup(self, name: str) -&gt; Any:\n    r\"\"\"\n    Lookup for a component.\n\n    Parameters\n    ----------\n    name: str\n        The name of the component.\n\n    Returns\n    -------\n    value: Any\n\n    Raises\n    ------\n    KeyError\n        If the component is not registered.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; registry = Registry(\"test\")\n    &gt;&gt;&gt; @registry.register\n    ... class Module:\n    ...     def __init__(self, a, b):\n    ...         self.a = a\n    ...         self.b = b\n    &gt;&gt;&gt; registry.lookup(\"Module\")\n    &lt;class 'danling.registry.registry.Module'&gt;\n\n    ```\n    \"\"\"\n\n    return self.get(name)\n</code></pre>","location":"zh/registry/#danling.registry.registry.Registry.lookup"},{"title":"<code>build(name, *args, **kwargs)</code>","text":"<p>Build a component.</p> <p>Parameters:</p>    Name Type Description Default     <code>name</code>  <code>str</code>  <p>The name of the component.</p>  required    <code>*args</code>   <p>The arguments to pass to the component.</p>  <code>()</code>    <code>**kwargs</code>   <p>The keyword arguments to pass to the component.</p>  <code>{}</code>     <p>Returns:</p>    Name Type Description     <code>component</code>  <code>Callable</code>      <p>Raises:</p>    Type Description      <code>KeyError</code>  <p>If the component is not registered.</p>    <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; registry = Registry(\"test\")\n&gt;&gt;&gt; @registry.register\n... class Module:\n...     def __init__(self, a, b):\n...         self.a = a\n...         self.b = b\n&gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n&gt;&gt;&gt; # registry.register(Module)\n&gt;&gt;&gt; module = registry.build(config[\"module\"])\n&gt;&gt;&gt; type(module)\n&lt;class 'danling.registry.registry.Module'&gt;\n&gt;&gt;&gt; module.a\n1\n&gt;&gt;&gt; module.b\n2\n</code></pre>  Source code in <code>danling/registry/registry.py</code> Python<pre><code>def build(self, name: str, *args, **kwargs):\n    r\"\"\"\n    Build a component.\n\n    Parameters\n    ----------\n    name: str\n        The name of the component.\n    *args\n        The arguments to pass to the component.\n    **kwargs\n        The keyword arguments to pass to the component.\n\n    Returns\n    -------\n    component: Callable\n\n    Raises\n    ------\n    KeyError\n        If the component is not registered.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; registry = Registry(\"test\")\n    &gt;&gt;&gt; @registry.register\n    ... class Module:\n    ...     def __init__(self, a, b):\n    ...         self.a = a\n    ...         self.b = b\n    &gt;&gt;&gt; config = {\"module\": {\"name\": \"Module\", \"a\": 1, \"b\": 2}}\n    &gt;&gt;&gt; # registry.register(Module)\n    &gt;&gt;&gt; module = registry.build(config[\"module\"])\n    &gt;&gt;&gt; type(module)\n    &lt;class 'danling.registry.registry.Module'&gt;\n    &gt;&gt;&gt; module.a\n    1\n    &gt;&gt;&gt; module.b\n    2\n\n    ```\n    \"\"\"\n\n    if isinstance(name, Mapping) and not args and not kwargs:\n        name, kwargs = name.pop(\"name\"), name  # type: ignore\n    return self.get(name)(*args, **kwargs)\n</code></pre>","location":"zh/registry/#danling.registry.registry.Registry.build"},{"title":"DanLing","text":"","location":"zh/blog/"},{"title":"AverageMeter","text":"<p>Computes and stores the average and current value.</p> <p>Attributes:</p>    Name Type Description     <code>val</code>  <code>int</code>  <p>Current value.</p>   <code>avg</code>  <code>float</code>  <p>Average value.</p>   <code>sum</code>  <code>float</code>  <p>Sum of values.</p>   <code>count</code>  <code>int</code>  <p>Number of values.</p>    <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.update(0.9)\n&gt;&gt;&gt; meter.val\n0.9\n&gt;&gt;&gt; meter.avg\n0.8\n&gt;&gt;&gt; meter.sum\n1.6\n&gt;&gt;&gt; meter.count\n2\n&gt;&gt;&gt; meter.reset()\n&gt;&gt;&gt; meter.val\n0\n&gt;&gt;&gt; meter.avg\n0\n</code></pre>  Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>class AverageMeter:\n    r\"\"\"\n    Computes and stores the average and current value.\n\n    Attributes\n    ----------\n    val: int\n        Current value.\n    avg: float\n        Average value.\n    sum: float\n        Sum of values.\n    count: int\n        Number of values.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; meter = AverageMeter()\n    &gt;&gt;&gt; meter.update(0.7)\n    &gt;&gt;&gt; meter.val\n    0.7\n    &gt;&gt;&gt; meter.avg\n    0.7\n    &gt;&gt;&gt; meter.update(0.9)\n    &gt;&gt;&gt; meter.val\n    0.9\n    &gt;&gt;&gt; meter.avg\n    0.8\n    &gt;&gt;&gt; meter.sum\n    1.6\n    &gt;&gt;&gt; meter.count\n    2\n    &gt;&gt;&gt; meter.reset()\n    &gt;&gt;&gt; meter.val\n    0\n    &gt;&gt;&gt; meter.avg\n    0\n\n    ```\n    \"\"\"\n\n    val: int = 0\n    avg: float = 0\n    sum: int = 0\n    count: int = 0\n\n    def __init__(self) -&gt; None:\n        self.reset()\n\n    def reset(self) -&gt; None:\n        r\"\"\"\n        Resets the meter.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; meter = AverageMeter()\n        &gt;&gt;&gt; meter.update(0.7)\n        &gt;&gt;&gt; meter.val\n        0.7\n        &gt;&gt;&gt; meter.avg\n        0.7\n        &gt;&gt;&gt; meter.reset()\n        &gt;&gt;&gt; meter.val\n        0\n        &gt;&gt;&gt; meter.avg\n        0\n\n        ```\n        \"\"\"\n\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n: int = 1) -&gt; None:\n        r\"\"\"\n        Updates the average and current value in the meter.\n\n        Parameters\n        ----------\n        val: int\n            Value to be added to the average.\n        n: int = 1\n            Number of values to be added.\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; meter = AverageMeter()\n        &gt;&gt;&gt; meter.update(0.7)\n        &gt;&gt;&gt; meter.val\n        0.7\n        &gt;&gt;&gt; meter.avg\n        0.7\n        &gt;&gt;&gt; meter.update(0.9)\n        &gt;&gt;&gt; meter.val\n        0.9\n        &gt;&gt;&gt; meter.avg\n        0.8\n        &gt;&gt;&gt; meter.sum\n        1.6\n        &gt;&gt;&gt; meter.count\n        2\n\n        ```\n        \"\"\"\n\n        # pylint: disable=C0103\n\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n</code></pre>","location":"zh/metrics/average_meter/"},{"title":"<code>reset()</code>","text":"<p>Resets the meter.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.reset()\n&gt;&gt;&gt; meter.val\n0\n&gt;&gt;&gt; meter.avg\n0\n</code></pre>  Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>def reset(self) -&gt; None:\n    r\"\"\"\n    Resets the meter.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; meter = AverageMeter()\n    &gt;&gt;&gt; meter.update(0.7)\n    &gt;&gt;&gt; meter.val\n    0.7\n    &gt;&gt;&gt; meter.avg\n    0.7\n    &gt;&gt;&gt; meter.reset()\n    &gt;&gt;&gt; meter.val\n    0\n    &gt;&gt;&gt; meter.avg\n    0\n\n    ```\n    \"\"\"\n\n    self.val = 0\n    self.avg = 0\n    self.sum = 0\n    self.count = 0\n</code></pre>","location":"zh/metrics/average_meter/#danling.metrics.average_meter.AverageMeter.reset"},{"title":"<code>update(val, n=1)</code>","text":"<p>Updates the average and current value in the meter.</p> <p>Parameters:</p>    Name Type Description Default     <code>val</code>   <p>Value to be added to the average.</p>  required    <code>n</code>  <code>int</code>  <p>Number of values to be added.</p>  <code>1</code>     <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.update(0.9)\n&gt;&gt;&gt; meter.val\n0.9\n&gt;&gt;&gt; meter.avg\n0.8\n&gt;&gt;&gt; meter.sum\n1.6\n&gt;&gt;&gt; meter.count\n2\n</code></pre>  Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>def update(self, val, n: int = 1) -&gt; None:\n    r\"\"\"\n    Updates the average and current value in the meter.\n\n    Parameters\n    ----------\n    val: int\n        Value to be added to the average.\n    n: int = 1\n        Number of values to be added.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; meter = AverageMeter()\n    &gt;&gt;&gt; meter.update(0.7)\n    &gt;&gt;&gt; meter.val\n    0.7\n    &gt;&gt;&gt; meter.avg\n    0.7\n    &gt;&gt;&gt; meter.update(0.9)\n    &gt;&gt;&gt; meter.val\n    0.9\n    &gt;&gt;&gt; meter.avg\n    0.8\n    &gt;&gt;&gt; meter.sum\n    1.6\n    &gt;&gt;&gt; meter.count\n    2\n\n    ```\n    \"\"\"\n\n    # pylint: disable=C0103\n\n    self.val = val\n    self.sum += val * n\n    self.count += n\n    self.avg = self.sum / self.count\n</code></pre>","location":"zh/metrics/average_meter/#danling.metrics.average_meter.AverageMeter.update"},{"title":"Runner","text":"<p>The Runner of DanLing sets up the basic environment for running neural networks.</p>","location":"zh/runner/"},{"title":"Components","text":"<p>For readability and maintainability, there are three levels of Runner:</p>","location":"zh/runner/#components"},{"title":"[<code>RunnerBase</code>][danling.runner.bases.RunnerBase]","text":"<p>[<code>RunnerBase</code>][danling.runner.bases.RunnerBase] gives you a basic instinct on what attributes and properties are provided by the Runner.</p> <p>It works in an AbstractBaseClass manner and should neither be used directly nor be inherited from.</p>","location":"zh/runner/#runnerbasedanlingrunnerbasesrunnerbase"},{"title":"[<code>BaseRunner</code>][danling.runner.BaseRunner]","text":"<p>[<code>BaseRunner</code>][danling.runner.BaseRunner] contains core methods of general basic functionality, such as <code>init_logging</code>, <code>append_result</code>, <code>print_result</code>.</p>","location":"zh/runner/#baserunnerdanlingrunnerbaserunner"},{"title":"[<code>Runner</code>][danling.runner.TorchRunner].","text":"<p><code>Runner</code> should only contain platform-specific features. Currently, only [<code>TorchRunner</code>][danling.runner.TorchRunner] is supported.</p>","location":"zh/runner/#runnerdanlingrunnertorchrunner"},{"title":"Philosophy","text":"<p>DanLing Runner is designed for a 3.5-level experiments management system: Project, Group, Experiment, and, Run.</p>","location":"zh/runner/#philosophy"},{"title":"Project","text":"<p>Project corresponds to your project.</p> <p>Generally speaking, there should be only one project for each repository.</p>","location":"zh/runner/#project"},{"title":"Group","text":"<p>A group groups multiple experiments with similar characteristics.</p> <p>For example, if you run multiple experiments on learning rate, you may want to group them into a group.</p> <p>Note that Group is a virtual level (which is why it only counts 0.5) and does not corresponds to anything.</p>","location":"zh/runner/#group"},{"title":"Experiment","text":"<p>Experiment is the basic unit of experiments.</p> <p>Experiment usually corresponds to a commit, which means the code should be consistent across the experiment.</p>","location":"zh/runner/#experiment"},{"title":"Run","text":"<p>Run is the basic unit of running.</p> <p>Run corresponds to a single run of an experiment, each run may have different hyperparameters.</p>","location":"zh/runner/#run"},{"title":"Attributes &amp; Properties","text":"","location":"zh/runner/#attributes-properties"},{"title":"General","text":"<ul> <li>id</li> <li>name</li> <li>seed</li> <li>deterministic</li> </ul>","location":"zh/runner/#general"},{"title":"Progress","text":"<ul> <li>iters</li> <li>steps</li> <li>epochs</li> <li>iter_end</li> <li>step_end</li> <li>epoch_end</li> <li>progress</li> </ul> <p>Note that generally you should only use one of <code>iter</code>, <code>step</code>, <code>epoch</code> to indicate the progress.</p>","location":"zh/runner/#progress"},{"title":"Model","text":"<ul> <li>model</li> <li>criterion</li> <li>optimizer</li> <li>scheduler</li> </ul>","location":"zh/runner/#model"},{"title":"Data","text":"<ul> <li>datasets</li> <li>datasamplers</li> <li>dataloaders</li> <li>batch_size</li> <li>batch_size_equivalent</li> </ul> <p><code>datasets</code>, <code>datasamplers</code>, <code>dataloaders</code> should be a dict with the same keys. Their keys should be <code>split</code> (e.g. <code>train</code>, <code>val</code>, <code>test</code>).</p>","location":"zh/runner/#data"},{"title":"Results","text":"<ul> <li>results</li> <li>latest_result</li> <li>best_result</li> <li>scores</li> <li>latest_score</li> <li>best_score</li> <li>index_set</li> <li>index</li> <li>is_best</li> </ul> <p><code>results</code> should be a list of <code>result</code>. <code>result</code> should be a dict with the same <code>split</code> as keys, like <code>dataloaders</code>. A typical <code>result</code> might look like this: Python<pre><code>{\n    \"train\": {\n        \"loss\": 0.1,\n        \"accuracy\": 0.9,\n    },\n    \"val\": {\n        \"loss\": 0.2,\n        \"accuracy\": 0.8,\n    },\n    \"test\": {\n        \"loss\": 0.3,\n        \"accuracy\": 0.7,\n    },\n}\n</code></pre></p> <p><code>scores</code> are usually a list of <code>float</code>, and are dynamically extracted from <code>results</code> by <code>index_set</code> and <code>index</code>. If <code>index_set = \"val\"</code>, <code>index = \"accuracy\"</code>, then <code>scores = 0.9</code>.</p>","location":"zh/runner/#results"},{"title":"DDP","text":"<ul> <li>world_size</li> <li>rank</li> <li>local_rank</li> <li>distributed</li> <li>is_main_process</li> <li>is_local_main_process</li> </ul>","location":"zh/runner/#ddp"},{"title":"IO","text":"<ul> <li>experiments_root</li> <li>dir</li> <li>checkpoint_dir</li> <li>log_path</li> <li>checkpoint_dir_name</li> </ul> <p><code>experiments_root</code> is the root directory of all experiments, and should be consistent across the Project.</p> <p><code>dir</code> is the directory of a certain Run.</p> <p>There is no attributes/properties for Group and Experiment.</p> <p><code>checkpoint_dir_name</code> is relative to <code>dir</code>, and is passed to generate <code>checkpoint_dir</code> (<code>checkpoint_dir = os.path.join(dir, checkpoint_dir_name)</code>). In practice, <code>checkpoint_dir_name</code> is rarely called.</p>","location":"zh/runner/#io"},{"title":"logging","text":"<ul> <li>log</li> <li>logger</li> <li>tensorboard</li> <li>writer</li> </ul>","location":"zh/runner/#logging"},{"title":"BaseRunner","text":"<p>         Bases: <code>RunnerBase</code></p> <p>Set up everything for running a job.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>class BaseRunner(RunnerBase):\n    r\"\"\"\n    Set up everything for running a job.\n    \"\"\"\n\n    # pylint: disable=R0902\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        if self.seed is not None:\n            self.set_seed()\n        if self.deterministic:\n            self.set_deterministic()\n        if self.log:\n            self.init_logging()\n        self.init_print()\n        if self.tensorboard:\n            self.init_tensorboard()\n\n        atexit.register(self.print_result)\n\n    @on_main_process\n    def init_logging(self) -&gt; None:\n        r\"\"\"\n        Set up logging.\n        \"\"\"\n\n        # Why is setting up proper logging so !@?#! ugly?\n        logging.config.dictConfig(\n            {\n                \"version\": 1,\n                \"disable_existing_loggers\": False,\n                \"formatters\": {\n                    \"standard\": {\"format\": \"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"},\n                },\n                \"handlers\": {\n                    \"stdout\": {\n                        \"level\": \"INFO\",\n                        \"formatter\": \"standard\",\n                        \"class\": \"logging.StreamHandler\",\n                        \"stream\": \"ext://sys.stdout\",\n                    },\n                    \"logfile\": {\n                        \"level\": \"DEBUG\",\n                        \"formatter\": \"standard\",\n                        \"class\": \"logging.FileHandler\",\n                        \"filename\": self.log_path,\n                        \"mode\": \"a\",\n                    },\n                },\n                \"loggers\": {\n                    \"\": {\n                        \"handlers\": [\"stdout\", \"logfile\"],\n                        \"level\": \"DEBUG\",\n                        \"propagate\": True,\n                    },\n                },\n            }\n        )\n        logging.captureWarnings(True)\n        self.logger = logging.getLogger(\"runner\")\n        self.logger.flush = lambda: [h.flush() for h in self.logger.handlers]  # type: ignore\n\n    def init_print(self, process: int = 0) -&gt; None:\n        r\"\"\"\n        Set up print.\n\n        Only print on a specific process or when force is indicated.\n\n        Parameters\n        ----------\n        process: int, optional\n            The process to print on.\n\n        Notes\n        -----\n        If `self.log = True`, the default `print` function will be override by `logging.info`.\n        \"\"\"\n\n        logger = logging.getLogger(\"print\")\n        logger.flush = lambda: [h.flush for h in logger.handlers]  # type: ignore\n        import builtins as __builtin__  # pylint: disable=C0415\n\n        builtin_print = __builtin__.print\n\n        @catch\n        def print(*args, force=False, end=\"\\n\", file=None, flush=False, **kwargs):  # pylint: disable=W0622\n            if self.rank == process or force:\n                if self.log:\n                    logger.info(*args, **kwargs)\n                else:\n                    builtin_print(*args, end=end, file=file, flush=flush, **kwargs)\n\n        __builtin__.print = print\n\n    def set_deterministic(self) -&gt; None:\n        r\"\"\"\n        Set up deterministic.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def set_seed(self, bias: Optional[int] = None) -&gt; None:\n        r\"\"\"\n        Set up random seed.\n\n        Parameters\n        ----------\n        bias: Optional[int] = self.rank\n            Make the seed different for each processes.\n            This avoids same data augmentation are applied on every processes.\n            Set to `False` to disable this feature.\n        \"\"\"\n\n        if bias is None:\n            bias = self.rank\n        seed = self.seed + bias if bias else self.seed\n        np.random.seed(seed)\n        random.seed(seed)\n\n    def scale_lr(\n        self,\n        lr_scale_factor: Optional[float] = None,\n        batch_size_base: Optional[int] = None,\n    ) -&gt; None:\n        r\"\"\"\n        Scale learning rate according to [linear scaling rule](https://arxiv.org/abs/1706.02677).\n        \"\"\"\n\n        # pylint: disable=W0201\n\n        if lr_scale_factor is None:\n            if batch_size_base is None:\n                if batch_size_base := getattr(self, \"batch_size_base\", None) is None:\n                    raise ValueError(\"batch_size_base must be specified to auto scale lr\")\n            lr_scale_factor = self.batch_size_equivalent / batch_size_base\n        self.lr_scale_factor = lr_scale_factor\n        self.lr = self.lr * self.lr_scale_factor  # type: float  # pylint: disable=C0103\n        self.lr_final = self.lr_final * self.lr_scale_factor  # type: float\n\n    def step(self, zero_grad: bool = True) -&gt; None:\n        r\"\"\"\n        Step optimizer and scheduler.\n\n        This method also increment the `self.steps` attribute.\n\n        Parameters\n        ----------\n        zero_grad: bool, optional\n            Whether to zero the gradients.\n        \"\"\"\n\n        if self.optimizer is not None:\n            self.optimizer.step()\n            if zero_grad:\n                self.optimizer.zero_grad()\n        if self.scheduler is not None:\n            self.scheduler.step()\n        self.steps += 1\n        # TODO: Support `drop_last = False`\n        self.iters += self.batch_size_equivalent\n\n    def state_dict(self, cls: Callable = dict) -&gt; Mapping:\n        r\"\"\"\n        Return dict of all attributes for checkpoint.\n        \"\"\"\n\n        if self.model is None:\n            raise ValueError(\"Model must be defined when calling state_dict\")\n        model = self.accelerator.unwrap_model(self.model)\n        return cls(\n            runner=self.dict(),\n            model=model.state_dict(),\n            optimizer=self.optimizer.state_dict() if self.optimizer else None,\n            scheduler=self.scheduler.state_dict() if self.scheduler else None,\n        )\n\n    @catch\n    @on_main_process\n    def save_checkpoint(self) -&gt; None:\n        r\"\"\"\n        Save checkpoint to `runner.checkpoint_dir`.\n\n        The checkpoint will be saved to `runner.checkpoint_dir/latest.pth`.\n\n        If `save_freq` is specified and `self.epochs + 1` is a multiple of `save_freq`,\n        the checkpoint will also be copied to `runner.checkpoint_dir/epoch-{self.epochs}.pth`.\n\n        If `self.is_best` is `True`, the checkpoint will also be copied to `runner.checkpoint_dir/best.pth`.\n        \"\"\"\n\n        latest_path = os.path.join(self.checkpoint_dir, \"latest.pth\")\n        self.save(self.state_dict(), latest_path)\n        if hasattr(self, \"save_freq\") and (self.epochs + 1) % self.save_freq == 0:\n            save_path = os.path.join(self.checkpoint_dir, f\"epoch-{self.epochs}.pth\")\n            shutil.copy(latest_path, save_path)\n        if self.is_best:\n            best_path = os.path.join(self.checkpoint_dir, \"best.pth\")\n            shutil.copy(latest_path, best_path)\n\n    def load_checkpoint(  # pylint: disable=W1113\n        self, checkpoint: Optional[Union[Mapping, str]] = None, override_config: bool = True, *args, **kwargs\n    ) -&gt; None:\n        \"\"\"\n        Load info from checkpoint.\n\n        Parameters\n        ----------\n        checkpoint: Optional[Union[Mapping, str]] = latest_checkpoint\n            Checkpoint (or its path) to load.\n        override_config: bool = True\n            If True, override runner config with checkpoint config.\n        *args, **kwargs\n            Additional arguments to pass to `runner.load`.\n\n        Raises\n        ------\n        FileNotFoundError\n            If `checkpoint` does not exists.\n\n        See also\n        --------\n        from_checkpoint: Build runner from checkpoint.\n        \"\"\"\n\n        if checkpoint is None:\n            checkpoint = os.path.join(self.checkpoint_dir, \"latest.pth\")\n        # TODO: Support loading checkpoints in other format\n        if isinstance(checkpoint, str):\n            if not os.path.exists(checkpoint):\n                raise FileNotFoundError(f\"checkpoint is set to {checkpoint} but does not exist.\")\n            state_dict = self.load(checkpoint, *args, **kwargs)\n        # TODO: Wrap state_dict in a dataclass\n        if override_config:\n            self.__dict__.update(state_dict[\"runner\"])  # type: ignore\n        if self.model is not None and \"model\" in state_dict:  # type: ignore\n            self.model.load_state_dict(state_dict[\"model\"])  # type: ignore\n        if self.optimizer is not None and \"optimizer\" in state_dict:  # type: ignore\n            self.optimizer.load_state_dict(state_dict[\"optimizer\"])  # type: ignore\n        if self.scheduler is not None and \"scheduler\" in state_dict:  # type: ignore\n            self.scheduler.load_state_dict(state_dict[\"scheduler\"])  # type: ignore\n        self.checkpoint = checkpoint  # pylint: disable=W0201\n\n    @classmethod\n    def from_checkpoint(cls, checkpoint: Union[Mapping, str], *args, **kwargs) -&gt; BaseRunner:\n        r\"\"\"\n        Build BaseRunner from checkpoint.\n\n        Parameters\n        ----------\n        checkpoint: Optional[Union[Mapping, str]] = latest_checkpoint\n            Checkpoint (or its path) to load.\n        *args, **kwargs\n            Additional arguments to pass to `runner.load`.\n\n        Returns\n        -------\n        BaseRunner\n        \"\"\"\n\n        if isinstance(checkpoint, str):\n            checkpoint = cls.load(checkpoint, *args, **kwargs)\n        runner = cls(**checkpoint[\"runner\"])  # type: ignore\n        runner.load_checkpoint(checkpoint, override_config=False)\n        return runner\n\n    def append_result(self, result) -&gt; None:\n        r\"\"\"\n        Append result to `self.results`.\n\n        Warnings\n        --------\n        `self.results` is heavily relied upon for computing metrics.\n\n        Failed to use this method may lead to unexpected behavior.\n        \"\"\"\n\n        self.results.append(result)\n\n    def print_result(self) -&gt; None:\n        r\"\"\"\n        Print latest and best result.\n        \"\"\"\n\n        print(f\"latest result: {self.latest_result}\")\n        print(f\"best result: {self.best_result}\")\n\n    @catch\n    @on_main_process\n    def save_result(self) -&gt; None:\n        r\"\"\"\n        Save result to `runner.dir`.\n\n        This method will save latest and best result to\n        `runner.dir/latest.json` and `runner.dir/best.json` respectively.\n        \"\"\"\n\n        ret = {\"id\": self.id, \"name\": self.name}\n        result = self.latest_result  # type: ignore\n        if isinstance(result, FlatDict):\n            result = result.dict()  # type: ignore\n        # This is slower but ensure id is the first key\n        ret.update(result)  # type: ignore\n        latest_path = os.path.join(self.dir, \"latest.json\")\n        with FlatDict.open(latest_path, \"w\") as f:  # pylint: disable=C0103\n            json.dump(ret, f, indent=4)\n        if self.is_best:\n            best_path = os.path.join(self.dir, \"best.json\")\n            shutil.copy(latest_path, best_path)\n</code></pre>","location":"zh/runner/base_runner/"},{"title":"<code>step(zero_grad=True)</code>","text":"<p>Step optimizer and scheduler.</p> <p>This method also increment the <code>self.steps</code> attribute.</p> <p>Parameters:</p>    Name Type Description Default     <code>zero_grad</code>  <code>bool</code>  <p>Whether to zero the gradients.</p>  <code>True</code>      Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def step(self, zero_grad: bool = True) -&gt; None:\n    r\"\"\"\n    Step optimizer and scheduler.\n\n    This method also increment the `self.steps` attribute.\n\n    Parameters\n    ----------\n    zero_grad: bool, optional\n        Whether to zero the gradients.\n    \"\"\"\n\n    if self.optimizer is not None:\n        self.optimizer.step()\n        if zero_grad:\n            self.optimizer.zero_grad()\n    if self.scheduler is not None:\n        self.scheduler.step()\n    self.steps += 1\n    # TODO: Support `drop_last = False`\n    self.iters += self.batch_size_equivalent\n</code></pre>","location":"zh/runner/base_runner/#danling.runner.base_runner.BaseRunner.step"},{"title":"<code>append_result(result)</code>","text":"<p>Append result to <code>self.results</code>.</p>","location":"zh/runner/base_runner/#danling.runner.base_runner.BaseRunner.append_result"},{"title":"Warnings","text":"<p><code>self.results</code> is heavily relied upon for computing metrics.</p> <p>Failed to use this method may lead to unexpected behavior.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def append_result(self, result) -&gt; None:\n    r\"\"\"\n    Append result to `self.results`.\n\n    Warnings\n    --------\n    `self.results` is heavily relied upon for computing metrics.\n\n    Failed to use this method may lead to unexpected behavior.\n    \"\"\"\n\n    self.results.append(result)\n</code></pre>","location":"zh/runner/base_runner/#danling.runner.base_runner.BaseRunner.append_result--warnings"},{"title":"<code>print_result()</code>","text":"<p>Print latest and best result.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def print_result(self) -&gt; None:\n    r\"\"\"\n    Print latest and best result.\n    \"\"\"\n\n    print(f\"latest result: {self.latest_result}\")\n    print(f\"best result: {self.best_result}\")\n</code></pre>","location":"zh/runner/base_runner/#danling.runner.base_runner.BaseRunner.print_result"},{"title":"<code>save_result()</code>","text":"<p>Save result to <code>runner.dir</code>.</p> <p>This method will save latest and best result to <code>runner.dir/latest.json</code> and <code>runner.dir/best.json</code> respectively.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\n@on_main_process\ndef save_result(self) -&gt; None:\n    r\"\"\"\n    Save result to `runner.dir`.\n\n    This method will save latest and best result to\n    `runner.dir/latest.json` and `runner.dir/best.json` respectively.\n    \"\"\"\n\n    ret = {\"id\": self.id, \"name\": self.name}\n    result = self.latest_result  # type: ignore\n    if isinstance(result, FlatDict):\n        result = result.dict()  # type: ignore\n    # This is slower but ensure id is the first key\n    ret.update(result)  # type: ignore\n    latest_path = os.path.join(self.dir, \"latest.json\")\n    with FlatDict.open(latest_path, \"w\") as f:  # pylint: disable=C0103\n        json.dump(ret, f, indent=4)\n    if self.is_best:\n        best_path = os.path.join(self.dir, \"best.json\")\n        shutil.copy(latest_path, best_path)\n</code></pre>","location":"zh/runner/base_runner/#danling.runner.base_runner.BaseRunner.save_result"},{"title":"<code>state_dict(cls=dict)</code>","text":"<p>Return dict of all attributes for checkpoint.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def state_dict(self, cls: Callable = dict) -&gt; Mapping:\n    r\"\"\"\n    Return dict of all attributes for checkpoint.\n    \"\"\"\n\n    if self.model is None:\n        raise ValueError(\"Model must be defined when calling state_dict\")\n    model = self.accelerator.unwrap_model(self.model)\n    return cls(\n        runner=self.dict(),\n        model=model.state_dict(),\n        optimizer=self.optimizer.state_dict() if self.optimizer else None,\n        scheduler=self.scheduler.state_dict() if self.scheduler else None,\n    )\n</code></pre>","location":"zh/runner/base_runner/#danling.runner.base_runner.BaseRunner.state_dict"},{"title":"<code>save_checkpoint()</code>","text":"<p>Save checkpoint to <code>runner.checkpoint_dir</code>.</p> <p>The checkpoint will be saved to <code>runner.checkpoint_dir/latest.pth</code>.</p> <p>If <code>save_freq</code> is specified and <code>self.epochs + 1</code> is a multiple of <code>save_freq</code>, the checkpoint will also be copied to <code>runner.checkpoint_dir/epoch-{self.epochs}.pth</code>.</p> <p>If <code>self.is_best</code> is <code>True</code>, the checkpoint will also be copied to <code>runner.checkpoint_dir/best.pth</code>.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\n@on_main_process\ndef save_checkpoint(self) -&gt; None:\n    r\"\"\"\n    Save checkpoint to `runner.checkpoint_dir`.\n\n    The checkpoint will be saved to `runner.checkpoint_dir/latest.pth`.\n\n    If `save_freq` is specified and `self.epochs + 1` is a multiple of `save_freq`,\n    the checkpoint will also be copied to `runner.checkpoint_dir/epoch-{self.epochs}.pth`.\n\n    If `self.is_best` is `True`, the checkpoint will also be copied to `runner.checkpoint_dir/best.pth`.\n    \"\"\"\n\n    latest_path = os.path.join(self.checkpoint_dir, \"latest.pth\")\n    self.save(self.state_dict(), latest_path)\n    if hasattr(self, \"save_freq\") and (self.epochs + 1) % self.save_freq == 0:\n        save_path = os.path.join(self.checkpoint_dir, f\"epoch-{self.epochs}.pth\")\n        shutil.copy(latest_path, save_path)\n    if self.is_best:\n        best_path = os.path.join(self.checkpoint_dir, \"best.pth\")\n        shutil.copy(latest_path, best_path)\n</code></pre>","location":"zh/runner/base_runner/#danling.runner.base_runner.BaseRunner.save_checkpoint"},{"title":"<code>load_checkpoint(checkpoint=None, override_config=True, *args, **kwargs)</code>","text":"<p>Load info from checkpoint.</p> <p>Parameters:</p>    Name Type Description Default     <code>checkpoint</code>  <code>Optional[Union[Mapping, str]]</code>  <p>Checkpoint (or its path) to load.</p>  <code>None</code>    <code>override_config</code>  <code>bool</code>  <p>If True, override runner config with checkpoint config.</p>  <code>True</code>    <code>*args</code>   <p>Additional arguments to pass to <code>runner.load</code>.</p>  <code>()</code>    <code>**kwargs</code>   <p>Additional arguments to pass to <code>runner.load</code>.</p>  <code>()</code>     <p>Raises:</p>    Type Description      <code>FileNotFoundError</code>  <p>If <code>checkpoint</code> does not exists.</p>","location":"zh/runner/base_runner/#danling.runner.base_runner.BaseRunner.load_checkpoint"},{"title":"See also","text":"<p>from_checkpoint: Build runner from checkpoint.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def load_checkpoint(  # pylint: disable=W1113\n    self, checkpoint: Optional[Union[Mapping, str]] = None, override_config: bool = True, *args, **kwargs\n) -&gt; None:\n    \"\"\"\n    Load info from checkpoint.\n\n    Parameters\n    ----------\n    checkpoint: Optional[Union[Mapping, str]] = latest_checkpoint\n        Checkpoint (or its path) to load.\n    override_config: bool = True\n        If True, override runner config with checkpoint config.\n    *args, **kwargs\n        Additional arguments to pass to `runner.load`.\n\n    Raises\n    ------\n    FileNotFoundError\n        If `checkpoint` does not exists.\n\n    See also\n    --------\n    from_checkpoint: Build runner from checkpoint.\n    \"\"\"\n\n    if checkpoint is None:\n        checkpoint = os.path.join(self.checkpoint_dir, \"latest.pth\")\n    # TODO: Support loading checkpoints in other format\n    if isinstance(checkpoint, str):\n        if not os.path.exists(checkpoint):\n            raise FileNotFoundError(f\"checkpoint is set to {checkpoint} but does not exist.\")\n        state_dict = self.load(checkpoint, *args, **kwargs)\n    # TODO: Wrap state_dict in a dataclass\n    if override_config:\n        self.__dict__.update(state_dict[\"runner\"])  # type: ignore\n    if self.model is not None and \"model\" in state_dict:  # type: ignore\n        self.model.load_state_dict(state_dict[\"model\"])  # type: ignore\n    if self.optimizer is not None and \"optimizer\" in state_dict:  # type: ignore\n        self.optimizer.load_state_dict(state_dict[\"optimizer\"])  # type: ignore\n    if self.scheduler is not None and \"scheduler\" in state_dict:  # type: ignore\n        self.scheduler.load_state_dict(state_dict[\"scheduler\"])  # type: ignore\n    self.checkpoint = checkpoint  # pylint: disable=W0201\n</code></pre>","location":"zh/runner/base_runner/#danling.runner.base_runner.BaseRunner.load_checkpoint--see-also"},{"title":"<code>from_checkpoint(checkpoint, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Build BaseRunner from checkpoint.</p> <p>Parameters:</p>    Name Type Description Default     <code>checkpoint</code>  <code>Union[Mapping, str]</code>  <p>Checkpoint (or its path) to load.</p>  required    <code>*args</code>   <p>Additional arguments to pass to <code>runner.load</code>.</p>  <code>()</code>    <code>**kwargs</code>   <p>Additional arguments to pass to <code>runner.load</code>.</p>  <code>()</code>     <p>Returns:</p>    Type Description      <code>BaseRunner</code>       Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@classmethod\ndef from_checkpoint(cls, checkpoint: Union[Mapping, str], *args, **kwargs) -&gt; BaseRunner:\n    r\"\"\"\n    Build BaseRunner from checkpoint.\n\n    Parameters\n    ----------\n    checkpoint: Optional[Union[Mapping, str]] = latest_checkpoint\n        Checkpoint (or its path) to load.\n    *args, **kwargs\n        Additional arguments to pass to `runner.load`.\n\n    Returns\n    -------\n    BaseRunner\n    \"\"\"\n\n    if isinstance(checkpoint, str):\n        checkpoint = cls.load(checkpoint, *args, **kwargs)\n    runner = cls(**checkpoint[\"runner\"])  # type: ignore\n    runner.load_checkpoint(checkpoint, override_config=False)\n    return runner\n</code></pre>","location":"zh/runner/base_runner/#danling.runner.base_runner.BaseRunner.from_checkpoint"},{"title":"<code>init_logging()</code>","text":"<p>Set up logging.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@on_main_process\ndef init_logging(self) -&gt; None:\n    r\"\"\"\n    Set up logging.\n    \"\"\"\n\n    # Why is setting up proper logging so !@?#! ugly?\n    logging.config.dictConfig(\n        {\n            \"version\": 1,\n            \"disable_existing_loggers\": False,\n            \"formatters\": {\n                \"standard\": {\"format\": \"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"},\n            },\n            \"handlers\": {\n                \"stdout\": {\n                    \"level\": \"INFO\",\n                    \"formatter\": \"standard\",\n                    \"class\": \"logging.StreamHandler\",\n                    \"stream\": \"ext://sys.stdout\",\n                },\n                \"logfile\": {\n                    \"level\": \"DEBUG\",\n                    \"formatter\": \"standard\",\n                    \"class\": \"logging.FileHandler\",\n                    \"filename\": self.log_path,\n                    \"mode\": \"a\",\n                },\n            },\n            \"loggers\": {\n                \"\": {\n                    \"handlers\": [\"stdout\", \"logfile\"],\n                    \"level\": \"DEBUG\",\n                    \"propagate\": True,\n                },\n            },\n        }\n    )\n    logging.captureWarnings(True)\n    self.logger = logging.getLogger(\"runner\")\n    self.logger.flush = lambda: [h.flush() for h in self.logger.handlers]  # type: ignore\n</code></pre>","location":"zh/runner/base_runner/#danling.runner.base_runner.BaseRunner.init_logging"},{"title":"<code>init_print(process=0)</code>","text":"<p>Set up print.</p> <p>Only print on a specific process or when force is indicated.</p> <p>Parameters:</p>    Name Type Description Default     <code>process</code>  <code>int</code>  <p>The process to print on.</p>  <code>0</code>","location":"zh/runner/base_runner/#danling.runner.base_runner.BaseRunner.init_print"},{"title":"Notes","text":"<p>If <code>self.log = True</code>, the default <code>print</code> function will be override by <code>logging.info</code>.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def init_print(self, process: int = 0) -&gt; None:\n    r\"\"\"\n    Set up print.\n\n    Only print on a specific process or when force is indicated.\n\n    Parameters\n    ----------\n    process: int, optional\n        The process to print on.\n\n    Notes\n    -----\n    If `self.log = True`, the default `print` function will be override by `logging.info`.\n    \"\"\"\n\n    logger = logging.getLogger(\"print\")\n    logger.flush = lambda: [h.flush for h in logger.handlers]  # type: ignore\n    import builtins as __builtin__  # pylint: disable=C0415\n\n    builtin_print = __builtin__.print\n\n    @catch\n    def print(*args, force=False, end=\"\\n\", file=None, flush=False, **kwargs):  # pylint: disable=W0622\n        if self.rank == process or force:\n            if self.log:\n                logger.info(*args, **kwargs)\n            else:\n                builtin_print(*args, end=end, file=file, flush=flush, **kwargs)\n\n    __builtin__.print = print\n</code></pre>","location":"zh/runner/base_runner/#danling.runner.base_runner.BaseRunner.init_print--notes"},{"title":"<code>set_deterministic()</code>","text":"<p>Set up deterministic.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def set_deterministic(self) -&gt; None:\n    r\"\"\"\n    Set up deterministic.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>","location":"zh/runner/base_runner/#danling.runner.base_runner.BaseRunner.set_deterministic"},{"title":"<code>set_seed(bias=None)</code>","text":"<p>Set up random seed.</p> <p>Parameters:</p>    Name Type Description Default     <code>bias</code>  <code>Optional[int]</code>  <p>Make the seed different for each processes. This avoids same data augmentation are applied on every processes. Set to <code>False</code> to disable this feature.</p>  <code>None</code>      Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def set_seed(self, bias: Optional[int] = None) -&gt; None:\n    r\"\"\"\n    Set up random seed.\n\n    Parameters\n    ----------\n    bias: Optional[int] = self.rank\n        Make the seed different for each processes.\n        This avoids same data augmentation are applied on every processes.\n        Set to `False` to disable this feature.\n    \"\"\"\n\n    if bias is None:\n        bias = self.rank\n    seed = self.seed + bias if bias else self.seed\n    np.random.seed(seed)\n    random.seed(seed)\n</code></pre>","location":"zh/runner/base_runner/#danling.runner.base_runner.BaseRunner.set_seed"},{"title":"<code>scale_lr(lr_scale_factor=None, batch_size_base=None)</code>","text":"<p>Scale learning rate according to linear scaling rule.</p>  Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def scale_lr(\n    self,\n    lr_scale_factor: Optional[float] = None,\n    batch_size_base: Optional[int] = None,\n) -&gt; None:\n    r\"\"\"\n    Scale learning rate according to [linear scaling rule](https://arxiv.org/abs/1706.02677).\n    \"\"\"\n\n    # pylint: disable=W0201\n\n    if lr_scale_factor is None:\n        if batch_size_base is None:\n            if batch_size_base := getattr(self, \"batch_size_base\", None) is None:\n                raise ValueError(\"batch_size_base must be specified to auto scale lr\")\n        lr_scale_factor = self.batch_size_equivalent / batch_size_base\n    self.lr_scale_factor = lr_scale_factor\n    self.lr = self.lr * self.lr_scale_factor  # type: float  # pylint: disable=C0103\n    self.lr_final = self.lr_final * self.lr_scale_factor  # type: float\n</code></pre>","location":"zh/runner/base_runner/#danling.runner.base_runner.BaseRunner.scale_lr"},{"title":"RunnerBase","text":"<p>Base class for all runners.</p> <p><code>RunnerBase</code> is designed as a \u201cdataclass\u201d.</p> <p>It defines all basic attributes and relevant properties such as <code>scores</code>, <code>progress</code>, etc.</p> <p><code>RunnerBase</code> also defines basic IO operations such as <code>save</code>, <code>load</code>, <code>json</code>, <code>yaml</code>, etc.</p> <p>Attributes:</p>    Name Type Description     <code>id</code>  <code>str = f\"{self.name}-{self.seed}\"</code>     <code>name</code>  <code>str = \"danling\"</code>     <code>seed</code>  <code>int = randint(0, 2**32 - 1)</code>     <code>deterministic</code>  <code>bool = False</code>  <p>Ensure deterministic operations.</p>   <code>iters</code>  <code>int = 0</code>  <p>Current running iters. Iters refers to the number of data samples processed. Iters equals to steps when batch size is 1.</p>   <code>steps</code>  <code>int = 0</code>  <p>Current running steps. Steps refers to the number of <code>step</code> calls.</p>   <code>epochs</code>  <code>int = 0</code>  <p>Current running epochs. Epochs refers to the number of complete passes over the datasets.</p>   <code>iter_end</code>  <code>int</code>  <p>End running iters. Note that <code>step_end</code> not initialised since this variable may not apply to some Runners.</p>   <code>step_end</code>  <code>int</code>  <p>End running steps. Note that <code>step_end</code> not initialised since this variable may not apply to some Runners.</p>   <code>epoch_end</code>  <code>int</code>  <p>End running epochs. Note that <code>epoch_end</code> not initialised since this variable may not apply to some Runners.</p>   <code>model</code>  <code>Optional = None</code>     <code>criterion</code>  <code>Optional = None</code>     <code>optimizer</code>  <code>Optional = None</code>     <code>scheduler</code>  <code>Optional = None</code>     <code>datasets</code>  <code>FlatDict</code>  <p>All datasets, should be in the form of <code>{subset: dataset}</code>.</p>   <code>datasamplers</code>  <code>FlatDict</code>  <p>All datasamplers, should be in the form of <code>{subset: datasampler}</code>.</p>   <code>dataloaders</code>  <code>FlatDict</code>  <p>All dataloaders, should be in the form of <code>{subset: dataloader}</code>.</p>   <code>batch_size</code>  <code>int = 1</code>     <code>results</code>  <code>List[NestedDict] = []</code>  <p>All results, should be in the form of <code>[{subset: {index: score}}]</code>.</p>   <code>index_set</code>  <code>str = 'val'</code>  <p>The subset to calculate the core score.</p>   <code>index</code>  <code>str = 'loss'</code>  <p>The index to calculate the core score.</p>   <code>experiments_root</code>  <code>str = \"experiments\"</code>  <p>The root directory for all experiments.</p>   <code>checkpoint_dir_name</code>  <code>str = \"checkpoints\"</code>  <p>The name of the directory under <code>runner.dir</code> to save checkpoints.</p>   <code>log</code>  <code>bool = True</code>  <p>Whether to log the results.</p>   <code>logger</code>  <code>Optional[logging.Logger] = None</code>     <code>tensorboard</code>  <code>bool = False</code>  <p>Whether to use tensorboard.</p>   <code>writer</code>  <code>Optional[SummaryWriter] = None</code>","location":"zh/runner/bases/"},{"title":"Notes","text":"<p>The <code>RunnerBase</code> class is not intended to be used directly, nor to be directly inherit from.</p> <p>This is because <code>RunnerBase</code> is designed as a \u201cdataclass\u201d, and is meant for demonstrating all attributes and properties only.</p>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase--notes"},{"title":"See Also","text":"<p>[<code>BaseRunner</code>][danling.base_runner.BaseRunner]: The base runner class.</p>  Source code in <code>danling/runner/bases.py</code> Python<pre><code>class RunnerBase:\n    r\"\"\"\n    Base class for all runners.\n\n    `RunnerBase` is designed as a \"dataclass\".\n\n    It defines all basic attributes and relevant properties such as `scores`, `progress`, etc.\n\n    `RunnerBase` also defines basic IO operations such as `save`, `load`, `json`, `yaml`, etc.\n\n    Attributes\n    ----------\n    id: str = f\"{self.name}-{self.seed}\"\n    name: str = \"danling\"\n    seed: int = randint(0, 2**32 - 1)\n    deterministic: bool = False\n        Ensure [deterministic](https://pytorch.org/docs/stable/notes/randomness.html) operations.\n    iters: int = 0\n        Current running iters.\n        Iters refers to the number of data samples processed.\n        Iters equals to steps when batch size is 1.\n    steps: int = 0\n        Current running steps.\n        Steps refers to the number of `step` calls.\n    epochs: int = 0\n        Current running epochs.\n        Epochs refers to the number of complete passes over the datasets.\n    iter_end: int\n        End running iters.\n        Note that `step_end` not initialised since this variable may not apply to some Runners.\n    step_end: int\n        End running steps.\n        Note that `step_end` not initialised since this variable may not apply to some Runners.\n    epoch_end: int\n        End running epochs.\n        Note that `epoch_end` not initialised since this variable may not apply to some Runners.\n    model: Optional = None\n    criterion: Optional = None\n    optimizer: Optional = None\n    scheduler: Optional = None\n    datasets: FlatDict\n        All datasets, should be in the form of ``{subset: dataset}``.\n    datasamplers: FlatDict\n        All datasamplers, should be in the form of ``{subset: datasampler}``.\n    dataloaders: FlatDict\n        All dataloaders, should be in the form of ``{subset: dataloader}``.\n    batch_size: int = 1\n    results: List[NestedDict] = []\n        All results, should be in the form of ``[{subset: {index: score}}]``.\n    index_set: str = 'val'\n        The subset to calculate the core score.\n    index: str = 'loss'\n        The index to calculate the core score.\n    experiments_root: str = \"experiments\"\n        The root directory for all experiments.\n    checkpoint_dir_name: str = \"checkpoints\"\n        The name of the directory under `runner.dir` to save checkpoints.\n    log: bool = True\n        Whether to log the results.\n    logger: Optional[logging.Logger] = None\n    tensorboard: bool = False\n        Whether to use tensorboard.\n    writer: Optional[SummaryWriter] = None\n\n    Notes\n    -----\n    The `RunnerBase` class is not intended to be used directly, nor to be directly inherit from.\n\n    This is because `RunnerBase` is designed as a \"dataclass\",\n    and is meant for demonstrating all attributes and properties only.\n\n    See Also\n    --------\n    [`BaseRunner`][danling.base_runner.BaseRunner]: The base runner class.\n    \"\"\"\n\n    # pylint: disable=R0902, R0904\n\n    id: str = \"\"\n    name: str = \"DanLing\"\n\n    seed: int\n    deterministic: bool\n\n    iters: int\n    steps: int\n    epochs: int\n    # iter_begin: int  # Deprecated\n    # step_begin: int  # Deprecated\n    # epoch_begin: int  # Deprecated\n    iter_end: int\n    step_end: int\n    epoch_end: int\n\n    model: Optional = None\n    criterion: Optional = None\n    optimizer: Optional = None\n    scheduler: Optional = None\n\n    datasets: FlatDict\n    datasamplers: FlatDict\n    dataloaders: FlatDict\n\n    batch_size: int\n\n    results: List[NestedDict] = []\n    index_set: Optional[str]\n    index: str\n\n    experiments_root: str = \"experiments\"\n    checkpoint_dir_name: str = \"checkpoints\"\n    log: bool\n    logger: Optional[logging.Logger] = None\n    tensorboard: bool\n    writer: Optional = None\n\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        # Init attributes that should be kept in checkpoint inside `__init__`.\n        # Note that attributes should be init before redefine `self.__dict__`.\n        self.deterministic = False\n        self.iters = 0\n        self.steps = 0\n        self.epochs = 0\n        self.batch_size = 1\n        self.seed = randint(0, 2**32 - 1)\n        self.datasets = FlatDict()\n        self.datasamplers = FlatDict()\n        self.dataloaders = FlatDict()\n        self.index_set = None\n        self.index = \"loss\"\n        if len(args) == 1 and isinstance(args[0], FlatDict) and not kwargs:\n            args, kwargs = (), args[0]\n        self.__dict__ = NestedDict(**self.__dict__)\n        self.__dict__.update(args)\n        self.__dict__.update(kwargs)\n        if not self.id:\n            self.id = f\"{self.name}-{self.seed}\"  # pylint: disable=C0103\n\n    @property\n    def progress(self) -&gt; float:\n        r\"\"\"\n        Training Progress.\n\n        Returns\n        -------\n        float\n\n        Raises\n        ------\n        RuntimeError\n            If no terminal is defined.\n        \"\"\"\n\n        if hasattr(self, \"iter_end\"):\n            return self.iters / self.iter_end\n        if hasattr(self, \"step_end\"):\n            return self.steps / self.step_end\n        if hasattr(self, \"epoch_end\"):\n            return self.epochs / self.epoch_end\n        raise RuntimeError(\"DanLing cannot determine progress since no terminal is defined.\")\n\n    @property\n    def batch_size_equivalent(self) -&gt; int:\n        r\"\"\"\n        Actual batch size.\n\n        `batch_size` * `world_size` * `accum_steps`\n        \"\"\"\n\n        return self.batch_size * self.world_size * getattr(self, \"accum_steps\", 1)\n\n    @property\n    def latest_result(self) -&gt; Optional[NestedDict]:\n        r\"\"\"\n        Latest result.\n\n        Returns\n        -------\n        Optional[NestedDict]\n        \"\"\"\n\n        return self.results[-1] if self.results else None\n\n    @property\n    def best_result(self) -&gt; Optional[NestedDict]:\n        r\"\"\"\n        Best result.\n\n        Returns\n        -------\n        Optional[NestedDict]\n        \"\"\"\n\n        return self.results[-1 - self.scores[::-1].index(self.best_score)] if self.results else None  # type: ignore\n\n    @property\n    def scores(self) -&gt; List[float]:\n        r\"\"\"\n        All scores.\n\n        Scores are extracted from results by `index_set` and `runner.index`,\n        following `[r[index_set][self.index] for r in self.results]`.\n\n        By default, `index_set` points to `self.index_set` and is set to `val`,\n        if `self.index_set` is not set, it will be the last key of the last result.\n\n        Scores are considered as the index of the performance of the model.\n        It is useful to determine the best model and the best hyper-parameters.\n\n        Returns\n        -------\n        List[float]\n        \"\"\"\n\n        if not self.results:\n            return []\n        index_set = self.index_set or next(reversed(self.results[-1]))\n        return [r[index_set][self.index] for r in self.results]\n\n    @property\n    def latest_score(self) -&gt; Optional[float]:\n        r\"\"\"\n        Latest score.\n\n        Returns\n        -------\n        Optional[float]\n        \"\"\"\n\n        return self.scores[-1] if self.results else None\n\n    @property\n    def best_score(self) -&gt; Optional[float]:\n        r\"\"\"\n        Best score.\n\n        Returns\n        -------\n        Optional[float]\n        \"\"\"\n\n        return self.best_fn(self.scores) if self.results else None\n\n    @staticmethod\n    def best_fn(scores: Sequence[float], fn: Callable = max) -&gt; float:  # pylint: disable=C0103\n        r\"\"\"\n        Function to determine the best score from a list of scores.\n\n        Subclass can override this method to accommodate needs, such as `min(scores)`.\n\n        Parameters\n        ----------\n        scores: Sequence[float]\n            List of scores.\n        fn: Callable = max\n            Function to determine the best score from a list of scores.\n\n        Returns\n        -------\n        best_score: float\n            The best score from a list of scores.\n        \"\"\"\n\n        return fn(scores)\n\n    @property\n    def is_best(self) -&gt; bool:\n        r\"\"\"\n        If current epoch is the best epoch.\n\n        Returns\n        -------\n        bool\n        \"\"\"\n\n        return abs(self.latest_score - self.best_score) &lt; 1e-7\n        # return self.latest_score == self.best_score\n\n    @property\n    def world_size(self) -&gt; int:\n        r\"\"\"\n        Number of Processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return 1\n\n    @property\n    def rank(self) -&gt; int:\n        r\"\"\"\n        Process index in all processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return 0\n\n    @property\n    def local_rank(self) -&gt; int:\n        r\"\"\"\n        Process index in local processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return 0\n\n    @property\n    def distributed(self) -&gt; bool:\n        r\"\"\"\n        If runner is running in distributed mode.\n        \"\"\"\n\n        return self.world_size &gt; 1\n\n    @property\n    def is_main_process(self) -&gt; bool:\n        r\"\"\"\n        If current process is the main process.\n\n        Returns\n        -------\n        bool\n        \"\"\"\n\n        return self.rank == 0\n\n    @property\n    def is_local_main_process(self) -&gt; bool:\n        r\"\"\"\n        If current process is the main process in local.\n\n        Returns\n        -------\n        bool\n        \"\"\"\n\n        return self.local_rank == 0\n\n    @property\n    @ensure_dir\n    def dir(self) -&gt; str:\n        r\"\"\"\n        Directory of the experiment.\n\n        Returns\n        -------\n        str\n        \"\"\"\n\n        return os.path.join(self.experiments_root, self.id)\n\n    @property\n    def log_path(self) -&gt; str:\n        r\"\"\"\n        Path of log file.\n\n        Returns\n        -------\n        str\n        \"\"\"\n\n        return os.path.join(self.dir, \"run.log\")\n\n    @property\n    @ensure_dir\n    def checkpoint_dir(self) -&gt; str:\n        r\"\"\"\n        Directory of checkpoints.\n\n        Returns\n        -------\n        str\n        \"\"\"\n\n        return os.path.join(self.dir, self.checkpoint_dir_name)\n\n    @catch\n    def save(self, obj: Any, f: File, main_process_only: bool = True) -&gt; File:  # pylint: disable=C0103\n        r\"\"\"\n        Save object to a path or file.\n\n        Returns\n        -------\n        File\n        \"\"\"\n\n        raise NotImplementedError\n\n    @staticmethod\n    def load(f: File, *args, **kwargs) -&gt; Any:  # pylint: disable=C0103\n        r\"\"\"\n        Load object from a path or file.\n\n        Returns\n        -------\n        Any\n        \"\"\"\n\n        raise NotImplementedError\n\n    def dict(self, cls: Callable = dict, only_json_serializable: bool = True) -&gt; Mapping:\n        r\"\"\"\n        Convert config to Mapping.\n\n        Note that all non-json-serializable objects will be removed.\n\n        Parameters\n        ----------\n        cls : Callable = dict\n            Class to convert to.\n        only_json_serializable : bool = True\n            If only json serializable objects should be kept.\n\n        Returns\n        -------\n        Mapping\n        \"\"\"\n\n        # pylint: disable=C0103\n\n        ret = cls()\n        for k, v in self.__dict__.items():\n            if isinstance(v, FlatDict):\n                v = v.dict(cls)\n            if not only_json_serializable or is_json_serializable(v):\n                ret[k] = v\n        return ret\n\n    @catch\n    def json(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=W1113\n        r\"\"\"\n        Dump Runner to json file.\n        \"\"\"\n\n        if main_process_only and self.is_main_process or not main_process_only:\n            with FlatDict.open(file, mode=\"w\") as fp:  # pylint: disable=C0103\n                fp.write(self.jsons(*args, **kwargs))\n\n    @classmethod\n    def from_json(cls, file: File, *args, **kwargs) -&gt; RunnerBase:\n        r\"\"\"\n        Construct Runner from json file.\n\n        This function calls `self.from_jsons()` to construct object from json string.\n        You may overwrite `from_jsons` in case something is not json serializable.\n\n        Returns\n        -------\n        RunnerBase\n        \"\"\"\n\n        with FlatDict.open(file) as fp:  # pylint: disable=C0103\n            return cls.from_jsons(fp.read(), *args, **kwargs)\n\n    def jsons(self, *args, **kwargs) -&gt; str:\n        r\"\"\"\n        Dump Runner to json string.\n\n        Returns\n        -------\n        json: str\n        \"\"\"\n\n        if \"cls\" not in kwargs:\n            kwargs[\"cls\"] = JsonEncoder\n        return json_dumps(self.dict(), *args, **kwargs)\n\n    @classmethod\n    def from_jsons(cls, string: str, *args, **kwargs) -&gt; RunnerBase:\n        r\"\"\"\n        Construct Runner from json string.\n\n        Returns\n        -------\n        RunnerBase\n        \"\"\"\n\n        return cls(**Config.from_jsons(string, *args, **kwargs))\n\n    @catch\n    def yaml(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=W1113\n        r\"\"\"\n        Dump Runner to yaml file.\n        \"\"\"\n\n        if main_process_only and self.is_main_process or not main_process_only:\n            with FlatDict.open(file, mode=\"w\") as fp:  # pylint: disable=C0103\n                self.yamls(fp, *args, **kwargs)\n\n    @classmethod\n    def from_yaml(cls, file: File, *args, **kwargs) -&gt; RunnerBase:\n        r\"\"\"\n        Construct Runner from yaml file.\n\n        This function calls `self.from_yamls()` to construct object from yaml string.\n        You may overwrite `from_yamls` in case something is not yaml serializable.\n\n        Returns\n        -------\n        RunnerBase\n        \"\"\"\n\n        with FlatDict.open(file) as fp:  # pylint: disable=C0103\n            return cls.from_yamls(fp.read(), *args, **kwargs)\n\n    def yamls(self, *args, **kwargs) -&gt; str:\n        r\"\"\"\n        Dump Runner to yaml string.\n\n        Returns\n        -------\n        yaml: str\n        \"\"\"\n\n        if \"Dumper\" not in kwargs:\n            kwargs[\"Dumper\"] = YamlDumper\n        return yaml_dump(self.dict(), *args, **kwargs)  # type: ignore\n\n    @classmethod\n    def from_yamls(cls, string: str, *args, **kwargs) -&gt; RunnerBase:\n        r\"\"\"\n        Construct Runner from yaml string.\n\n        Returns\n        -------\n        RunnerBase\n        \"\"\"\n\n        return cls(**Config.from_yamls(string, *args, **kwargs))\n\n    def __getattr__(self, name) -&gt; Any:\n        if \"accelerator\" not in self:\n            raise RuntimeError(f\"{self.__class__.__name__} is not properly initialised\")\n        if hasattr(self.accelerator, name):\n            return getattr(self.accelerator, name)\n        raise AttributeError(f\"{self.__class__.__name__} does not contain {name}\")\n\n    def __contains__(self, name) -&gt; bool:\n        return name in self.__dict__\n\n    def __repr__(self):\n        lines = []\n        for key, value in self.__dict__.items():\n            value_str = repr(value)\n            value_str = self._add_indent(value_str)\n            lines.append(\"(\" + key + \"): \" + value_str)\n\n        main_str = self.__class__.__name__ + \"(\"\n        if lines:\n            main_str += \"\\n  \" + \"\\n  \".join(lines) + \"\\n\"\n\n        main_str += \")\"\n        return main_str\n\n    def _add_indent(self, text):\n        lines = text.split(\"\\n\")\n        # don't do anything for single-line stuff\n        if len(lines) == 1:\n            return text\n        first = lines.pop(0)\n        # add 2 spaces to each line but the first\n        lines = [(2 * \" \") + line for line in lines]\n        lines = \"\\n\".join(lines)\n        lines = first + \"\\n\" + lines\n        return lines\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase--see-also"},{"title":"<code>progress()</code>  <code>property</code>","text":"<p>Training Progress.</p> <p>Returns:</p>    Type Description      <code>float</code>      <p>Raises:</p>    Type Description      <code>RuntimeError</code>  <p>If no terminal is defined.</p>     Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef progress(self) -&gt; float:\n    r\"\"\"\n    Training Progress.\n\n    Returns\n    -------\n    float\n\n    Raises\n    ------\n    RuntimeError\n        If no terminal is defined.\n    \"\"\"\n\n    if hasattr(self, \"iter_end\"):\n        return self.iters / self.iter_end\n    if hasattr(self, \"step_end\"):\n        return self.steps / self.step_end\n    if hasattr(self, \"epoch_end\"):\n        return self.epochs / self.epoch_end\n    raise RuntimeError(\"DanLing cannot determine progress since no terminal is defined.\")\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.progress"},{"title":"<code>batch_size_equivalent()</code>  <code>property</code>","text":"<p>Actual batch size.</p> <p><code>batch_size</code> * <code>world_size</code> * <code>accum_steps</code></p>  Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef batch_size_equivalent(self) -&gt; int:\n    r\"\"\"\n    Actual batch size.\n\n    `batch_size` * `world_size` * `accum_steps`\n    \"\"\"\n\n    return self.batch_size * self.world_size * getattr(self, \"accum_steps\", 1)\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.batch_size_equivalent"},{"title":"<code>best_fn(scores, fn=max)</code>  <code>staticmethod</code>","text":"<p>Function to determine the best score from a list of scores.</p> <p>Subclass can override this method to accommodate needs, such as <code>min(scores)</code>.</p> <p>Parameters:</p>    Name Type Description Default     <code>scores</code>  <code>Sequence[float]</code>  <p>List of scores.</p>  required    <code>fn</code>  <code>Callable</code>  <p>Function to determine the best score from a list of scores.</p>  <code>max</code>     <p>Returns:</p>    Name Type Description     <code>best_score</code>  <code>float</code>  <p>The best score from a list of scores.</p>     Source code in <code>danling/runner/bases.py</code> Python<pre><code>@staticmethod\ndef best_fn(scores: Sequence[float], fn: Callable = max) -&gt; float:  # pylint: disable=C0103\n    r\"\"\"\n    Function to determine the best score from a list of scores.\n\n    Subclass can override this method to accommodate needs, such as `min(scores)`.\n\n    Parameters\n    ----------\n    scores: Sequence[float]\n        List of scores.\n    fn: Callable = max\n        Function to determine the best score from a list of scores.\n\n    Returns\n    -------\n    best_score: float\n        The best score from a list of scores.\n    \"\"\"\n\n    return fn(scores)\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.best_fn"},{"title":"<code>latest_result()</code>  <code>property</code>","text":"<p>Latest result.</p> <p>Returns:</p>    Type Description      <code>Optional[NestedDict]</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef latest_result(self) -&gt; Optional[NestedDict]:\n    r\"\"\"\n    Latest result.\n\n    Returns\n    -------\n    Optional[NestedDict]\n    \"\"\"\n\n    return self.results[-1] if self.results else None\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.latest_result"},{"title":"<code>best_result()</code>  <code>property</code>","text":"<p>Best result.</p> <p>Returns:</p>    Type Description      <code>Optional[NestedDict]</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef best_result(self) -&gt; Optional[NestedDict]:\n    r\"\"\"\n    Best result.\n\n    Returns\n    -------\n    Optional[NestedDict]\n    \"\"\"\n\n    return self.results[-1 - self.scores[::-1].index(self.best_score)] if self.results else None  # type: ignore\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.best_result"},{"title":"<code>scores()</code>  <code>property</code>","text":"<p>All scores.</p> <p>Scores are extracted from results by <code>index_set</code> and <code>runner.index</code>, following <code>[r[index_set][self.index] for r in self.results]</code>.</p> <p>By default, <code>index_set</code> points to <code>self.index_set</code> and is set to <code>val</code>, if <code>self.index_set</code> is not set, it will be the last key of the last result.</p> <p>Scores are considered as the index of the performance of the model. It is useful to determine the best model and the best hyper-parameters.</p> <p>Returns:</p>    Type Description      <code>List[float]</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef scores(self) -&gt; List[float]:\n    r\"\"\"\n    All scores.\n\n    Scores are extracted from results by `index_set` and `runner.index`,\n    following `[r[index_set][self.index] for r in self.results]`.\n\n    By default, `index_set` points to `self.index_set` and is set to `val`,\n    if `self.index_set` is not set, it will be the last key of the last result.\n\n    Scores are considered as the index of the performance of the model.\n    It is useful to determine the best model and the best hyper-parameters.\n\n    Returns\n    -------\n    List[float]\n    \"\"\"\n\n    if not self.results:\n        return []\n    index_set = self.index_set or next(reversed(self.results[-1]))\n    return [r[index_set][self.index] for r in self.results]\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.scores"},{"title":"<code>latest_score()</code>  <code>property</code>","text":"<p>Latest score.</p> <p>Returns:</p>    Type Description      <code>Optional[float]</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef latest_score(self) -&gt; Optional[float]:\n    r\"\"\"\n    Latest score.\n\n    Returns\n    -------\n    Optional[float]\n    \"\"\"\n\n    return self.scores[-1] if self.results else None\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.latest_score"},{"title":"<code>best_score()</code>  <code>property</code>","text":"<p>Best score.</p> <p>Returns:</p>    Type Description      <code>Optional[float]</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef best_score(self) -&gt; Optional[float]:\n    r\"\"\"\n    Best score.\n\n    Returns\n    -------\n    Optional[float]\n    \"\"\"\n\n    return self.best_fn(self.scores) if self.results else None\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.best_score"},{"title":"<code>is_best()</code>  <code>property</code>","text":"<p>If current epoch is the best epoch.</p> <p>Returns:</p>    Type Description      <code>bool</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef is_best(self) -&gt; bool:\n    r\"\"\"\n    If current epoch is the best epoch.\n\n    Returns\n    -------\n    bool\n    \"\"\"\n\n    return abs(self.latest_score - self.best_score) &lt; 1e-7\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.is_best"},{"title":"<code>world_size()</code>  <code>property</code>","text":"<p>Number of Processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef world_size(self) -&gt; int:\n    r\"\"\"\n    Number of Processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return 1\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.world_size"},{"title":"<code>rank()</code>  <code>property</code>","text":"<p>Process index in all processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef rank(self) -&gt; int:\n    r\"\"\"\n    Process index in all processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return 0\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.rank"},{"title":"<code>local_rank()</code>  <code>property</code>","text":"<p>Process index in local processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef local_rank(self) -&gt; int:\n    r\"\"\"\n    Process index in local processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return 0\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.local_rank"},{"title":"<code>distributed()</code>  <code>property</code>","text":"<p>If runner is running in distributed mode.</p>  Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef distributed(self) -&gt; bool:\n    r\"\"\"\n    If runner is running in distributed mode.\n    \"\"\"\n\n    return self.world_size &gt; 1\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.distributed"},{"title":"<code>is_main_process()</code>  <code>property</code>","text":"<p>If current process is the main process.</p> <p>Returns:</p>    Type Description      <code>bool</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef is_main_process(self) -&gt; bool:\n    r\"\"\"\n    If current process is the main process.\n\n    Returns\n    -------\n    bool\n    \"\"\"\n\n    return self.rank == 0\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.is_main_process"},{"title":"<code>is_local_main_process()</code>  <code>property</code>","text":"<p>If current process is the main process in local.</p> <p>Returns:</p>    Type Description      <code>bool</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef is_local_main_process(self) -&gt; bool:\n    r\"\"\"\n    If current process is the main process in local.\n\n    Returns\n    -------\n    bool\n    \"\"\"\n\n    return self.local_rank == 0\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.is_local_main_process"},{"title":"<code>dir()</code>  <code>property</code>","text":"<p>Directory of the experiment.</p> <p>Returns:</p>    Type Description      <code>str</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\n@ensure_dir\ndef dir(self) -&gt; str:\n    r\"\"\"\n    Directory of the experiment.\n\n    Returns\n    -------\n    str\n    \"\"\"\n\n    return os.path.join(self.experiments_root, self.id)\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.dir"},{"title":"<code>log_path()</code>  <code>property</code>","text":"<p>Path of log file.</p> <p>Returns:</p>    Type Description      <code>str</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\ndef log_path(self) -&gt; str:\n    r\"\"\"\n    Path of log file.\n\n    Returns\n    -------\n    str\n    \"\"\"\n\n    return os.path.join(self.dir, \"run.log\")\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.log_path"},{"title":"<code>checkpoint_dir()</code>  <code>property</code>","text":"<p>Directory of checkpoints.</p> <p>Returns:</p>    Type Description      <code>str</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@property\n@ensure_dir\ndef checkpoint_dir(self) -&gt; str:\n    r\"\"\"\n    Directory of checkpoints.\n\n    Returns\n    -------\n    str\n    \"\"\"\n\n    return os.path.join(self.dir, self.checkpoint_dir_name)\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.checkpoint_dir"},{"title":"<code>save(obj, f, main_process_only=True)</code>","text":"<p>Save object to a path or file.</p> <p>Returns:</p>    Type Description      <code>File</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@catch\ndef save(self, obj: Any, f: File, main_process_only: bool = True) -&gt; File:  # pylint: disable=C0103\n    r\"\"\"\n    Save object to a path or file.\n\n    Returns\n    -------\n    File\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.save"},{"title":"<code>load(f, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Load object from a path or file.</p> <p>Returns:</p>    Type Description      <code>Any</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@staticmethod\ndef load(f: File, *args, **kwargs) -&gt; Any:  # pylint: disable=C0103\n    r\"\"\"\n    Load object from a path or file.\n\n    Returns\n    -------\n    Any\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.load"},{"title":"<code>dict(cls=dict, only_json_serializable=True)</code>","text":"<p>Convert config to Mapping.</p> <p>Note that all non-json-serializable objects will be removed.</p> <p>Parameters:</p>    Name Type Description Default     <code>cls</code>  <code>Callable</code>  <p>Class to convert to.</p>  <code>dict</code>    <code>only_json_serializable</code>  <code>bool</code>  <p>If only json serializable objects should be kept.</p>  <code>True</code>     <p>Returns:</p>    Type Description      <code>Mapping</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>def dict(self, cls: Callable = dict, only_json_serializable: bool = True) -&gt; Mapping:\n    r\"\"\"\n    Convert config to Mapping.\n\n    Note that all non-json-serializable objects will be removed.\n\n    Parameters\n    ----------\n    cls : Callable = dict\n        Class to convert to.\n    only_json_serializable : bool = True\n        If only json serializable objects should be kept.\n\n    Returns\n    -------\n    Mapping\n    \"\"\"\n\n    # pylint: disable=C0103\n\n    ret = cls()\n    for k, v in self.__dict__.items():\n        if isinstance(v, FlatDict):\n            v = v.dict(cls)\n        if not only_json_serializable or is_json_serializable(v):\n            ret[k] = v\n    return ret\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.dict"},{"title":"<code>json(file, main_process_only=True, *args, **kwargs)</code>","text":"<p>Dump Runner to json file.</p>  Source code in <code>danling/runner/bases.py</code> Python<pre><code>@catch\ndef json(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=W1113\n    r\"\"\"\n    Dump Runner to json file.\n    \"\"\"\n\n    if main_process_only and self.is_main_process or not main_process_only:\n        with FlatDict.open(file, mode=\"w\") as fp:  # pylint: disable=C0103\n            fp.write(self.jsons(*args, **kwargs))\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.json"},{"title":"<code>from_json(file, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from json file.</p> <p>This function calls <code>self.from_jsons()</code> to construct object from json string. You may overwrite <code>from_jsons</code> in case something is not json serializable.</p> <p>Returns:</p>    Type Description      <code>RunnerBase</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@classmethod\ndef from_json(cls, file: File, *args, **kwargs) -&gt; RunnerBase:\n    r\"\"\"\n    Construct Runner from json file.\n\n    This function calls `self.from_jsons()` to construct object from json string.\n    You may overwrite `from_jsons` in case something is not json serializable.\n\n    Returns\n    -------\n    RunnerBase\n    \"\"\"\n\n    with FlatDict.open(file) as fp:  # pylint: disable=C0103\n        return cls.from_jsons(fp.read(), *args, **kwargs)\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.from_json"},{"title":"<code>jsons(*args, **kwargs)</code>","text":"<p>Dump Runner to json string.</p> <p>Returns:</p>    Name Type Description     <code>json</code>  <code>str</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>def jsons(self, *args, **kwargs) -&gt; str:\n    r\"\"\"\n    Dump Runner to json string.\n\n    Returns\n    -------\n    json: str\n    \"\"\"\n\n    if \"cls\" not in kwargs:\n        kwargs[\"cls\"] = JsonEncoder\n    return json_dumps(self.dict(), *args, **kwargs)\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.jsons"},{"title":"<code>from_jsons(string, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from json string.</p> <p>Returns:</p>    Type Description      <code>RunnerBase</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@classmethod\ndef from_jsons(cls, string: str, *args, **kwargs) -&gt; RunnerBase:\n    r\"\"\"\n    Construct Runner from json string.\n\n    Returns\n    -------\n    RunnerBase\n    \"\"\"\n\n    return cls(**Config.from_jsons(string, *args, **kwargs))\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.from_jsons"},{"title":"<code>yaml(file, main_process_only=True, *args, **kwargs)</code>","text":"<p>Dump Runner to yaml file.</p>  Source code in <code>danling/runner/bases.py</code> Python<pre><code>@catch\ndef yaml(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=W1113\n    r\"\"\"\n    Dump Runner to yaml file.\n    \"\"\"\n\n    if main_process_only and self.is_main_process or not main_process_only:\n        with FlatDict.open(file, mode=\"w\") as fp:  # pylint: disable=C0103\n            self.yamls(fp, *args, **kwargs)\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.yaml"},{"title":"<code>from_yaml(file, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from yaml file.</p> <p>This function calls <code>self.from_yamls()</code> to construct object from yaml string. You may overwrite <code>from_yamls</code> in case something is not yaml serializable.</p> <p>Returns:</p>    Type Description      <code>RunnerBase</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@classmethod\ndef from_yaml(cls, file: File, *args, **kwargs) -&gt; RunnerBase:\n    r\"\"\"\n    Construct Runner from yaml file.\n\n    This function calls `self.from_yamls()` to construct object from yaml string.\n    You may overwrite `from_yamls` in case something is not yaml serializable.\n\n    Returns\n    -------\n    RunnerBase\n    \"\"\"\n\n    with FlatDict.open(file) as fp:  # pylint: disable=C0103\n        return cls.from_yamls(fp.read(), *args, **kwargs)\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.from_yaml"},{"title":"<code>yamls(*args, **kwargs)</code>","text":"<p>Dump Runner to yaml string.</p> <p>Returns:</p>    Name Type Description     <code>yaml</code>  <code>str</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>def yamls(self, *args, **kwargs) -&gt; str:\n    r\"\"\"\n    Dump Runner to yaml string.\n\n    Returns\n    -------\n    yaml: str\n    \"\"\"\n\n    if \"Dumper\" not in kwargs:\n        kwargs[\"Dumper\"] = YamlDumper\n    return yaml_dump(self.dict(), *args, **kwargs)  # type: ignore\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.yamls"},{"title":"<code>from_yamls(string, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from yaml string.</p> <p>Returns:</p>    Type Description      <code>RunnerBase</code>       Source code in <code>danling/runner/bases.py</code> Python<pre><code>@classmethod\ndef from_yamls(cls, string: str, *args, **kwargs) -&gt; RunnerBase:\n    r\"\"\"\n    Construct Runner from yaml string.\n\n    Returns\n    -------\n    RunnerBase\n    \"\"\"\n\n    return cls(**Config.from_yamls(string, *args, **kwargs))\n</code></pre>","location":"zh/runner/bases/#danling.runner.bases.RunnerBase.from_yamls"},{"title":"TorchRunner","text":"<p>         Bases: <code>BaseRunner</code></p> <p>Set up everything for running a job.</p> <p>Attributes:</p>    Name Type Description     <code>accelerator</code>  <code>Accelerator</code>     <code>accelerate</code>  <code>Dict[str, Any] = {}</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>class TorchRunner(BaseRunner):\n    r\"\"\"\n    Set up everything for running a job.\n\n    Attributes\n    ----------\n    accelerator: Accelerator\n    accelerate: Dict[str, Any] = {}\n    \"\"\"\n\n    # pylint: disable=R0902\n\n    accelerator: Accelerator\n    accelerate: Dict[str, Any] = {}\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        self.accelerator = Accelerator(**self.accelerate)\n        super().__init__(*args, **kwargs)\n\n    @on_main_process\n    def init_tensorboard(self, *args, **kwargs) -&gt; None:\n        r\"\"\"\n        Set up Tensoraoard SummaryWriter.\n        \"\"\"\n        from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n        if \"log_dir\" not in kwargs:\n            kwargs[\"log_dir\"] = self.dir\n\n        self.writer = SummaryWriter(*args, **kwargs)\n\n    def set_seed(self, bias: Optional[int] = None) -&gt; None:\n        r\"\"\"\n        Set up random seed.\n\n        Parameters\n        ----------\n        bias: Optional[int] = self.rank\n            Make the seed different for each processes.\n            This avoids same data augmentation are applied on every processes.\n            Set to `False` to disable this feature.\n        \"\"\"\n\n        if self.distributed:\n            # TODO: use broadcast_object instead.\n            self.seed = (\n                self.accelerator.gather(torch.tensor(self.seed).cuda()).unsqueeze(0).flatten()[0]  # pylint: disable=E1101\n            )\n        if bias is None:\n            bias = self.rank\n        seed = self.seed + bias if bias else self.seed\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        np.random.seed(seed)\n        random.seed(seed)\n\n    def set_deterministic(self) -&gt; None:\n        r\"\"\"\n        Set up deterministic.\n        \"\"\"\n\n        cudnn.benchmark = False\n        cudnn.deterministic = True\n        if torch.__version__ &gt;= \"1.8.0\":\n            torch.use_deterministic_algorithms(True)\n\n    def init_distributed(self) -&gt; None:\n        r\"\"\"\n        Set up distributed training.\n\n        Initialise process group and set up DDP variables.\n\n        .. deprecated:: 0.1.0\n            `init_distributed` is deprecated in favor of `Accelerator()`.\n        \"\"\"\n\n        # pylint: disable=W0201\n\n        dist.init_process_group(backend=\"nccl\")\n        self.rank = dist.get_rank()\n        self.world_size = dist.get_world_size()\n        self.local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n        torch.cuda.set_device(self.local_rank)\n        self.is_main_process = self.rank == 0\n        self.is_local_main_process = self.local_rank == 0\n\n    @catch\n    def save(self, obj: Any, f: File, main_process_only: bool = True) -&gt; File:  # pylint: disable=C0103\n        r\"\"\"\n        Save object to a path or file.\n        Returns\n        -------\n        File\n        \"\"\"\n\n        if main_process_only and self.is_main_process or not on_main_process:\n            self.accelerator.save(obj, f)\n        return f\n\n    @staticmethod\n    def load(f: File, *args, **kwargs) -&gt; Any:  # pylint: disable=C0103\n        r\"\"\"\n        Load object from a path or file.\n        Returns\n        -------\n        Any\n        \"\"\"\n\n        return torch.load(f, *args, **kwargs)  # type: ignore\n\n    @property\n    def world_size(self) -&gt; int:\n        r\"\"\"\n        Number of Processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return self.accelerator.num_processes\n\n    @property\n    def rank(self) -&gt; int:\n        r\"\"\"\n        Process index in all processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return self.accelerator.process_index\n\n    @property\n    def local_rank(self) -&gt; int:\n        r\"\"\"\n        Process index in local processes.\n\n        Returns\n        -------\n        int\n        \"\"\"\n\n        return self.accelerator.local_process_index\n</code></pre>","location":"zh/runner/torch_runner/"},{"title":"<code>init_tensorboard(*args, **kwargs)</code>","text":"<p>Set up Tensoraoard SummaryWriter.</p>  Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@on_main_process\ndef init_tensorboard(self, *args, **kwargs) -&gt; None:\n    r\"\"\"\n    Set up Tensoraoard SummaryWriter.\n    \"\"\"\n    from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n    if \"log_dir\" not in kwargs:\n        kwargs[\"log_dir\"] = self.dir\n\n    self.writer = SummaryWriter(*args, **kwargs)\n</code></pre>","location":"zh/runner/torch_runner/#danling.runner.torch_runner.TorchRunner.init_tensorboard"},{"title":"<code>save(obj, f, main_process_only=True)</code>","text":"<p>Save object to a path or file.</p> <p>Returns:</p>    Type Description      <code>File</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@catch\ndef save(self, obj: Any, f: File, main_process_only: bool = True) -&gt; File:  # pylint: disable=C0103\n    r\"\"\"\n    Save object to a path or file.\n    Returns\n    -------\n    File\n    \"\"\"\n\n    if main_process_only and self.is_main_process or not on_main_process:\n        self.accelerator.save(obj, f)\n    return f\n</code></pre>","location":"zh/runner/torch_runner/#danling.runner.torch_runner.TorchRunner.save"},{"title":"<code>load(f, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Load object from a path or file.</p> <p>Returns:</p>    Type Description      <code>Any</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@staticmethod\ndef load(f: File, *args, **kwargs) -&gt; Any:  # pylint: disable=C0103\n    r\"\"\"\n    Load object from a path or file.\n    Returns\n    -------\n    Any\n    \"\"\"\n\n    return torch.load(f, *args, **kwargs)  # type: ignore\n</code></pre>","location":"zh/runner/torch_runner/#danling.runner.torch_runner.TorchRunner.load"},{"title":"<code>world_size()</code>  <code>property</code>","text":"<p>Number of Processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@property\ndef world_size(self) -&gt; int:\n    r\"\"\"\n    Number of Processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return self.accelerator.num_processes\n</code></pre>","location":"zh/runner/torch_runner/#danling.runner.torch_runner.TorchRunner.world_size"},{"title":"<code>rank()</code>  <code>property</code>","text":"<p>Process index in all processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@property\ndef rank(self) -&gt; int:\n    r\"\"\"\n    Process index in all processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return self.accelerator.process_index\n</code></pre>","location":"zh/runner/torch_runner/#danling.runner.torch_runner.TorchRunner.rank"},{"title":"<code>local_rank()</code>  <code>property</code>","text":"<p>Process index in local processes.</p> <p>Returns:</p>    Type Description      <code>int</code>       Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@property\ndef local_rank(self) -&gt; int:\n    r\"\"\"\n    Process index in local processes.\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    return self.accelerator.local_process_index\n</code></pre>","location":"zh/runner/torch_runner/#danling.runner.torch_runner.TorchRunner.local_rank"},{"title":"<code>set_deterministic()</code>","text":"<p>Set up deterministic.</p>  Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def set_deterministic(self) -&gt; None:\n    r\"\"\"\n    Set up deterministic.\n    \"\"\"\n\n    cudnn.benchmark = False\n    cudnn.deterministic = True\n    if torch.__version__ &gt;= \"1.8.0\":\n        torch.use_deterministic_algorithms(True)\n</code></pre>","location":"zh/runner/torch_runner/#danling.runner.torch_runner.TorchRunner.set_deterministic"},{"title":"<code>set_seed(bias=None)</code>","text":"<p>Set up random seed.</p> <p>Parameters:</p>    Name Type Description Default     <code>bias</code>  <code>Optional[int]</code>  <p>Make the seed different for each processes. This avoids same data augmentation are applied on every processes. Set to <code>False</code> to disable this feature.</p>  <code>None</code>      Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def set_seed(self, bias: Optional[int] = None) -&gt; None:\n    r\"\"\"\n    Set up random seed.\n\n    Parameters\n    ----------\n    bias: Optional[int] = self.rank\n        Make the seed different for each processes.\n        This avoids same data augmentation are applied on every processes.\n        Set to `False` to disable this feature.\n    \"\"\"\n\n    if self.distributed:\n        # TODO: use broadcast_object instead.\n        self.seed = (\n            self.accelerator.gather(torch.tensor(self.seed).cuda()).unsqueeze(0).flatten()[0]  # pylint: disable=E1101\n        )\n    if bias is None:\n        bias = self.rank\n    seed = self.seed + bias if bias else self.seed\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n</code></pre>","location":"zh/runner/torch_runner/#danling.runner.torch_runner.TorchRunner.set_seed"},{"title":"<code>init_tensorboard(*args, **kwargs)</code>","text":"<p>Set up Tensoraoard SummaryWriter.</p>  Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@on_main_process\ndef init_tensorboard(self, *args, **kwargs) -&gt; None:\n    r\"\"\"\n    Set up Tensoraoard SummaryWriter.\n    \"\"\"\n    from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n    if \"log_dir\" not in kwargs:\n        kwargs[\"log_dir\"] = self.dir\n\n    self.writer = SummaryWriter(*args, **kwargs)\n</code></pre>","location":"zh/runner/torch_runner/#danling.runner.torch_runner.TorchRunner.init_tensorboard"},{"title":"Runner","text":"<p>The Runner of DanLing sets up the basic environment for running neural networks.</p>","location":"zh/tensors/"},{"title":"Components","text":"<p>For readability and maintainability, there are three levels of Runner:</p>","location":"zh/tensors/#components"},{"title":"[<code>RunnerBase</code>][danling.runner.bases.RunnerBase]","text":"<p>[<code>RunnerBase</code>][danling.runner.bases.RunnerBase] gives you a basic instinct on what attributes and properties are provided by the Runner.</p> <p>It works in an AbstractBaseClass manner and should neither be used directly nor be inherited from.</p>","location":"zh/tensors/#runnerbasedanlingrunnerbasesrunnerbase"},{"title":"[<code>BaseRunner</code>][danling.runner.BaseRunner]","text":"<p>[<code>BaseRunner</code>][danling.runner.BaseRunner] contains core methods of general basic functionality, such as <code>init_logging</code>, <code>append_result</code>, <code>print_result</code>.</p>","location":"zh/tensors/#baserunnerdanlingrunnerbaserunner"},{"title":"[<code>Runner</code>][danling.runner.TorchRunner].","text":"<p><code>Runner</code> should only contain platform-specific features. Currently, only [<code>TorchRunner</code>][danling.runner.TorchRunner] is supported.</p>","location":"zh/tensors/#runnerdanlingrunnertorchrunner"},{"title":"Philosophy","text":"<p>DanLing Runner is designed for a 3.5-level experiments management system: Project, Group, Experiment, and, Run.</p>","location":"zh/tensors/#philosophy"},{"title":"Project","text":"<p>Project corresponds to your project.</p> <p>Generally speaking, there should be only one project for each repository.</p>","location":"zh/tensors/#project"},{"title":"Group","text":"<p>A group groups multiple experiments with similar characteristics.</p> <p>For example, if you run multiple experiments on learning rate, you may want to group them into a group.</p> <p>Note that Group is a virtual level (which is why it only counts 0.5) and does not corresponds to anything.</p>","location":"zh/tensors/#group"},{"title":"Experiment","text":"<p>Experiment is the basic unit of experiments.</p> <p>Experiment usually corresponds to a commit, which means the code should be consistent across the experiment.</p>","location":"zh/tensors/#experiment"},{"title":"Run","text":"<p>Run is the basic unit of running.</p> <p>Run corresponds to a single run of an experiment, each run may have different hyperparameters.</p>","location":"zh/tensors/#run"},{"title":"Attributes &amp; Properties","text":"","location":"zh/tensors/#attributes-properties"},{"title":"General","text":"<ul> <li>id</li> <li>name</li> <li>seed</li> <li>deterministic</li> </ul>","location":"zh/tensors/#general"},{"title":"Progress","text":"<ul> <li>iters</li> <li>steps</li> <li>epochs</li> <li>iter_end</li> <li>step_end</li> <li>epoch_end</li> <li>progress</li> </ul> <p>Note that generally you should only use one of <code>iter</code>, <code>step</code>, <code>epoch</code> to indicate the progress.</p>","location":"zh/tensors/#progress"},{"title":"Model","text":"<ul> <li>model</li> <li>criterion</li> <li>optimizer</li> <li>scheduler</li> </ul>","location":"zh/tensors/#model"},{"title":"Data","text":"<ul> <li>datasets</li> <li>datasamplers</li> <li>dataloaders</li> <li>batch_size</li> <li>batch_size_equivalent</li> </ul> <p><code>datasets</code>, <code>datasamplers</code>, <code>dataloaders</code> should be a dict with the same keys. Their keys should be <code>split</code> (e.g. <code>train</code>, <code>val</code>, <code>test</code>).</p>","location":"zh/tensors/#data"},{"title":"Results","text":"<ul> <li>results</li> <li>latest_result</li> <li>best_result</li> <li>scores</li> <li>latest_score</li> <li>best_score</li> <li>index_set</li> <li>index</li> <li>is_best</li> </ul> <p><code>results</code> should be a list of <code>result</code>. <code>result</code> should be a dict with the same <code>split</code> as keys, like <code>dataloaders</code>. A typical <code>result</code> might look like this: Python<pre><code>{\n    \"train\": {\n        \"loss\": 0.1,\n        \"accuracy\": 0.9,\n    },\n    \"val\": {\n        \"loss\": 0.2,\n        \"accuracy\": 0.8,\n    },\n    \"test\": {\n        \"loss\": 0.3,\n        \"accuracy\": 0.7,\n    },\n}\n</code></pre></p> <p><code>scores</code> are usually a list of <code>float</code>, and are dynamically extracted from <code>results</code> by <code>index_set</code> and <code>index</code>. If <code>index_set = \"val\"</code>, <code>index = \"accuracy\"</code>, then <code>scores = 0.9</code>.</p>","location":"zh/tensors/#results"},{"title":"DDP","text":"<ul> <li>world_size</li> <li>rank</li> <li>local_rank</li> <li>distributed</li> <li>is_main_process</li> <li>is_local_main_process</li> </ul>","location":"zh/tensors/#ddp"},{"title":"IO","text":"<ul> <li>experiments_root</li> <li>dir</li> <li>checkpoint_dir</li> <li>log_path</li> <li>checkpoint_dir_name</li> </ul> <p><code>experiments_root</code> is the root directory of all experiments, and should be consistent across the Project.</p> <p><code>dir</code> is the directory of a certain Run.</p> <p>There is no attributes/properties for Group and Experiment.</p> <p><code>checkpoint_dir_name</code> is relative to <code>dir</code>, and is passed to generate <code>checkpoint_dir</code> (<code>checkpoint_dir = os.path.join(dir, checkpoint_dir_name)</code>). In practice, <code>checkpoint_dir_name</code> is rarely called.</p>","location":"zh/tensors/#io"},{"title":"logging","text":"<ul> <li>log</li> <li>logger</li> <li>tensorboard</li> <li>writer</li> </ul>","location":"zh/tensors/#logging"},{"title":"NestedTensor","text":"<p>Wrap a sequence of tensors into a single tensor with a mask.</p> <p>In sequence to sequence tasks, elements of a batch are usually not of the same length. This made it tricky to use a single tensor to represent a batch of sequences.</p> <p>NestedTensor allows to store a sequence of tensors of different lengths in a single object. It also provides a mask that can be used to retrieve the original sequence of tensors.</p> <p>Attributes:</p>    Name Type Description     <code>storage</code>  <code>Sequence[torch.Tensor]</code>  <p>The sequence of tensors.</p>   <code>batch_first</code>  <code>bool = True</code>  <p>Whether the first dimension of the tensors is the batch dimension.</p> <p>If <code>True</code>, the first dimension is the batch dimension, i.e., <code>B, N, *</code>.</p> <p>If <code>False</code>, the first dimension is the sequence dimension, i.e., <code>N, B, *</code></p>    <p>Parameters:</p>    Name Type Description Default     <code>tensors</code>  <code>Sequence[Tensor]</code>    required    <code>batch_first</code>  <code>bool</code>    <code>True</code>     <p>Raises:</p>    Type Description      <code>ValueError</code>  <p>If <code>tensors</code> is not a sequence.</p> <p>If <code>tensors</code> is empty.</p>","location":"zh/tensors/nested_tensor/"},{"title":"Notes","text":"<p>We have rewritten the <code>__getattr__</code> function to support as much native tensor operations as possible. However, not all operations are tested.</p> <p>Please file an issue if you find any bugs.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.device\ndevice(type='cpu')\n&gt;&gt;&gt; nested_tensor.dtype\ntorch.int64\n&gt;&gt;&gt; nested_tensor.tensor\ntensor([[1, 2, 3],\n        [4, 5, 0]])\n&gt;&gt;&gt; nested_tensor.mask\ntensor([[ True,  True,  True],\n        [ True,  True, False]])\n&gt;&gt;&gt; nested_tensor.to(torch.float).tensor\ntensor([[1., 2., 3.],\n        [4., 5., 0.]])\n&gt;&gt;&gt; nested_tensor.half().tensor\ntensor([[1., 2., 3.],\n        [4., 5., 0.]], dtype=torch.float16)\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>class NestedTensor:\n    r\"\"\"\n    Wrap a sequence of tensors into a single tensor with a mask.\n\n    In sequence to sequence tasks, elements of a batch are usually not of the same length.\n    This made it tricky to use a single tensor to represent a batch of sequences.\n\n    NestedTensor allows to store a sequence of tensors of different lengths in a single object.\n    It also provides a mask that can be used to retrieve the original sequence of tensors.\n\n    Attributes\n    ----------\n    storage: Sequence[torch.Tensor]\n        The sequence of tensors.\n    batch_first: bool = True\n        Whether the first dimension of the tensors is the batch dimension.\n\n        If `True`, the first dimension is the batch dimension, i.e., `B, N, *`.\n\n        If `False`, the first dimension is the sequence dimension, i.e., `N, B, *`\n\n    Parameters\n    ----------\n    tensors: Sequence[torch.Tensor]\n    batch_first: bool = True\n\n    Raises\n    ------\n    ValueError\n        If `tensors` is not a sequence.\n\n        If `tensors` is empty.\n\n    Notes\n    -----\n    We have rewritten the `__getattr__` function to support as much native tensor operations as possible.\n    However, not all operations are tested.\n\n    Please file an issue if you find any bugs.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.shape\n    torch.Size([2, 3])\n    &gt;&gt;&gt; nested_tensor.device\n    device(type='cpu')\n    &gt;&gt;&gt; nested_tensor.dtype\n    torch.int64\n    &gt;&gt;&gt; nested_tensor.tensor\n    tensor([[1, 2, 3],\n            [4, 5, 0]])\n    &gt;&gt;&gt; nested_tensor.mask\n    tensor([[ True,  True,  True],\n            [ True,  True, False]])\n    &gt;&gt;&gt; nested_tensor.to(torch.float).tensor\n    tensor([[1., 2., 3.],\n            [4., 5., 0.]])\n    &gt;&gt;&gt; nested_tensor.half().tensor\n    tensor([[1., 2., 3.],\n            [4., 5., 0.]], dtype=torch.float16)\n\n    ```\n    \"\"\"\n\n    storage: Sequence[Tensor] = []\n    batch_first: bool = True\n\n    def __init__(self, tensors: Sequence[Tensor], batch_first: bool = True) -&gt; None:\n        if not isinstance(tensors, Sequence):\n            raise ValueError(f\"NestedTensor should be initialised with a Sequence, bug got {type(values)}.\")\n        if len(tensors) == 0:\n            raise ValueError(f\"NestedTensor should be initialised with a non-empty sequence.\")\n        if not isinstance(tensors[0], Tensor):\n            tensors = tuple(torch.tensor(tensor) for tensor in tensors)\n        self.storage = tensors\n        self.batch_first = batch_first\n\n    @property\n    def tensor(self) -&gt; Tensor:\n        r\"\"\"\n        Return a single tensor by padding all the tensors.\n\n        Returns\n        -------\n        torch.Tensor\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.tensor\n        tensor([[1, 2, 3],\n                [4, 5, 0]])\n\n        ```\n        \"\"\"\n\n        return self._tensor(tuple(self.storage), self.batch_first)\n\n    @property\n    def mask(self) -&gt; Tensor:\n        r\"\"\"\n        Padding mask of `tensor`.\n\n        Returns\n        -------\n        torch.Tensor\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.mask\n        tensor([[ True,  True,  True],\n                [ True,  True, False]])\n\n        ```\n        \"\"\"\n\n        return self._mask(tuple(self.storage))\n\n    @property\n    def device(self) -&gt; torch.device:\n        r\"\"\"\n        Device of the NestedTensor.\n\n        Returns\n        -------\n        torch.Tensor\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.device\n        device(type='cpu')\n\n        ```\n        \"\"\"\n\n        return self._device(tuple(self.storage))\n\n    @property\n    def shape(self) -&gt; torch.Size:\n        r\"\"\"\n        Alias for `size`.\n\n        Returns\n        -------\n        torch.Size\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.shape\n        torch.Size([2, 3])\n        &gt;&gt;&gt; nested_tensor.storage.append(torch.tensor([6, 7, 8, 9]))\n        &gt;&gt;&gt; nested_tensor.shape\n        torch.Size([3, 4])\n\n        ```\n        \"\"\"\n\n        return self.size()\n\n    def size(self) -&gt; torch.Size:\n        r\"\"\"\n        Shape of the NestedTensor.\n\n        Returns\n        -------\n        torch.Size\n\n        Examples\n        --------\n        ```python\n        &gt;&gt;&gt; from danling.tensors import NestedTensor\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.size()\n        torch.Size([2, 3])\n        &gt;&gt;&gt; nested_tensor.storage[1] = torch.tensor([4, 5, 6, 7])\n        &gt;&gt;&gt; nested_tensor.size()\n        torch.Size([2, 4])\n\n        ```\n        \"\"\"\n\n        return self._size(tuple(self.storage))\n\n    def __getitem__(self, index) -&gt; Tuple[Tensor, Tensor]:\n        ret = self.storage[index]\n        if isinstance(ret, Tensor):\n            return ret, torch.ones_like(ret)  # pylint: disable=E1101\n        return self.tensor, self.mask\n\n    def __getattr__(self, name) -&gt; Any:\n        if not self.storage:\n            raise ValueError(f\"Unable to get {name} from an empty {self.__class__.__name__}\")\n        ret = [getattr(i, name) for i in self.storage]\n        elem = ret[0]\n        if isinstance(elem, Tensor):\n            return NestedTensor(ret)\n        if callable(elem):\n            return _TensorFuncWrapper(ret)\n        if len(set(ret)) == 1:\n            return elem\n        return ret\n\n    def __len__(self) -&gt; int:\n        return len(self.storage)\n\n    def __setstate__(self, storage) -&gt; None:\n        self.storage = storage\n\n    def __getstate__(self) -&gt; Sequence[Tensor]:\n        return self.storage\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _tensor(storage, batch_first) -&gt; Tensor:\n        return pad_sequence(storage, batch_first=batch_first)\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _mask(storage) -&gt; Tensor:\n        lens = torch.tensor([len(t) for t in storage], device=storage[0].device)\n        return torch.arange(max(lens), device=storage[0].device)[None, :] &lt; lens[:, None]  # pylint: disable=E1101\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _device(storage) -&gt; torch.device:\n        return storage[0].device\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _size(storage) -&gt; torch.Size:\n        return torch.Size(  # pylint: disable=E1101\n            [len(storage), max(t.shape[0] for t in storage), *storage[0].shape[1:]]\n        )\n</code></pre>","location":"zh/tensors/nested_tensor/#danling.tensors.NestedTensor--notes"},{"title":"<code>device()</code>  <code>property</code>","text":"<p>Device of the NestedTensor.</p> <p>Returns:</p>    Type Description      <code>torch.Tensor</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.device\ndevice(type='cpu')\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@property\ndef device(self) -&gt; torch.device:\n    r\"\"\"\n    Device of the NestedTensor.\n\n    Returns\n    -------\n    torch.Tensor\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.device\n    device(type='cpu')\n\n    ```\n    \"\"\"\n\n    return self._device(tuple(self.storage))\n</code></pre>","location":"zh/tensors/nested_tensor/#danling.tensors.nested_tensor.NestedTensor.device"},{"title":"<code>mask()</code>  <code>property</code>","text":"<p>Padding mask of <code>tensor</code>.</p> <p>Returns:</p>    Type Description      <code>torch.Tensor</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.mask\ntensor([[ True,  True,  True],\n        [ True,  True, False]])\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@property\ndef mask(self) -&gt; Tensor:\n    r\"\"\"\n    Padding mask of `tensor`.\n\n    Returns\n    -------\n    torch.Tensor\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.mask\n    tensor([[ True,  True,  True],\n            [ True,  True, False]])\n\n    ```\n    \"\"\"\n\n    return self._mask(tuple(self.storage))\n</code></pre>","location":"zh/tensors/nested_tensor/#danling.tensors.nested_tensor.NestedTensor.mask"},{"title":"<code>shape()</code>  <code>property</code>","text":"<p>Alias for <code>size</code>.</p> <p>Returns:</p>    Type Description      <code>torch.Size</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.storage.append(torch.tensor([6, 7, 8, 9]))\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([3, 4])\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@property\ndef shape(self) -&gt; torch.Size:\n    r\"\"\"\n    Alias for `size`.\n\n    Returns\n    -------\n    torch.Size\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.shape\n    torch.Size([2, 3])\n    &gt;&gt;&gt; nested_tensor.storage.append(torch.tensor([6, 7, 8, 9]))\n    &gt;&gt;&gt; nested_tensor.shape\n    torch.Size([3, 4])\n\n    ```\n    \"\"\"\n\n    return self.size()\n</code></pre>","location":"zh/tensors/nested_tensor/#danling.tensors.nested_tensor.NestedTensor.shape"},{"title":"<code>size()</code>","text":"<p>Shape of the NestedTensor.</p> <p>Returns:</p>    Type Description      <code>torch.Size</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.size()\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.storage[1] = torch.tensor([4, 5, 6, 7])\n&gt;&gt;&gt; nested_tensor.size()\ntorch.Size([2, 4])\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def size(self) -&gt; torch.Size:\n    r\"\"\"\n    Shape of the NestedTensor.\n\n    Returns\n    -------\n    torch.Size\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.size()\n    torch.Size([2, 3])\n    &gt;&gt;&gt; nested_tensor.storage[1] = torch.tensor([4, 5, 6, 7])\n    &gt;&gt;&gt; nested_tensor.size()\n    torch.Size([2, 4])\n\n    ```\n    \"\"\"\n\n    return self._size(tuple(self.storage))\n</code></pre>","location":"zh/tensors/nested_tensor/#danling.tensors.nested_tensor.NestedTensor.size"},{"title":"<code>tensor()</code>  <code>property</code>","text":"<p>Return a single tensor by padding all the tensors.</p> <p>Returns:</p>    Type Description      <code>torch.Tensor</code>      <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; from danling.tensors import NestedTensor\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.tensor\ntensor([[1, 2, 3],\n        [4, 5, 0]])\n</code></pre>  Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@property\ndef tensor(self) -&gt; Tensor:\n    r\"\"\"\n    Return a single tensor by padding all the tensors.\n\n    Returns\n    -------\n    torch.Tensor\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; from danling.tensors import NestedTensor\n    &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n    &gt;&gt;&gt; nested_tensor.tensor\n    tensor([[1, 2, 3],\n            [4, 5, 0]])\n\n    ```\n    \"\"\"\n\n    return self._tensor(tuple(self.storage), self.batch_first)\n</code></pre>","location":"zh/tensors/nested_tensor/#danling.tensors.nested_tensor.NestedTensor.tensor"},{"title":"Decorator","text":"","location":"zh/utils/decorator/"},{"title":"<code>catch(error=Exception, exclude=None, print_args=False)</code>","text":"<p>Decorator to catch <code>error</code> except for <code>exclude</code>. Detailed traceback will be printed to <code>stdout</code>.</p> <p><code>catch</code> is extremely useful for unfatal errors. For example, <code>Runner</code> by de</p> <p>Parameters:</p>    Name Type Description Default     <code>error</code>  <code>Exception</code>    <code>Exception</code>    <code>exclude</code>  <code>Optional[Exception]</code>    <code>None</code>    <code>print_args</code>  <code>bool</code>  <p>Whether to print the arguments passed to the function.</p>  <code>False</code>      Source code in <code>danling/utils/decorator.py</code> Python<pre><code>@flexible_decorator\ndef catch(error: Exception = Exception, exclude: Optional[Exception] = None, print_args: bool = False):\n    \"\"\"\n    Decorator to catch `error` except for `exclude`.\n    Detailed traceback will be printed to `stdout`.\n\n    `catch` is extremely useful for unfatal errors.\n    For example, `Runner` by de\n\n    Parameters\n    ----------\n    error: Exception\n    exclude: Optional[Exception] = None\n    print_args: bool = False\n        Whether to print the arguments passed to the function.\n    \"\"\"\n\n    def decorator(func, error: Exception = Exception, exclude: Optional[Exception] = None, print_args: bool = False):\n        @wraps(func)\n        def wrapper(*args, **kwargs):  # pylint: disable=R1710\n            try:\n                return func(*args, **kwargs)\n            except error as exc:  # pylint: disable=W0703\n                if exclude is not None and isinstance(exc, exclude):\n                    raise exc\n                message = format_exc()\n                message += f\"\\nencoutered when calling {func}\"\n                if print_args:\n                    message += (f\"with args {args} and kwargs {kwargs}\",)\n                print(message, file=stderr, force=True)\n\n        return wrapper\n\n    return lambda func: decorator(func, error, exclude, print_args)\n</code></pre>","location":"zh/utils/decorator/#danling.utils.decorator.catch"},{"title":"<code>ensure_dir(func)</code>","text":"<p>Decorator to ensure a directory property exists.</p>  Source code in <code>danling/utils/decorator.py</code> Python<pre><code>def ensure_dir(func):\n    \"\"\"\n    Decorator to ensure a directory property exists.\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        path = abspath(func(*args, **kwargs))\n        makedirs(path, exist_ok=True)\n        return path\n\n    return wrapper\n</code></pre>","location":"zh/utils/decorator/#danling.utils.decorator.ensure_dir"},{"title":"<code>flexible_decorator(maybe_decorator=None)</code>","text":"<p>Decorator to allow bracket-less when no arguments are passed.</p> <p>Examples:</p> <p>For decorator defined as follows:</p> Python Console Session<pre><code>&gt;&gt;&gt; @flexible_decorator\n&gt;&gt;&gt; def decorator(*args, **kwargs):\n...     pass\n</code></pre> <p>The following two are equivalent:</p> Python Console Session<pre><code>&gt;&gt;&gt; @decorator\n&gt;&gt;&gt; def func(*args, **kwargs):\n...     pass\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; @decorator()\n&gt;&gt;&gt; def func(*args, **kwargs):\n...     pass\n</code></pre>  Source code in <code>danling/utils/decorator.py</code> Python<pre><code>def flexible_decorator(maybe_decorator: Optional[Callable] = None):\n    \"\"\"\n    Decorator to allow bracket-less when no arguments are passed.\n\n    Examples\n    --------\n    For decorator defined as follows:\n    &gt;&gt;&gt; @flexible_decorator\n    &gt;&gt;&gt; def decorator(*args, **kwargs):\n    ...     pass\n\n    The following two are equivalent:\n    &gt;&gt;&gt; @decorator\n    &gt;&gt;&gt; def func(*args, **kwargs):\n    ...     pass\n\n    &gt;&gt;&gt; @decorator()\n    &gt;&gt;&gt; def func(*args, **kwargs):\n    ...     pass\n\n    \"\"\"\n\n    def decorator(func: Callable):\n        @wraps(decorator)\n        def wrapper(*args, **kwargs):\n            if len(args) == 1 and isfunction(args[0]):\n                return func(**kwargs)(args[0])\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    if maybe_decorator is None:\n        return decorator\n    return decorator(maybe_decorator)\n</code></pre>","location":"zh/utils/decorator/#danling.utils.decorator.flexible_decorator"},{"title":"<code>method_cache(*cache_args, **lru_kwargs)</code>","text":"<p>Decorator to cache the result of an instance method.</p> <p><code>functools.lru_cache</code> uses a strong reference to the instance, which will make the instance immortal and break the garbage collection.</p> <p><code>method_cache</code> uses a weak reference to the instance and works fine.</p> <p>https://rednafi.github.io/reflections/dont-wrap-instance-methods-with-functoolslru_cache-decorator-in-python.html</p>  Source code in <code>danling/utils/decorator.py</code> Python<pre><code>def method_cache(*cache_args, **lru_kwargs):\n    r\"\"\"\n    Decorator to cache the result of an instance method.\n\n    `functools.lru_cache` uses a strong reference to the instance,\n    which will make the instance immortal and break the garbage collection.\n\n    `method_cache` uses a weak reference to the instance and works fine.\n\n    https://rednafi.github.io/reflections/dont-wrap-instance-methods-with-functoolslru_cache-decorator-in-python.html\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(self, *args, **kwargs):\n            self_ref = ref(self)\n\n            @wraps(func)\n            @lru_cache(*cache_args, **lru_kwargs)\n            def cached_method(*args, **kwargs):\n                return func(self_ref(), *args, **kwargs)\n\n            setattr(self, func.__name__, cached_method)\n            return cached_method(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n</code></pre>","location":"zh/utils/decorator/#danling.utils.decorator.method_cache"},{"title":"IO","text":"","location":"zh/utils/io/"},{"title":"<code>is_json_serializable(obj)</code>","text":"<p>Check if <code>obj</code> is JSON serializable.</p>  Source code in <code>danling/utils/io.py</code> Python<pre><code>def is_json_serializable(obj: Any) -&gt; bool:\n    \"\"\"\n    Check if `obj` is JSON serializable.\n    \"\"\"\n    try:\n        json_dumps(obj)\n        return True\n    except (TypeError, OverflowError):\n        return False\n</code></pre>","location":"zh/utils/io/#danling.utils.io.is_json_serializable"},{"title":"<code>load(path, *args, **kwargs)</code>","text":"<p>Load any file with supported extensions.</p>  Source code in <code>danling/utils/io.py</code> Python<pre><code>def load(path: str, *args: List[Any], **kwargs: Dict[str, Any]) -&gt; Any:\n    \"\"\"\n    Load any file with supported extensions.\n    \"\"\"\n    if not os.path.isfile(path):\n        raise ValueError(f\"Trying to load {path} but it is not a file.\")\n    extension = os.path.splitext(path)[-1].lower()[1:]\n    if extension in PYTORCH:\n        if torch_load is None:\n            raise ImportError(f\"Trying to load {path} but torch is not installed.\")\n        return torch_load(path)\n    if extension in NUMPY:\n        if numpy_load is None:\n            raise ImportError(f\"Trying to load {path} but numpy is not installed.\")\n        return numpy_load(path, allow_pickle=True)\n    if extension in CSV:\n        if read_csv is None:\n            raise ImportError(f\"Trying to load {path} but pandas is not installed.\")\n        return read_csv(path, *args, **kwargs)\n    if extension in JSON:\n        with open(path, \"r\") as fp:  # pylint: disable=W1514, C0103\n            return json_load(fp, *args, **kwargs)  # type: ignore\n    if extension in PICKLE:\n        with open(path, \"rb\") as fp:  # pylint: disable=C0103\n            return pickle_load(fp, *args, **kwargs)  # type: ignore\n    raise ValueError(f\"Tying to load {path} with unsupported extension={extension}\")\n</code></pre>","location":"zh/utils/io/#danling.utils.io.load"}]}